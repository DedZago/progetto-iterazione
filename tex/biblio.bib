
@book{aalen2008,
  title = {Survival and {{Event History Analysis}}},
  author = {Aalen, Odd O. and Borgan, Ørnulf and Gjessing, Håkon K.},
  date = {2008},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-0-387-68560-1},
  editorb = {Gail, M. and Krickeberg, K. and Samet, J. and Tsiatis, A. and Wong, W.},
  editorbtype = {redactor},
  isbn = {978-0-387-20287-7 978-0-387-68560-1},
  keywords = {cox,kaplan-meier,nelson-aalen,survival},
  series = {Statistics for {{Biology}} and {{Health}}}
}

@book{abate2011,
  title = {Geometria differenziale},
  author = {Abate, Marco and Tovena, Francesca},
  date = {2011-07-28},
  edition = {2011 edizione},
  publisher = {{Springer Verlag}},
  location = {{Milano}},
  abstract = {L'opera fornisce una introduzione alla geometria delle varietà differenziabili, illustrandone le principali proprietà e descrivendo le tecniche e gli strumenti usati per il loro studio.},
  isbn = {978-88-470-1919-5},
  langid = {Italiano},
  pagetotal = {478}
}

@book{abbott2015,
  title = {Understanding {{Analysis}}},
  author = {Abbott, Stephen},
  date = {2015-05-20},
  edition = {2nd ed. 2015, Corr. 2nd printing 2016 edition},
  publisher = {{Springer}},
  location = {{New York}},
  abstract = {This lively introductory text exposes the student to the rewards of a rigorous study of functions of a real variable. In each chapter, informal discussions of questions that give analysis its inherent fascination are followed by precise, but not overly formal, developments of the techniques needed to make sense of ~them. By focusing on the unifying themes of approximation and the resolution of paradoxes that arise in the transition from the finite to the infinite, the text turns what could be a daunting cascade of definitions and theorems into a coherent and engaging progression of ideas. Acutely aware of the need for rigor, the student is much better prepared to understand what constitutes a proper mathematical proof and how to write one.Fifteen years of classroom experience with the first edition of Understanding Analysis have solidified and refined the central narrative of the second edition. Roughly 150 new exercises join a selection of the best exercises from the first edition, and three more project-style sections have been added. Investigations of Euler’s computation of ζ(2), the Weierstrass Approximation ­ Theorem, and the gamma function are now among the book’s cohort of seminal results serving as motivation and payoff for the beginning student to master the methods of analysis.},
  isbn = {978-1-4939-2711-1},
  langid = {english},
  pagetotal = {324}
}

@article{abdella2019,
  title = {An Adaptive Thresholding-Based Process Variability Monitoring},
  author = {Abdella, Galal M. and Kim, Jinho and Kim, Sangahn and Al-Khalifa, Khalifa N. and Jeong, Myong K. (MK) and Hamouda, Abdel Magid and Elsayed, Elsayed A.},
  date = {2019-07-03},
  journaltitle = {Journal of Quality Technology},
  volume = {51},
  pages = {242--256},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2019.1569952},
  abstract = {In high-dimensional processes, monitoring process variability is considerably difficult due to the large number of variables and the limited number of samples. Monitoring changes in the covariance matrix of a multivariate process is often used for monitoring process variability under the assumption that only a few elements in the covariance matrix are changed simultaneously from the in-control values. The existing LASSO-based covariance monitoring charts in the high-dimensional settings provide good performance in detecting some shift patterns depending on the prespecified tuning parameter. In practice, control charts that perform reasonably well over various shift patterns are desired when shift patterns are unknown. In this article, we propose a control chart based on an adaptive LASSO-thresholding for monitoring changes in the covariance matrix. The performance of the proposed chart, which is called the ALT-norm chart, is evaluated for various shift patterns and compared with the existing penalized likelihood-based methods. The results show the effectiveness of the proposed chart. Finally, we illustrate the advantages of the ALT-norm chart through simulated and real data from both the semiconductor industry and a high-dimensional milling process.},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2019.1569952},
  keywords = {adaptive thresholding estimation,done,monitoring covariance matrix,multivariate statistical process control},
  number = {3}
}

@software{adams2021,
  title = {{{PrincetonLIPS}}/Numpy-Hilbert-Curve},
  author = {Adams, Ryan P.},
  date = {2021},
  origdate = {2020-11-04T19:34:15Z},
  abstract = {Numpy implementation of Hilbert curves in arbitrary dimensions},
  keywords = {done},
  organization = {{Princeton Laboratory for Intelligent Probabilistic Systems}}
}

@book{agresti2010,
  title = {Analysis of Ordinal Categorical Data},
  author = {Agresti, Alan},
  date = {2010-03-30},
  edition = {2 edizione},
  publisher = {{John Wiley \& Sons Inc}},
  location = {{Hoboken, N.J}},
  abstract = {Statistical science’s first coordinated manual of methods for analyzing ordered categorical data, now fully revised and updated, continues to present applications and case studies in fields as diverse as sociology, public health, ecology, marketing, and pharmacy. Analysis of Ordinal Categorical Data, Second Edition provides an introduction to basic descriptive and inferential methods for categorical data, giving thorough coverage of new developments and recent methods. Special emphasis is placed on interpretation and application of methods including an integrated comparison of the available strategies for analyzing ordinal data. Practitioners of statistics in government, industry (particularly pharmaceutical), and academia will want this new edition.},
  isbn = {978-0-470-08289-8},
  langid = {Inglese},
  pagetotal = {396}
}

@book{aho1995,
  title = {Foundations of Computer Science: C Edition},
  shorttitle = {Foundations of Computer Science},
  author = {Aho, Alfred V. and Ullman, Jeffrey D.},
  date = {1995-02-14},
  edition = {New edition edizione},
  publisher = {{W H Freeman \& Co}},
  location = {{New York}},
  abstract = {This text combines the theoretical foundations of computing with essential discrete mathematics. It follows the same organization as its predecessor, Foundations of Computer Science (also published by W.H. Freeman), with all examples and exercises in C.},
  isbn = {978-0-7167-8284-1},
  keywords = {computer science,introduction},
  langid = {Inglese},
  pagetotal = {800}
}

@book{albert2009,
  title = {Bayesian {{Computation}} with {{R}}},
  author = {Albert, Jim},
  date = {2009},
  edition = {2},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  doi = {10.1007/978-0-387-92298-0},
  abstract = {There has been a dramatic growth in the development and application of Bayesian inferential methods. Some of this growth is due to the availability of powerful simulation-based algorithms to summarize posterior distributions. There has been also a growing interest in the use of the system R for statistical analyses. R's open source nature, free availability, and large number of contributor packages have made R the software of choice for many statisticians in education and industry. Bayesian Computation with R introduces Bayesian modeling by the use of computation using the R language. The early chapters present the basic tenets of Bayesian thinking by use of familiar one and two-parameter inferential problems. Bayesian computational methods such as Laplace's method, rejection sampling, and the SIR algorithm are illustrated in the context of a random effects model. The construction and implementation of Markov Chain Monte Carlo (MCMC) methods is introduced. These simulation-based algorithms are implemented for a variety of Bayesian applications such as normal and binary response regression, hierarchical modeling, order-restricted inference, and robust modeling. Algorithms written in R are used to develop Bayesian tests and assess Bayesian models by use of the posterior predictive distribution. The use of R to interface with WinBUGS, a popular MCMC computing language, is described with several illustrative examples. This book is a suitable companion book for an introductory course on Bayesian methods and is valuable to the statistical practitioner who wishes to learn more about the R language and Bayesian methodology. The LearnBayes package, written by the author and available from the CRAN website, contains all of the R functions described in the book. The second edition contains several new topics such as the use of mixtures of conjugate priors and the use of Zellner’s g priors to choose between models in linear regression. There are more illustrations of the construction of informative prior distributions, such as the use of conditional means priors and multivariate normal priors in binary regressions. The new edition contains changes in the R code illustrations according to the latest edition of the LearnBayes package. Jim Albert is Professor of Statistics at Bowling Green State University. He is Fellow of the American Statistical Association and is past editor of The American Statistician. His books include Ordinal Data Modeling (with Val Johnson), Workshop Statistics: Discovery with Data, A Bayesian Approach (with Allan Rossman), and Bayesian Computation using Minitab.},
  isbn = {978-0-387-92297-3},
  langid = {english},
  series = {Use {{R}}!}
}

@book{allen2011,
  title = {Global Economic History: A Very Short Introduction},
  shorttitle = {Global Economic History},
  author = {Allen, Robert C.},
  date = {2011-09-15},
  publisher = {{OUP Oxford}},
  location = {{Oxford ; New York}},
  abstract = {Why are some countries rich and others poor? In 1500, the income differences were small, but they have grown dramatically since Columbus reached America. Since then, the interplay between geography, globalization, technological change, and economic policy has determined the wealth and poverty of nations. The industrial revolution was Britain's path breaking response to the challenge of globalization. Western Europe and North America joined Britain to form a club of rich nations by pursuing four polices-creating a national market by abolishing internal tariffs and investing in transportation, erecting an external tariff to protect their fledgling industries from British competition, banks to stabilize the currency and mobilize domestic savings for investment, and mass education to prepare people for industrial work.  Together these countries pioneered new technologies that have made them ever richer. Before the Industrial Revolution, most of the world's manufacturing was done in Asia, but industries from Casablanca to Canton were destroyed by western competition in the nineteenth century, and Asia was transformed into 'underdeveloped countries' specializing in agriculture. The spread of economic development has been slow since modern technology was invented to fit the needs of rich countries and is ill adapted to the economic and geographical conditions of poor countries. A few countries - Japan, Soviet Russia, South Korea, Taiwan, and perhaps China - have, nonetheless, caught up with the West through creative responses to the technological challenge and with Big Push industrialization that has achieved rapid growth through investment coordination. Whether other countries can emulate the success of East Asia is a challenge for the future.  ABOUT THE SERIES: The Very Short Introductions series from Oxford University Press contains hundreds of titles in almost every subject area. These pocket-sized books are the perfect way to get ahead in a new subject quickly. Our expert authors combine facts, analysis, perspective, new ideas, and enthusiasm to make interesting and challenging topics highly readable.},
  isbn = {978-0-19-959665-2},
  langid = {Inglese},
  pagetotal = {192}
}

@incollection{alt2004,
  title = {Multivariate {{Quality Control}}},
  booktitle = {Encyclopedia of {{Statistical Sciences}}},
  author = {Alt, Frank B.},
  date = {2004},
  publisher = {{American Cancer Society}},
  doi = {10.1002/0471667196.ess1742},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/0471667196.ess1742},
  isbn = {978-0-471-66719-3},
  langid = {english}
}

@online{alvarez-melis2020,
  title = {Geometric {{Dataset Distances}} via {{Optimal Transport}}},
  author = {Alvarez-Melis, David and Fusi, Nicolò},
  date = {2020-02-07},
  abstract = {The notion of task similarity is at the core of various machine learning paradigms, such as domain adaptation and meta-learning. Current methods to quantify it are often heuristic, make strong assumptions on the label sets across the tasks, and many are architecture-dependent, relying on task-specific optimal parameters (e.g., require training a model on each dataset). In this work we propose an alternative notion of distance between datasets that (i) is model-agnostic, (ii) does not involve training, (iii) can compare datasets even if their label sets are completely disjoint and (iv) has solid theoretical footing. This distance relies on optimal transport, which provides it with rich geometry awareness, interpretable correspondences and well-understood properties. Our results show that this novel distance provides meaningful comparison of datasets, and correlates well with transfer learning hardness across various experimental settings and datasets.},
  archiveprefix = {arXiv},
  eprint = {2002.02923},
  eprinttype = {arxiv},
  keywords = {Computer Science - Machine Learning,distance,metric spaces,optimal mass transport,Statistics - Machine Learning,todo,Wasserstein distance},
  primaryclass = {cs, stat}
}

@book{ambrosio2008,
  title = {Gradient {{Flows}}: {{In Metric Spaces}} and in the {{Space}} of {{Probability Measures}}},
  shorttitle = {Gradient {{Flows}}},
  author = {Ambrosio, Luigi and Gigli, Nicola and Savare, Giuseppe},
  date = {2008-03-13},
  edition = {2nd edition},
  publisher = {{Birkhäuser}},
  location = {{Basel}},
  abstract = {The book is devoted to the theory of gradient flows in the general framework of metric spaces, and in the more specific setting of the space of probability measures, which provide a surprising link between optimal transportation theory and many evolutionary PDE's related to (non)linear diffusion. Particular emphasis is given to the convergence of the implicit time discretization method and to the error estimates for this discretization, extending the well established theory in Hilbert spaces. The book is split in two main parts that can be read independently of each other.},
  isbn = {978-3-7643-8721-1},
  langid = {english},
  pagetotal = {343}
}

@incollection{andersen2014,
  title = {{{ARCH}} and {{GARCH Models}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Andersen, Torben and Bollerslev, Tim and Hadi, Ali},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2014-09-29},
  pages = {stat03491},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat03491},
  isbn = {978-1-118-44511-2},
  keywords = {todo},
  langid = {english}
}

@online{andreoli2018,
  title = {A Conjugate Prior for the {{Dirichlet}} Distribution},
  author = {Andreoli, Jean-Marc},
  date = {2018},
  abstract = {This note investigates a conjugate class for the Dirichlet distribution class in the exponential family.},
  archiveprefix = {arXiv},
  eprint = {1811.05266},
  eprinttype = {arxiv},
  keywords = {60E99,Computer Science - Machine Learning,dirichlet prior boojum,done,Statistics - Machine Learning}
}

@article{andrieu2008,
  title = {A Tutorial on Adaptive {{MCMC}}},
  author = {Andrieu, Christophe and Thoms, Johannes},
  date = {2008-12},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {18},
  pages = {343--373},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-008-9110-y},
  keywords = {doing},
  langid = {english},
  number = {4}
}

@book{applebaum2009,
  title = {Lévy {{Processes}} and {{Stochastic Calculus}}},
  author = {Applebaum, David},
  date = {2009},
  edition = {2},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9780511809781},
  abstract = {Lévy processes form a wide and rich class of random process, and have many applications ranging from physics to finance. Stochastic calculus is the mathematics of systems interacting with random noise. Here, the author ties these two subjects together, beginning with an introduction to the general theory of Lévy processes, then leading on to develop the stochastic calculus for Lévy processes in a direct and accessible way. This fully revised edition now features a number of new topics. These include: regular variation and subexponential distributions; necessary and sufficient conditions for Lévy processes to have finite moments; characterisation of Lévy processes with finite variation; Kunita's estimates for moments of Lévy type stochastic integrals; new proofs of Ito representation and martingale representation theorems for general Lévy processes; multiple Wiener-Lévy integrals and chaos decomposition; an introduction to Malliavin calculus; an introduction to stability theory for Lévy-driven SDEs.},
  isbn = {978-0-521-73865-1},
  series = {Cambridge {{Studies}} in {{Advanced Mathematics}}}
}

@book{argiento2019,
  title = {Bayesian {{Statistics}} and {{New Generations}}: {{BAYSM}} 2018, {{Warwick}}, {{UK}}, {{July}} 2-3 {{Selected Contributions}}},
  shorttitle = {Bayesian {{Statistics}} and {{New Generations}}},
  editor = {Argiento, Raffaele and Durante, Daniele and Wade, Sara},
  date = {2019},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-30611-3},
  abstract = {This book presents a selection of peer-reviewed contributions to the fourth Bayesian Young Statisticians Meeting, BAYSM 2018, held at the University of Warwick on 2-3 July 2018. The meeting provided a valuable opportunity for young researchers, MSc students, PhD students, and postdocs interested in Bayesian statistics to connect with the broader Bayesian community. The proceedings offer cutting-edge papers on a wide range of topics in Bayesian statistics, identify important challenges and investigate promising methodological approaches, while also assessing current methods and stimulating applications. The book is intended for a broad audience of statisticians, and demonstrates how theoretical, methodological, and computational aspects are often combined in the Bayesian framework to successfully tackle complex problems.},
  isbn = {978-3-030-30610-6},
  langid = {english},
  series = {Springer {{Proceedings}} in {{Mathematics}} \& {{Statistics}}}
}

@book{ash1999,
  title = {Probability and {{Measure Theory}}},
  author = {Ash, Robert B. and Doléans-Dade, Catherine A.},
  date = {1999-12-20},
  edition = {2nd edition},
  publisher = {{Academic Press}},
  location = {{San Diego}},
  abstract = {Probability and Measure Theory, Second Edition, is a text for a graduate-level course in probability that includes essential background topics in analysis. It provides extensive coverage of conditional probability and expectation, strong laws of large numbers, martingale theory, the central limit theorem, ergodic theory, and Brownian motion.},
  isbn = {978-0-12-065202-0},
  langid = {english},
  pagetotal = {528}
}

@book{athreya2006,
  title = {Measure Theory and Probability Theory},
  author = {Athreya, Krishna B. and Lahiri, Soumendra N.},
  date = {2006},
  publisher = {{Springer Science \& Business Media}},
  abstract = {This book arose out of two graduate courses that the authors have taught duringthepastseveralyears;the?rstonebeingonmeasuretheoryfollowed by the second one on advanced probability theory. The traditional approach to a ?rst course in measure theory, such as in Royden (1988), is to teach the Lebesgue measure on the real line, then the p di?erentation theorems of Lebesgue, L -spaces on R, and do general m- sure at the end of the course with one main application to the construction of product measures. This approach does have the pedagogic advantage of seeing one concrete case ?rst before going to the general one. But this also has the disadvantage in making many students’ perspective on m- sure theory somewhat narrow. It leads them to think only in terms of the Lebesgue measure on the real line and to believe that measure theory is intimately tied to the topology of the real line. As students of statistics, probability, physics, engineering, economics, and biology know very well, there are mass distributions that are typically nonuniform, and hence it is useful to gain a general perspective. This book attempts to provide that general perspective right from the beginning. The opening chapter gives an informal introduction to measure and integration theory. It shows that the notions of ?-algebra of sets and countable additivity of a set function are dictated by certain very na- ral approximation procedures from practical applications and that they are not just some abstract ideas.},
  isbn = {978-0-387-35434-7},
  keywords = {Business & Economics / Operations Research,Computers / Computer Science,Mathematics / Calculus,Mathematics / Mathematical Analysis,Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes},
  pagetotal = {625}
}

@incollection{atkinson2014,
  title = {Optimal {{Design}} of {{Experiments}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Atkinson, A. C. and Fedorov, V. V.},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2014-09-29},
  pages = {stat00911},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat00911},
  isbn = {978-1-118-44511-2},
  keywords = {todo},
  langid = {english}
}

@article{azzalini1993,
  title = {On the {{Use}} of {{Nonparametric Regression}} for {{Checking Linear Relationships}}},
  author = {Azzalini, Adelchi and Bowman, Adrian},
  date = {1993},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {55},
  pages = {549--557},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1993.tb01923.x},
  abstract = {The problem of checking the linearity of a regression relationship is addressed through the idea of smoothing of a residual plot. A pseudolikelihood ratio test statistic, which measures the distance between the nonparametric and the parametric models, is derived as a ratio of quadratic forms. The distribution of this statistic under the null hypothesis of linearity is calculated numerically by using Johnson curves. A power study shows the new statistic to be more sensitive to non-linearity than the Durbin-Watson statistic.},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1993.tb01923.x},
  keywords = {done,durbin-watson test,johnson curves,model checking,nonparametric regression,pseudolikelihood ratio test,residuals,testing linearity},
  langid = {english},
  number = {2}
}

@book{azzalini2001,
  title = {Inferenza Statistica: Una Presentazione Basata Sul Concetto Di Verosimiglianza},
  shorttitle = {Inferenza Statistica},
  author = {Azzalini, A.},
  date = {2001},
  edition = {2},
  publisher = {{Springer-Verlag}},
  abstract = {Il concetto di verosimiglianza gioca un ruolo fondamentale nell'impostazione corrente della Statistica, sia per introdurre nozioni generali della teoria che per lo sviluppo di metodi specifici. Questo libro presenta un'esposizione della teoria statistica basata sulla verosimiglianza, osservata dal punto di vista della "teoria classica", e dimostra come il corpo principale delle tecniche statistiche attualmente in uso possano essere desunte da un numero limitato di concetti-chiave. L'attuale edizione integra la precedente con un capitolo sui modelli lineari generalizzati e con altri aggiornamenti quali numerose illustrazioni numeriche, basate su applicazioni reali, che facilitano la percezione della rilevanza operativa dei metodi presentati.},
  isbn = {978-88-470-0130-5}
}

@book{azzalini2012,
  title = {Data {{Analysis}} and {{Data Mining}}: {{An Introduction}}},
  shorttitle = {Data {{Analysis}} and {{Data Mining}}},
  author = {Azzalini, Adelchi and Scarpa, Bruno},
  date = {2012},
  publisher = {{Oxford University Press}}
}

@inproceedings{baadel2016,
  title = {Overlapping Clustering: {{A}} Review},
  shorttitle = {Overlapping Clustering},
  booktitle = {2016 {{SAI Computing Conference}} ({{SAI}})},
  author = {Baadel, Said and Thabtah, Fadi and Lu, Joan},
  date = {2016-07},
  pages = {233--237},
  publisher = {{IEEE}},
  location = {{London, United Kingdom}},
  doi = {10.1109/SAI.2016.7555988},
  eventtitle = {2016 {{SAI Computing Conference}} ({{SAI}})},
  isbn = {978-1-4673-8460-5},
  keywords = {done}
}

@online{bahdanau2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2016-05-19},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning,todo},
  primaryclass = {cs, stat}
}

@book{baltagi2008,
  title = {Econometric Analysis of Panel Data},
  author = {Baltagi, Badi H.},
  date = {2008-04-29},
  edition = {4 edizione},
  publisher = {{John Wiley \& Sons Inc}},
  location = {{Chichester, UK ; Hoboken, NJ}},
  abstract = {Written by one of the world's leading researchers and writers in the field, Econometric Analysis of Panel Data has become established as the leading textbook for postgraduate courses in panel data. This new edition reflects the rapid developments in the field covering the vast research that has been conducted on panel data since its initial publication. Featuring the most recent empirical examples from panel data literature, data sets are also provided as well as the programs to implement the estimation and testing procedures described in the book. These programs will be made available via an accompanying website which will also contain solutions to end of chapter exercises that will appear in the book.The text has been fully updated with new material on dynamic panel data models and recent results on non-linear panel models and in particular work on limited dependent variables panel data models.},
  isbn = {978-0-470-51886-1},
  langid = {Inglese},
  pagetotal = {351}
}

@article{bandyopadhyay2016,
  title = {Nonparametric Spatial Models for Clustered Ordered Periodontal Data},
  author = {Bandyopadhyay, Dipankar and Canale, Antonio},
  date = {2016-08},
  journaltitle = {Journal of the Royal Statistical Society. Series C, Applied statistics},
  shortjournal = {J R Stat Soc Ser C Appl Stat},
  volume = {65},
  pages = {619--640},
  issn = {0035-9254},
  doi = {10.1111/rssc.12150},
  abstract = {Clinical attachment level (CAL) is regarded as the most popular measure to assess periodontal disease (PD). These probed tooth-site level measures are usually rounded and recorded as whole numbers (in mm) producing clustered (site measures within a mouth) error-prone ordinal responses representing some ordering of the underlying PD progression. In addition, it is hypothesized that PD progression can be spatially-referenced, i.e., proximal tooth-sites share similar PD status in comparison to sites that are distantly located. In this paper, we develop a Bayesian multivariate probit framework for these ordinal responses where the cut-point parameters linking the observed ordinal CAL levels to the latent underlying disease process can be fixed in advance. The latent spatial association characterizing conditional independence under Gaussian graphs is introduced via a nonparametric Bayesian approach motivated by the probit stick-breaking process, where the components of the stick-breaking weights follows a multivariate Gaussian density with the precision matrix distributed as G-Wishart. This yields a computationally simple, yet robust and flexible framework to capture the latent disease status leading to a natural clustering of tooth-sites and subjects with similar PD status (beyond spatial clustering), and improved parameter estimation through sharing of information. Both simulation studies and application to a motivating PD dataset reveal the advantages of considering this flexible nonparametric ordinal framework over other alternatives.},
  eprint = {27524839},
  eprinttype = {pmid},
  keywords = {done},
  number = {4},
  pmcid = {PMC4979584}
}

@inproceedings{banerjee2005,
  title = {Model-Based Overlapping Clustering},
  booktitle = {Proceedings of the Eleventh {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery in Data Mining},
  author = {Banerjee, Arindam and Krumpelman, Chase and Ghosh, Joydeep and Basu, Sugato and Mooney, Raymond J.},
  date = {2005-08-21},
  pages = {532--537},
  publisher = {{Association for Computing Machinery}},
  location = {{Chicago, Illinois, USA}},
  doi = {10.1145/1081870.1081932},
  abstract = {While the vast majority of clustering algorithms are partitional, many real world datasets have inherently overlapping clusters. Several approaches to finding overlapping clusters have come from work on analysis of biological datasets. In this paper, we interpret an overlapping clustering model proposed by Segal et al. [23] as a generalization of Gaussian mixture models, and we extend it to an overlapping clustering model based on mixtures of any regular exponential family distribution and the corresponding Bregman divergence. We provide the necessary algorithm modifications for this extension, and present results on synthetic data as well as subsets of 20-Newsgroups and EachMovie datasets.},
  isbn = {978-1-59593-135-1},
  keywords = {Bregman divergences,done,exponential model,graphical model,high-dimensional clustering,overlapping clustering},
  series = {{{KDD}} '05}
}

@book{banerjee2014,
  title = {Hierarchical {{Modeling}} and {{Analysis}} for {{Spatial Data}}},
  author = {Banerjee, Sudipto and Carlin, Bradley P. and Gelfand, Alan E.},
  date = {2014-09-12},
  edition = {2nd Edition},
  publisher = {{Chapman and Hall/CRC}},
  location = {{Boca Raton}},
  abstract = {Keep Up to Date with the Evolving Landscape of Space and Space-Time Data Analysis and Modeling Since the publication of the first edition, the statistical landscape has substantially changed for analyzing space and space-time data. More than twice the size of its predecessor, Hierarchical Modeling and Analysis for Spatial Data, Second Edition reflects the major growth in spatial statistics as both a research area and an area of application. New to the Second Edition   New chapter on spatial point patterns developed primarily from a modeling perspective New chapter on big data that shows how the predictive process handles reasonably large datasets New chapter on spatial and spatiotemporal gradient modeling that incorporates recent developments in spatial boundary analysis and wombling New chapter on the theoretical aspects of geostatistical (point-referenced) modeling  Greatly expanded chapters on methods for multivariate and spatiotemporal modeling New special topics sections on data fusion/assimilation and spatial analysis for data on extremes Double the number of exercises  Many more color figures integrated throughout the text Updated computational aspects, including the latest version of WinBUGS, the new flexible spBayes software, and assorted R packages  The Only Comprehensive Treatment of the Theory, Methods, and Software This second edition continues to provide a complete treatment of the theory, methods, and application of hierarchical modeling for spatial and spatiotemporal data. It tackles current challenges in handling this type of data, with increased emphasis on observational data, big data, and the upsurge of associated software tools. The authors also explore important application domains, including environmental science, forestry, public health, and real estate.},
  isbn = {978-1-4398-1917-3},
  keywords = {todo},
  langid = {english},
  pagetotal = {584}
}

@book{barndorff-nielsen1989,
  title = {Asymptotic {{Techniques}} for {{Use}} in {{Statistics}}},
  author = {Barndorff-Nielsen, O. E. and Cox, D. R.},
  date = {1989-01-01},
  publisher = {{Springer}},
  location = {{London ; New York}},
  abstract = {The use in statistical theory of approximate arguments based on such methods as local linearization (the delta method) and approxi­ mate normality has a long history. Such ideas play at least three roles. First they may give simple approximate answers to distributional problems where an exact solution is known in principle but difficult to implement. The second role is to yield higher-order expansions from which the accuracy of simple approximations may be assessed and where necessary improved. Thirdly the systematic development of a theoretical approach to statistical inference that will apply to quite general families of statistical models demands an asymptotic formulation, as far as possible one that will recover 'exact' results where these are available. The approximate arguments are developed by supposing that some defining quantity, often a sample size but more generally an amount of information, becomes large: it must be stressed that this is a technical device for generating approximations whose adequacy always needs assessing, rather than a 'physical' limiting notion. Of the three roles outlined above, the first two are quite close to the traditional roles of asymptotic expansions in applied mathematics and much ofthe very extensive literature on the asymptotic expansion of integrals and of the special functions of mathematical physics is quite directly relevant, although the recasting of these methods into a probability mould is quite often enlightening.},
  isbn = {978-0-412-31400-1},
  langid = {english},
  pagetotal = {252}
}

@book{barnett1999,
  title = {Comparative Statistical Inference},
  author = {Barnett, Vic},
  date = {1999},
  publisher = {{John Wiley \& Sons}},
  isbn = {978-0-471-97643-1},
  keywords = {Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes},
  pagetotal = {418}
}

@book{bartholomew2011,
  title = {Latent Variable Models and Factor Analysis : A Unified Approach},
  shorttitle = {Latent Variable Models and Factor Analysis},
  author = {Bartholomew, David J.},
  date = {2011},
  edition = {3rd ed.},
  publisher = {{Wiley,}},
  location = {{Chichester, West Sussex :}},
  isbn = {978-0-470-97192-5},
  keywords = {factor analysis,latent class}
}

@book{bartolucci2012,
  title = {Latent {{Markov Models}} for {{Longitudinal Data}}},
  author = {Bartolucci, Francesco and Farcomeni, Alessio and Pennoni, Fulvia},
  date = {2012},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {Drawing on the authors’ extensive research in the analysis of categorical longitudinal data, Latent Markov Models for Longitudinal Data focuses on the formulation of latent Markov models and the practical use of these models. Numerous examples illustrate how latent Markov models are used in econom},
  langid = {english},
  series = {Chapman \& {{Hall}}/{{CRC Statistics}} in the {{Social}} and {{Behavioral Sciences}}}
}

@article{bassetti2006,
  title = {On Minimum {{Kantorovich}} Distance Estimators},
  author = {Bassetti, Federico and Bodini, Antonella and Regazzini, Eugenio},
  date = {2006-07-01},
  journaltitle = {Statistics \& Probability Letters},
  shortjournal = {Statistics \& Probability Letters},
  volume = {76},
  pages = {1298--1302},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2006.02.001},
  abstract = {This article introduces estimators defined as minimizers of Kantorovich distances between statistical models and empirical distributions. Existence, measurability and consistency of these estimators are studied. A few significant examples illustrate the applicability of the theoretical results dealt with in the paper.},
  keywords = {Consistency of point estimators,Kantorovich distance,Minimum dissimilarity estimators,Minimum Kantorovich distance estimators,optimal transport,todo,Wasserstein distance},
  langid = {english},
  number = {12}
}

@article{basu1955,
  title = {On Statistics Independent of a Complete Sufficient Statistic},
  author = {Basu, Debabrata},
  date = {1955},
  journaltitle = {Sankhyā: The Indian Journal of Statistics},
  volume = {15},
  keywords = {done}
}

@article{bayer2020,
  title = {The Look-Elsewhere Effect from a Unified {{Bayesian}} and Frequentist Perspective},
  author = {Bayer, Adrian E. and Seljak, Uroš},
  date = {2020-10-02},
  journaltitle = {Journal of Cosmology and Astroparticle Physics},
  shortjournal = {J. Cosmol. Astropart. Phys.},
  volume = {2020},
  pages = {009},
  publisher = {{IOP Publishing}},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2020/10/009},
  keywords = {todo},
  langid = {english},
  number = {10}
}

@book{beaumont1980,
  title = {Intermediate {{Mathematical Statistics}}},
  author = {Beaumont, G. P.},
  date = {1980},
  publisher = {{Springer Netherlands}},
  doi = {10.1007/978-94-009-5794-7},
  abstract = {This book covers those basic topics which usually form the core of intermediate courses in statistical theory; it is largely about estima­ tion and hypothesis testing. It is intended for undergraduates following courses in statistics but is also suitable preparatory read­ ing for some postgraduate courses. It is assumed that the reader has completed an introductory course which covered probability, random variables, moments and the sampling distributions. The level of mathematics required does not go beyond first year calculus. In case the reader has not acquired much facility in handling matrices, the results in least squares estimation are first obtained directly and then given an (optional) matrix formulation. If techniques for changing from one set of variables to another have not been met, then the appendix on these topics should be studied first. The same appendix contains essential discussion of the order statistics which are frequently used for illustrative purposes. Introductory courses usually include the elements of hypothesis testing and of point and interval estimation though the treatment must perforce become rather thin since at that stage it is difficult to provide adequate justifications for some procedures-plausible though they may seem. This text discusses these important topics in considerable detail, starting from scratch. The level is nowhere advanced and proofs of asymptotic results are omitted. Methods deriving from the Bayesian point of view are gradually introduced and alternate with the more usual techniques.},
  isbn = {978-0-412-15480-5},
  langid = {english}
}

@article{bell2014,
  title = {A {{Distribution}}-{{Free Multivariate Phase I Location Control Chart}} for {{Subgrouped Data}} from {{Elliptical Distributions}}},
  author = {Bell, Richard C. and Jones-Farmer, L. Allison and Billor, Nedret},
  date = {2014-10-02},
  journaltitle = {Technometrics},
  volume = {56},
  pages = {528--538},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2013.879264},
  abstract = {In quality control, a proper Phase I analysis is essential to the success of Phase II monitoring. A literature review reveals no distribution-free Phase I multivariate techniques in existence. This research develops a Phase I location control chart for multivariate elliptical processes. The resulting in-control reference sample can then be used to estimate the parameters for Phase II monitoring. Using Monte Carlo simulation, the proposed method is compared with the Hotelling's T2 Phase I chart. Although Hotelling's T2 chart is preferred when the data are multivariate normal, the proposed method is shown to perform significantly better under nonnormality. This article has supplementary material online.},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2013.879264},
  keywords = {Data depth,done,Mahalanobis depth,Mean rank chart,Nonparametric,Outliers,Retrospective analysis,Robust estimators},
  number = {4}
}

@online{bengio2012,
  title = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  author = {Bengio, Yoshua},
  date = {2012-09-16},
  abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
  archiveprefix = {arXiv},
  eprint = {1206.5533},
  eprinttype = {arxiv},
  keywords = {Computer Science - Machine Learning,done},
  primaryclass = {cs}
}

@article{benjamini2001,
  title = {The Control of the False Discovery Rate in Multiple Testing under Dependency},
  author = {Benjamini, Yoav and Yekutieli, Daniel},
  date = {2001-08},
  journaltitle = {The Annals of Statistics},
  volume = {29},
  pages = {1165--1188},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1013699998},
  abstract = {Benjamini and Hochberg suggest that the false discovery rate may be the appropriate error rate to control in many applied multiple testing problems. A simple procedure was given there as an FDR controlling procedure for independent test statistics and was shown to be much more powerful than comparable procedures which control the traditional familywise error rate. We prove that this same procedure also controls the false discovery rate when the test statistics have positive regression dependency on each of the test statistics corresponding to the true null hypotheses. This condition for positive dependency is general enough to cover many problems of practical interest, including the comparisons of many treatments with a single control, multivariate normal test statistics with positive correlation matrix and multivariate \$t\$. Furthermore, the test statistics may be discrete, and the tested hypotheses composite without posing special difficulties. For all other forms of dependency, a simple conservative modification of the procedure controls the false discovery rate. Thus the range of problems for which a procedure with proven FDR control can be offered is greatly increased.},
  keywords = {47N30,62G30,62J15,comparisons with control,discrete test statistics,FDR,Hochberg’s procedure,MTP2 densities,Multiple comparisons procedures,multiple endpoints many-to-one comparisons,positive regression dependency,Simes’equality,todo,unidimensional latent variables},
  number = {4}
}

@article{bennett2010,
  title = {Neural {{Correlates}} of {{Interspecies Perspective Taking}} in the {{Post}}-{{Mortem Atlantic Salmon}} : {{An Argument For Proper Multiple Comparisons Correction}}},
  author = {Bennett, C. M. and Baird, A. and Miller, M. and Wolford, G.},
  date = {2010},
  journaltitle = {Journal of Serendipitous and Unexpected Results},
  abstract = {Neural Correlates of Interspecies Perspective Taking in the Post-Mortem Atlantic Salmon: An Argument For Proper Multiple Comparisons Correction Craig M. Bennett 1∗, Abigail A. Baird 2, Michael B. Miller 1 and George L. Wolford 3 Department of Psychology, University of California at Santa Barbara, Santa Barbara, CA 93106 Department of Psychology, Blodgett Hall, Vassar College, Poughkeepsie, NY 12604 Department of Psychological and Brain Sciences, Moore Hall, Dartmouth College, Hanover, NH 03755},
  keywords = {todo},
  langid = {english}
}

@online{bernton2019,
  title = {On Parameter Estimation with the {{Wasserstein}} Distance},
  author = {Bernton, Espen and Jacob, Pierre E. and Gerber, Mathieu and Robert, Christian P.},
  date = {2019-05-09},
  abstract = {Statistical inference can be performed by minimizing, over the parameter space, the Wasserstein distance between model distributions and the empirical distribution of the data. We study asymptotic properties of such minimum Wasserstein distance estimators, complementing results derived by Bassetti, Bodini and Regazzini in 2006. In particular, our results cover the misspecified setting, in which the data-generating process is not assumed to be part of the family of distributions described by the model. Our results are motivated by recent applications of minimum Wasserstein estimators to complex generative models. We discuss some difficulties arising in the approximation of these estimators and illustrate their behavior in several numerical experiments. Two of our examples are taken from the literature on approximate Bayesian computation and have likelihood functions that are not analytically tractable. Two other examples involve misspecified models.},
  archiveprefix = {arXiv},
  eprint = {1701.05146},
  eprinttype = {arxiv},
  keywords = {generative models,Kantorovich distance,Mathematics - Statistics Theory,minimum distance estimation,optimal transport,parameter inference,Statistics - Computation,Statistics - Methodology,todo,Wasserstein distance},
  primaryclass = {math, stat}
}

@article{bernton2019a,
  title = {Approximate {{Bayesian}} Computation with the {{Wasserstein}} Distance},
  author = {Bernton, Espen and Jacob, Pierre E. and Gerber, Mathieu and Robert, Christian P.},
  date = {2019-04},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  shortjournal = {J. R. Stat. Soc. B},
  volume = {81},
  pages = {235--269},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/rssb.12312},
  abstract = {A growing number of generative statistical models do not permit the numerical evaluation of their likelihood functions. Approximate Bayesian computation (ABC) has become a popular approach to overcome this issue, in which one simulates synthetic data sets given parameters and compares summaries of these data sets with the corresponding observed values. We propose to avoid the use of summaries and the ensuing loss of information by instead using the Wasserstein distance between the empirical distributions of the observed and synthetic data. This generalizes the well-known approach of using order statistics within ABC to arbitrary dimensions. We describe how recently developed approximations of the Wasserstein distance allow the method to scale to realistic data sizes, and propose a new distance based on the Hilbert space-filling curve. We provide a theoretical study of the proposed method, describing consistency as the threshold goes to zero while the observations are kept fixed, and concentration properties as the number of observations grows. Various extensions to time series data are discussed. The approach is illustrated on various examples, including univariate and multivariate g-and-k distributions, a toggle switch model from systems biology, a queueing model, and a L\textbackslash 'evy-driven stochastic volatility model.},
  archiveprefix = {arXiv},
  eprint = {1905.03747},
  eprinttype = {arxiv},
  keywords = {Statistics - Methodology,todo},
  number = {2}
}

@online{betancourt2018,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  date = {2018-07-15},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archiveprefix = {arXiv},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  keywords = {Statistics - Methodology,todo},
  primaryclass = {stat}
}

@article{bezdek1984,
  title = {{{FCM}}—the {{Fuzzy C}}-{{Means}} Clustering-Algorithm},
  author = {Bezdek, James and Ehrlich, Robert and Full, William},
  date = {1984},
  journaltitle = {Computers \& Geosciences},
  shortjournal = {Computers \& Geosciences},
  volume = {10},
  pages = {191--203},
  abstract = {This paper transmits a FORTRAN-IV coding of the fuzzy c-means (FCM) clustering program. The FCM program is applicable to a wide variety of geostatistical data analysis problems. This program generates fuzzy partitions and prototypes for any set of numerical data. These partitions are useful for corroborating known substructures or suggesting substructure in unexplored data. The clustering criterion used to aggregate subsets is a generalized least-squares objective function. Features of this program include a choice of three norms (Euclidean, Diagonal, or Mahalonobis), an adjustable weighting factor that essentially controls sensitivity to noise, acceptance of variable numbers of clusters, and outputs that include several measures of cluster validity.},
  keywords = {done}
}

@book{bickel2015,
  title = {Mathematical Statistics: Basic Ideas and Selected Topics, Volume I, Second Edition: 1},
  shorttitle = {Mathematical Statistics},
  author = {Bickel, Peter J. and Doksum, Kjell A.},
  date = {2015-04-13},
  publisher = {{Chapman and Hall/CRC}},
  location = {{Boca Raton}},
  abstract = {Mathematical Statistics: Basic Ideas and Selected Topics, Volume I, Second Edition presents fundamental, classical statistical concepts at the doctorate level. It covers estimation, prediction, testing, confidence sets, Bayesian analysis, and the general approach of decision theory. This edition gives careful proofs of major results and explains how the theory sheds light on the properties of practical methods.   The book first discusses non- and semiparametric models before covering parameters and parametric models. It then offers a detailed treatment of maximum likelihood estimates (MLEs) and examines the theory of testing and confidence regions, including optimality theory for estimation and elementary robustness considerations. It next presents basic asymptotic approximations with one-dimensional parameter models as examples. The book also describes inference in multivariate (multiparameter) models, exploring asymptotic normality and optimality of MLEs, Wald and Rao statistics, generalized linear models, and more.  Mathematical Statistics: Basic Ideas and Selected Topics, Volume II will be published in 2015. It will present important statistical concepts, methods, and tools not covered in Volume I.},
  isbn = {978-1-4987-2380-0},
  langid = {Inglese},
  pagetotal = {576}
}

@book{billingsley2012,
  title = {Probability and {{Measure}}},
  author = {Billingsley, Patrick},
  date = {2012-02-28},
  edition = {Anniversary edition},
  publisher = {{Wiley}},
  location = {{Hoboken, N.J}},
  abstract = {Praise for the Third Edition "It is, as far as I'm concerned, among the best books in math ever written....if you are a mathematician and want to have the top reference in probability, this is it." (Amazon.com, January 2006) A complete and comprehensive classic in probability and measure theory Probability and Measure, Anniversary Edition by Patrick Billingsley celebrates the achievements and advancements that have made this book a classic in its field for the past 35 years. Now re-issued in a new style and format, but with the reliable content that the third edition was revered for, this Anniversary Edition builds on its strong foundation of measure theory and probability with Billingsley's unique writing style. In recognition of 35 years of publication, impacting tens of thousands of readers, this Anniversary Edition has been completely redesigned in a new, open and user-friendly way in order to appeal to university-level students. This book adds a new foreward by Steve Lally of the Statistics Department at The University of Chicago in order to underscore the many years of successful publication and world-wide popularity and emphasize the educational value of this book. The Anniversary Edition contains features including:   An improved treatment of Brownian motion   Replacement of queuing theory with ergodic theory   Theory and applications used to illustrate real-life situations   Over 300 problems with corresponding, intensive notes and solutions   Updated bibliography   An extensive supplement of additional notes on the problems and chapter commentaries   Patrick Billingsley was a first-class, world-renowned authority in probability and measure theory at a leading U.S. institution of higher education. He continued to be an influential probability theorist until his unfortunate death in 2011. Billingsley earned his Bachelor's Degree in Engineering from the U.S. Naval Academy where he served as an officer. he went on to receive his Master's Degree and doctorate in Mathematics from Princeton University.Among his many professional awards was the Mathematical Association of America's Lester R. Ford Award for mathematical exposition. His achievements through his long and esteemed career have solidified Patrick Billingsley's place as a leading authority in the field and been a large reason for his books being regarded as classics. This Anniversary Edition of Probability and Measure offers advanced students, scientists, and engineers an integrated introduction to measure theory and probability. Like the previous editions, this Anniversary Edition is a key resource for students of mathematics, statistics, economics, and a wide variety of disciplines that require a solid understanding of probability theory.},
  isbn = {978-1-118-12237-2},
  langid = {english},
  pagetotal = {656}
}

@book{bishop2006,
  title = {Pattern Recognition And Machine Learning},
  author = {Bishop, Christopher M.},
  date = {2006-08-01},
  publisher = {{Springer Nature}},
  location = {{New York}},
  abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
  isbn = {978-0-387-31073-2},
  langid = {Inglese},
  pagetotal = {738}
}

@article{blackwell1973,
  title = {Ferguson {{Distributions Via Polya Urn Schemes}}},
  author = {Blackwell, David and MacQueen, James B.},
  date = {1973-03},
  journaltitle = {The Annals of Statistics},
  volume = {1},
  pages = {353--355},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176342372},
  abstract = {The Polya urn scheme is extended by allowing a continuum of colors. For the extended scheme, the distribution of colors after \$n\$ draws is shown to converge as \$n \textbackslash rightarrow \textbackslash infty\$ to a limiting discrete distribution \$\textbackslash mu\^\textbackslash ast\$. The distribution of \$\textbackslash mu\^\textbackslash ast\$ is shown to be one introduced by Ferguson and, given \$\textbackslash mu\^\textbackslash ast\$, the colors drawn from the urn are shown to be independent with distribution \$\textbackslash mu\^\textbackslash ast\$.},
  number = {2}
}

@online{blazquez-garcia2020,
  title = {A Review on Outlier/Anomaly Detection in Time Series Data},
  author = {Blázquez-García, Ane and Conde, Angel and Mori, Usue and Lozano, Jose A.},
  date = {2020-02-11},
  abstract = {Recent advances in technology have brought major breakthroughs in data collection, enabling a large amount of data to be gathered over time and thus generating time series. Mining this data has become an important task for researchers and practitioners in the past few years, including the detection of outliers or anomalies that may represent errors or events of interest. This review aims to provide a structured and comprehensive state-of-the-art on outlier detection techniques in the context of time series. To this end, a taxonomy is presented based on the main aspects that characterize an outlier detection technique.},
  archiveprefix = {arXiv},
  eprint = {2002.04236},
  eprinttype = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,todo},
  primaryclass = {cs, stat}
}

@article{blei2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  date = {2017-04-03},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {112},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  archiveprefix = {arXiv},
  eprint = {1601.00670},
  eprinttype = {arxiv},
  keywords = {Algorithms; Statistical Computing; Computationally Intensive Methods,Computer Science - Machine Learning,doing,Statistics - Computation,Statistics - Machine Learning,variational inference},
  number = {518}
}

@article{blum1954,
  title = {Multidimensional {{Stochastic Approximation Methods}}},
  author = {Blum, Julius R.},
  date = {1954-12},
  journaltitle = {The Annals of Mathematical Statistics},
  shortjournal = {Ann. Math. Statist.},
  volume = {25},
  pages = {737--744},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177728659},
  keywords = {todo},
  langid = {english},
  number = {4}
}

@book{bobrowski2005,
  title = {Functional {{Analysis}} for {{Probability}} and {{Stochastic Processes}}: {{An Introduction}}},
  shorttitle = {Functional {{Analysis}} for {{Probability}} and {{Stochastic Processes}}},
  author = {Bobrowski, Adam},
  date = {2005-09-12},
  edition = {1 edition},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, UK ; New York}},
  abstract = {Designed for students of probability and stochastic processes, as well as for students of functional analysis, specifically, this volume presents some chosen parts of functional analysis that can help clarify probability and stochastic processes. The subjects range from basic Hilbert and Banach spaces, through weak topologies and Banach algebras, to the theory of semigroups of bounded linear operators. Numerous standard and non-standard examples and exercises make the book suitable as a course textbook or for self-study.},
  isbn = {978-0-521-53937-1},
  langid = {english},
  pagetotal = {406}
}

@report{bodoia,
  title = {{{MapReduce Algorithms}} for K-Means {{Clustering}}},
  author = {Bodoia, Max},
  pages = {11},
  keywords = {done},
  langid = {english}
}

@book{boothby2002,
  title = {An {{Introduction}} to {{Differentiable Manifolds}} and {{Riemannian Geometry}}, {{Revised}}},
  author = {Boothby, William M.},
  date = {2002-08-19},
  edition = {2 edition},
  publisher = {{Academic Press}},
  location = {{Amsterdam ; New York}},
  abstract = {The second edition of An Introduction to Differentiable Manifolds and Riemannian Geometry, Revised has sold over 6,000 copies since publication in 1986 and this revision will make it even more useful. This is the only book available that is approachable by "beginners" in this subject. It has become an essential introduction to the subject for mathematics students, engineers, physicists, and economists who need to learn how to apply these vital methods. It is also the only book that thoroughly reviews certain areas of advanced calculus that are necessary to understand the subject.},
  isbn = {978-0-12-116051-7},
  langid = {english},
  pagetotal = {440}
}

@article{botev2017,
  title = {The {{Normal Law Under Linear Restrictions}}: {{Simulation}} and {{Estimation}} via {{Minimax Tilting}}},
  shorttitle = {The {{Normal Law Under Linear Restrictions}}},
  author = {Botev, Z. I.},
  date = {2017-01},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  shortjournal = {J. R. Stat. Soc. B},
  volume = {79},
  pages = {125--148},
  issn = {13697412},
  doi = {10.1111/rssb.12162},
  abstract = {Simulation from the truncated multivariate normal distribution in high dimensions is a recurrent problem in statistical computing, and is typically only feasible using approximate MCMC sampling. In this article we propose a minimax tilting method for exact iid simulation from the truncated multivariate normal distribution. The new methodology provides both a method for simulation and an efficient estimator to hitherto intractable Gaussian integrals. We prove that the estimator possesses a rare vanishing relative error asymptotic property. Numerical experiments suggest that the proposed scheme is accurate in a wide range of setups for which competing estimation schemes fail. We give an application to exact iid simulation from the Bayesian posterior of the probit regression model.},
  archiveprefix = {arXiv},
  eprint = {1603.04166},
  eprinttype = {arxiv},
  keywords = {65C05; 68W20,done,Statistics - Computation},
  number = {1}
}

@incollection{boucheron2004,
  title = {Concentration {{Inequalities}}},
  booktitle = {Advanced {{Lectures}} on {{Machine Learning}}: {{ML Summer Schools}} 2003, {{Canberra}}, {{Australia}}, {{February}} 2 - 14, 2003, {{Tübingen}}, {{Germany}}, {{August}} 4 - 16, 2003, {{Revised Lectures}}},
  author = {Boucheron, Stéphane and Lugosi, Gábor and Bousquet, Olivier},
  editor = {Bousquet, Olivier and von Luxburg, Ulrike and Rätsch, Gunnar},
  date = {2004},
  pages = {208--240},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-28650-9_9},
  abstract = {Concentration inequalities deal with deviations of functions of independent random variables from their expectation. In the last decade new tools have been introduced making it possible to establish simple and powerful inequalities. These inequalities are at the heart of the mathematical analysis of various problems in machine learning and made it possible to derive new efficient algorithms. This text attempts to summarize some of the basic tools.},
  isbn = {978-3-540-28650-9},
  keywords = {Conditional Entropy,Empirical Process,Independent Random Variable,Moment Generate Function,Relative Entropy,todo},
  langid = {english},
  options = {useprefix=true},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{box1951,
  title = {On the {{Experimental Attainment}} of {{Optimum Conditions}}},
  author = {Box, G. E. P. and Wilson, K. B.},
  date = {1951},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {13},
  pages = {1--45},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  eprint = {2983966},
  eprinttype = {jstor},
  keywords = {todo},
  number = {1}
}

@book{boyd2004,
  title = {Convex {{Optimization}}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  date = {2004},
  edition = {1 edition},
  publisher = {{Cambridge University Press}},
  abstract = {Convex optimization problems arise frequently in many different fields. This book provides a comprehensive introduction to the subject, and shows in detail how such problems can be solved numerically with great efficiency. The book begins with the basic elements of convex sets and functions, and then describes various classes of convex optimization problems. Duality and approximation techniques are then covered, as are statistical estimation techniques. Various geometrical problems are then presented, and there is detailed discussion of unconstrained and constrained minimization problems, and interior-point methods. The focus of the book is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. It contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance and economics.},
  langid = {english},
  pagetotal = {742}
}

@book{breiman1984,
  title = {Classification and Regression Trees},
  author = {Breiman, Leo and Friedman, Jerome and Olshen, Richard Avery and Stone, Charles John},
  date = {1984-01-01},
  edition = {1 edizione},
  publisher = {{Chapman and Hall/CRC}},
  location = {{Boca Raton}},
  abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
  isbn = {978-0-412-04841-8},
  langid = {Inglese},
  pagetotal = {368}
}

@article{breiman1996,
  title = {Bagging Predictors},
  author = {Breiman, Leo},
  date = {1996-08},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {24},
  pages = {123--140},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00058655},
  keywords = {done},
  langid = {english},
  number = {2}
}

@article{breiman2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  date = {2001},
  journaltitle = {Machine Learning},
  volume = {45},
  pages = {5--32},
  issn = {08856125},
  doi = {10.1023/A:1010933404324},
  keywords = {done},
  number = {1}
}

@article{breiman2001a,
  title = {Statistical {{Modeling}}: {{The Two Cultures}} (with Comments and a Rejoinder by the Author)},
  shorttitle = {Statistical {{Modeling}}},
  author = {Breiman, Leo},
  date = {2001},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {16},
  pages = {199--231},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1009213726},
  abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  keywords = {done},
  langid = {english},
  mrnumber = {MR1874152},
  number = {3},
  zmnumber = {1059.62505}
}

@book{brezis2010,
  title = {Functional Analysis, Sobolev Spaces and Partial Differential Equations},
  author = {Brezis, Haim},
  date = {2010-11-10},
  edition = {2011 edizione},
  publisher = {{Springer}},
  location = {{New York ; London}},
  abstract = {This is a completely revised English edition of the important Analyse fonctionnelle (1983). It contains a wealth of problems and exercises to guide the reader. It is also the first single-volume textbook to cover related fields of functional analysis and PDEs.},
  isbn = {978-0-387-70913-0},
  langid = {Inglese},
  pagetotal = {616}
}

@book{brooks2011,
  title = {Handbook of Markov Chain Monte Carlo},
  author = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  date = {2011-05-13},
  edition = {1 edizione},
  publisher = {{Chapman and Hall/CRC}},
  location = {{Boca Raton}},
  abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisheries science and economics. The wide-ranging practical importance of MCMC has sparked an expansive and deep investigation into fundamental Markov chain theory.  The Handbook of Markov Chain Monte Carlo provides a reference for the broad audience of developers and users of MCMC methodology interested in keeping up with cutting-edge theory and applications. The first half of the book covers MCMC foundations, methodology, and algorithms. The second half considers the use of MCMC in a variety of practical applications including in educational research, astrophysics, brain imaging, ecology, and sociology.  The in-depth introductory section of the book allows graduate students and practicing scientists new to MCMC to become thoroughly acquainted with the basic theory, algorithms, and applications. The book supplies detailed examples and case studies of realistic scientific problems presenting the diversity of methods used by the wide-ranging MCMC community. Those familiar with MCMC methods will find this book a useful refresher of current theory and recent developments.},
  isbn = {978-1-4200-7941-8},
  langid = {Inglese},
  pagetotal = {619}
}

@article{buchholz2008,
  title = {On Properties of Predictors Derived with a Two-Step Bootstrap Model Averaging Approach—{{A}} Simulation Study in the Linear Regression Model},
  author = {Buchholz, Anika and Holländer, Norbert and Sauerbrei, Willi},
  date = {2008-01},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  volume = {52},
  pages = {2778--2793},
  issn = {01679473},
  doi = {10.1016/j.csda.2007.10.007},
  keywords = {todo},
  langid = {english},
  number = {5}
}

@article{butz1969,
  title = {Convergence with {{Hilbert}}'s Space Filling Curve},
  author = {Butz, Arthur R.},
  date = {1969-05},
  journaltitle = {Journal of Computer and System Sciences},
  shortjournal = {Journal of Computer and System Sciences},
  volume = {3},
  pages = {128--146},
  issn = {00220000},
  doi = {10.1016/S0022-0000(69)80010-3},
  keywords = {todo},
  langid = {english},
  number = {2}
}

@article{canale2011,
  title = {Bayesian {{Kernel Mixtures}} for {{Counts}}},
  author = {Canale, Antonio and Dunson, David B.},
  date = {2011-12-01},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {J Am Stat Assoc},
  volume = {106},
  pages = {1528--1539},
  issn = {0162-1459},
  doi = {10.1198/jasa.2011.tm10552},
  abstract = {Although Bayesian nonparametric mixture models for continuous data are well developed, there is a limited literature on related approaches for count data. A common strategy is to use a mixture of Poissons, which unfortunately is quite restrictive in not accounting for distributions having variance less than the mean. Other approaches include mixing multinomials, which requires finite support, and using a Dirichlet process prior with a Poisson base measure, which does not allow smooth deviations from the Poisson. As a broad class of alternative models, we propose to use nonparametric mixtures of rounded continuous kernels. An efficient Gibbs sampler is developed for posterior computation, and a simulation study is performed to assess performance. Focusing on the rounded Gaussian case, we generalize the modeling framework to account for multivariate count data, joint modeling with continuous and categorical variables, and other complications. The methods are illustrated through applications to a developmental toxicity study and marketing data. This article has .},
  eprint = {22523437},
  eprinttype = {pmid},
  keywords = {done},
  number = {496},
  pmcid = {PMC3329131}
}

@incollection{canale2016,
  title = {Bayesian {{Nonparametrics}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Canale, Antonio and Lijoi, Antonio and Prünster, Igor},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2016-08-05},
  pages = {1--11},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat07850},
  isbn = {978-1-118-44511-2},
  keywords = {todo},
  langid = {english}
}

@article{canale2016a,
  title = {Constrained Functional Time Series: {{Applications}} to the {{Italian}} Gas Market},
  shorttitle = {Constrained Functional Time Series},
  author = {Canale, Antonio and Vantini, Simone},
  date = {2016-10-01},
  journaltitle = {International Journal of Forecasting},
  shortjournal = {International Journal of Forecasting},
  volume = {32},
  pages = {1340--1351},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2016.05.002},
  abstract = {Motivated by market dynamic modelling in the Italian Natural Gas Balancing Platform, we propose a model for analyzing time series of functions, subject to equality and inequality constraints at the two edges of the domain, respectively, such as daily demand and offer curves. Specifically, we provide the constrained functions with suitable pre-Hilbert structures, and introduce a useful isometric bijective map that associates each possible bounded and monotonic function to an unconstrained one. We introduce a functional-to-functional autoregressive model that is used to forecast future demand/offer functions, and estimate the model via the minimization of a penalized mean squared error of prediction, with a penalty term based on the Hilbert–Schmidt squared norm of autoregressive lagged operators. The approach is of general interest and could be generalized to any situation in which one has to deal with functions that are subject to the above constraints which evolve over time.},
  keywords = {Autoregressive model,Demand and offer model,done,Energy forecasting,Functional data analysis,Functional ridge regression},
  langid = {english},
  number = {4}
}

@article{canale2016b,
  title = {Multiscale {{Bernstein}} Polynomials for Densities},
  author = {Canale, Antonio and Dunson, David B.},
  date = {2016},
  journaltitle = {Statistica Sinica},
  volume = {26},
  pages = {1175--1195},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  abstract = {Our focus is on constructing a multiscale nonparametric prior for densities. The Bayes density estimation literature is dominated by single scale methods, with the exception of Polya trees, which favor overly-spiky densities even when the truth is smooth. We propose a multiscale Bernstein polynomial family of priors, which produce smooth realizations that do not rely on hard partitioning of the support. At each level in an infinitely-deep binary tree, we place a beta dictionary density; within a scale the densities are equivalent to Bernstein polynomials. Using a stick-breaking characterization, stochastically decreasing weights are allocated to the finer scale dictionary elements. A slice sampler is used for posterior computation, and properties are described. The method characterizes densities with locally-varying smoothness, and can produce a sequence of coarse to fine density estimates. An extension for Bayesian testing of group differences is introduced and applied to DNA methylation array data.},
  eprint = {24721271},
  eprinttype = {jstor},
  keywords = {done},
  number = {3}
}

@article{canale2017,
  title = {{{msBP}}: {{An R Package}} to {{Perform Bayesian Nonparametric Inference Using Multiscale Bernstein Polynomials Mixtures}}},
  shorttitle = {{{msBP}}},
  author = {Canale, Antonio},
  date = {2017-06-07},
  journaltitle = {Journal of Statistical Software},
  volume = {78},
  pages = {1--19},
  issn = {1548-7660},
  doi = {10.18637/jss.v078.i06},
  issue = {1},
  keywords = {binary trees,density estimation,multiscale stick-breaking,multiscale testing,todo},
  langid = {english},
  number = {1}
}

@article{capizzi2003,
  title = {An {{Adaptive Exponentially Weighted Moving Average Control Chart}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  date = {2003},
  journaltitle = {Technometrics},
  volume = {45},
  pages = {199--207},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  abstract = {Lucas and Saccucci showed that exponentially weighted moving average (EWMA) control charts can be designed to quickly detect either small or large shifts in the mean of a sequence of independent observations. But a single EWMA chart cannot perform well for small and large shifts simultaneously. Furthermore, in the worst-case situation, this scheme requires a few observations to overcome its initial inertia. The main goal of this article is to suggest an adaptive EWMA (AEWMA) chart that weights the past observations of the monitored process using a suitable function of the current "error." The resulting scheme can be viewed as a smooth combination of a Shewhart chart and an EWMA chart. A design procedure for the new control schemes is suggested. Comparisons of the standard and worst-case average run length profiles of the new scheme with those of different control charts show that AEWMA schemes offer a more balanced protection against shifts of different sizes.},
  eprint = {25047047},
  eprinttype = {jstor},
  keywords = {done},
  number = {3}
}

@article{capizzi2011,
  title = {A {{Least Angle Regression Control Chart}} for {{Multidimensional Data}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  date = {2011-08-01},
  journaltitle = {Technometrics},
  volume = {53},
  pages = {285--296},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/TECH.2011.10027},
  abstract = {In multidimensional applications, it is very rare that all variables shift at the same time. A statistical process control procedure would have superior efficiency when limited to the subset of variables likely responsible for the out-of-control conditions. The key idea of this article consists of combining a variable selection method with a multivariate control chart to detect changes in both the mean and variability of a multidimensional process with Gaussian errors. In particular, we develop a control chart for Phase II monitoring which integrates the least angle regression algorithm with a multivariate exponentially weighted moving average. Comparisons with related multivariate control schemes demonstrate the efficiency of the proposed control chart in a wide range of practical applications, including profile and multistage process monitoring. Further, the proposed scheme may also provide valuable diagnostic information for fault isolation. Supplemental materials, including an R package, are available online.},
  annotation = {\_eprint: https://doi.org/10.1198/TECH.2011.10027},
  keywords = {Change-point detection,done,Exponentially weighted moving average,Multistage process,Profile monitoring,Statistical process control,Variable selection},
  number = {3}
}

@article{capizzi2012,
  title = {An {{Enhanced Control Chart}} for {{Start}}-{{Up Processes}} and {{Short Runs}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  date = {2012-01},
  journaltitle = {Quality Technology \& Quantitative Management},
  shortjournal = {Quality Technology \& Quantitative Management},
  volume = {9},
  pages = {189--202},
  issn = {1684-3703},
  doi = {10.1080/16843703.2012.11673285},
  keywords = {todo},
  langid = {english},
  number = {2}
}

@article{capizzi2012a,
  title = {Adaptive {{Generalized Likelihood Ratio Control Charts}} for {{Detecting Unknown Patterned Mean Shifts}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  date = {2012-10},
  journaltitle = {Journal of Quality Technology},
  shortjournal = {Journal of Quality Technology},
  volume = {44},
  pages = {281--303},
  issn = {0022-4065, 2575-6230},
  doi = {10.1080/00224065.2012.11917902},
  keywords = {todo},
  langid = {english},
  number = {4}
}

@article{capizzi2013,
  title = {Phase {{I Distribution}}-{{Free Analysis}} of {{Univariate Data}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  date = {2013-07-01},
  journaltitle = {Journal of Quality Technology},
  shortjournal = {Journal of Quality Technology},
  volume = {45},
  pages = {273--284},
  doi = {10.1080/00224065.2013.11917938},
  abstract = {In Phase I analysis, data are used retrospectively for checking process stability and defining the in-control state. Most Phase I control charts are based on the assumption of normally distributed observations. However, distribution-free methods appear to be ideal candidates for Phase I applications. Indeed, because little information is available, it is difficult to validate a distributional assumption in Phase I or at least at its beginning stage. In addition, as has been noted in the literature, this assumption cannot be checked before process stability is established. In this article, we propose a new distribution-free Phase I procedure for univariate observations. The suggested method, based on recursive segmentation and permutation, detects single or multiple mean and/or scale shifts. A simulation study shows that our method compares favorably with parametric control charts when the process is normally distributed and performs better than other nonparametric control charts when the process distribution is skewed or heavy tailed. An R package can be found in the supplemental materials.},
  keywords = {done}
}

@article{capizzi2015,
  title = {Recent {{Advances}} in {{Process Monitoring}}: {{Nonparametric}} and {{Variable}}-{{Selection Methods}} for {{Phase I}} and {{Phase II}}},
  shorttitle = {Recent {{Advances}} in {{Process Monitoring}}},
  author = {Capizzi, Giovanna},
  date = {2015-01-02},
  journaltitle = {Quality Engineering},
  shortjournal = {Quality Engineering},
  volume = {27},
  pages = {44--67},
  issn = {0898-2112, 1532-4222},
  doi = {10.1080/08982112.2015.968046},
  keywords = {done},
  langid = {english},
  number = {1}
}

@article{capizzi2016,
  title = {Efficient Control Chart Calibration by Simulated Stochastic Approximation},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  date = {2016-01-02},
  journaltitle = {IIE Transactions},
  volume = {48},
  pages = {57--65},
  publisher = {{Taylor \& Francis}},
  issn = {0740-817X},
  doi = {10.1080/0740817X.2015.1055392},
  abstract = {The accurate determination of control limits is crucial in statistical process control. The usual approach consists in computing the limits so that the in-control run-length distribution has some desired properties; for example, a prescribed mean. However, as a consequence of the increasing complexity of process data, the run-length of many control charts discussed in the recent literature can be studied only through simulation. Furthermore, in some scenarios, such as profile and autocorrelated data monitoring, the limits cannot be tabulated in advance, and when different charts are combined, the control limits depend on a multidimensional vector of parameters. In this article, we propose the use of stochastic approximation methods for control chart calibration and discuss enhancements for their implementation (e.g., the initialization of the algorithm, an adaptive choice of the gain, a suitable stopping rule for the iterative process, and the advantages of using multicore workstations). Examples are used to show that simulated stochastic approximation provides a reliable and fully automatic approach for computing the control limits in complex applications. An R package implementing the algorithm is available in the supplemental materials.},
  annotation = {\_eprint: https://doi.org/10.1080/0740817X.2015.1055392},
  keywords = {control charts,done,multi-chart schemes,statistical process control,Stochastic approximation,stochastic root finding},
  number = {1}
}

@article{capizzi2017,
  title = {Phase {{I Distribution}}-{{Free Analysis}} of {{Multivariate Data}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  date = {2017-10-02},
  journaltitle = {Technometrics},
  shortjournal = {null},
  volume = {59},
  pages = {484--495},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2016.1272494},
  abstract = {In this study, a new distribution-free Phase I control chart for retrospectively monitoring multivariate data is developed. The suggested approach, based on the multivariate signed ranks, can be applied to individual or subgrouped data for detection of location shifts with an arbitrary pattern (e.g., isolated, transitory, sustained, progressive, etc.). The procedure is complemented with a LASSO-based post-signal diagnostic method for identification of the shifted variables. A simulation study shows that the method compares favorably with parametric control charts when the process is normally distributed, and largely outperforms other multivariate nonparametric control charts when the process distribution is skewed or heavy-tailed. An R package can be found in the supplementary material.},
  keywords = {done},
  number = {4}
}

@article{capizzi2020,
  title = {Guaranteed In-Control Control Chart Performance with Cautious Parameter Learning},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  date = {2020-10-01},
  journaltitle = {Journal of Quality Technology},
  volume = {52},
  pages = {385--403},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2019.1640096},
  abstract = {Parameter estimation has a large impact on control chart performance. Recently, widened control limits have been suggested to guarantee an acceptable in-control behavior. However, the consequence is a reduced ability to detect a real change in the process. In order to overcome this undesired effect, we explore an alternative design based on a delayed updating of parameter estimates. We consider an application to the Shewhart X, EWMA, and CUSUM control charts for the process mean. This approach is simple to implement, reduces the variation of the in-control average run lengths, and significantly improves the out-of-control performance.},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2019.1640096},
  keywords = {control limits,done,estimation effects,recursive estimation,statistical process control,statistical process monitoring},
  number = {4}
}

@article{caron2008,
  title = {Bayesian {{Inference}} for {{Linear Dynamic Models With Dirichlet Process Mixtures}}},
  author = {Caron, F. and Davy, M. and Doucet, A. and Duflos, E. and Vanheeghe, P.},
  date = {2008-01},
  journaltitle = {IEEE Transactions on Signal Processing},
  volume = {56},
  pages = {71--84},
  issn = {1941-0476},
  doi = {10.1109/TSP.2007.900167},
  abstract = {Using Kalman techniques, it is possible to perform optimal estimation in linear Gaussian state-space models. Here, we address the case where the noise probability density functions are of unknown functional form. A flexible Bayesian nonparametric noise model based on Dirichlet process mixtures is introduced. Efficient Markov chain Monte Carlo and sequential Monte Carlo methods are then developed to perform optimal batch and sequential estimation in such contexts. The algorithms are applied to blind deconvolution and change point detection. Experimental results on synthetic and real data demonstrate the efficiency of this approach in various contexts.},
  eventtitle = {{{IEEE Transactions}} on {{Signal Processing}}},
  keywords = {Bayesian methods,Bayesian nonparametrics,Change detection algorithms,Dirichlet process mixture (DPM),Filtering,Gaussian noise,Kalman filters,Markov chain Monte Carlo (MCMC),Monte Carlo methods,Noise shaping,particle filter,Probability density function,Rao-Blackwellization,Rao–Blackwellization,Smoothing methods,State estimation,todo},
  number = {1}
}

@article{carvalho2010,
  title = {The Horseshoe Estimator for Sparse Signals},
  author = {Carvalho, C. M. and Polson, N. G. and Scott, J. G.},
  date = {2010-06-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {97},
  pages = {465--480},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asq017},
  keywords = {done},
  langid = {english},
  number = {2}
}

@book{casella2001,
  title = {Statistical Inference},
  author = {Casella, George and Berger, Roger L.},
  date = {2001-07-18},
  edition = {2 edizione},
  publisher = {{Duxbury Pr}},
  location = {{Australia ; Pacific Grove, CA}},
  abstract = {This book builds theoretical statistics from the first principles of probability theory. Starting from the basics of probability, the authors develop the theory of statistical inference using techniques, definitions, and concepts that are statistical and are natural extensions and consequences of previous concepts. Intended for first-year graduate students, this book can be used for students majoring in statistics who have a solid mathematics background. It can also be used in a way that stresses the more practical uses of statistical theory, being more concerned with understanding basic statistical concepts and deriving reasonable statistical procedures for a variety of situations, and less concerned with formal optimality investigations.},
  isbn = {978-0-534-24312-8},
  langid = {Inglese},
  pagetotal = {660}
}

@incollection{cavanaugh2016,
  title = {Model {{Selection}}: {{Bayesian Information Criterion}}},
  shorttitle = {Model {{Selection}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Cavanaugh, Joseph E.},
  date = {2016},
  pages = {1--3},
  publisher = {{American Cancer Society}},
  doi = {10.1002/9781118445112.stat00247.pub2},
  abstract = {This article presents an overview of the Bayesian information criterion (BIC), along with its motivation and some of its asymptotic optimality properties. It also compares and contrasts the criterion to the Akaike information criterion (AIC).},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat00247.pub2},
  isbn = {978-1-118-44511-2},
  keywords = {Akaike information criterion,Bayes factors,model selection criterion,Schwarz information criterion,todo},
  langid = {english}
}

@book{ceron2013,
  title = {Social Media e Sentiment Analysis: L'evoluzione dei fenomeni sociali attraverso la Rete},
  shorttitle = {Social Media e Sentiment Analysis},
  author = {Ceron, Andrea and Curini, Luigi and Iacus, Stefano Maria},
  date = {2013-05-12},
  publisher = {{Springer}},
  abstract = {Due miliardi e mezzo di utenti internet, oltre un miliardo di account Facebook, 550 milioni di profili Twitter. Che parlano, discutono, si confrontano sui temi più svariati. Un flusso in continuo divenire di informazioni che dà sostanza ogni giorno al mondo dei Big Data. Ma come si analizza concretamente il "sentiment" della Rete? Quali sono i pregi e i limiti dei diversi metodi esistenti? E a quali domande possiamo dare una risposta? Dopo aver presentato le varie tecniche di analisi testuale applicate ai social media, questo libro discute di come l'informazione presente in Rete sia in grado di aiutarci a meglio comprendere il presente e a fare previsioni sul futuro riguardo a una molteplicità di fenomeni sociali, che spaziano dall'andamento dei mercati finanziari, alla diffusione di malattie, alle rivolte e ai sommovimenti popolari fino ai risultati dei talent show, prima di concentrarsi su due casi specifici: l'andamento della felicità degli italiani giorno per giorno, e i risultati delle campagne elettorali in Francia, Stati Uniti e Italia tra il 2012 e il 2013.},
  isbn = {978-88-470-5531-5},
  langid = {Italiano},
  pagetotal = {152}
}

@article{ceron2016,
  title = {{{iSA}}: {{A}} Fast, Scalable and Accurate Algorithm for Sentiment Analysis of Social Media Content},
  shorttitle = {{{iSA}}},
  author = {Ceron, Andrea and Curini, Luigi and Iacus, Stefano Maria},
  date = {2016-11-01},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {367-368},
  pages = {105--124},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2016.05.052},
  abstract = {We present iSA (integrated sentiment analysis), a novel algorithm designed for social networks and Web 2.0 sphere (Twitter, blogs, etc.) opinion analysis, i.e. developed for the digital environments characterized by abundance of noise compared to the amount of information. Instead of performing an individual classification and then aggregate the predicted values, iSA directly estimates the aggregated distribution of opinions. Based on supervised hand-coding rather than NLP techniques or ontological dictionaries, iSA is a language-agnostic algorithm (based on human coders’ abilities). iSA exploits a dimensionality reduction approach which makes it scalable, fast, memory efficient, stable and statistically accurate. The cross-tabulation of opinions is possible with iSA thanks to its stability. Through empirical analysis it will be shown when iSA outperforms machine learning techniques of individual classification (e.g. SVM, Random Forests, etc) as well as the only other alternative for aggregated sentiment analysis known as ReadMe.},
  keywords = {done,Opinion mining,Sentiment analysis,Twitter analysis},
  langid = {english}
}

@book{ceron2016a,
  title = {Politics and {{Big Data}}: {{Nowcasting}} and {{Forecasting Elections}} with {{Social Media}}},
  shorttitle = {Politics and {{Big Data}}},
  author = {Ceron, Andrea and Curini, Luigi and Iacus, Stefano Maria},
  date = {2016},
  publisher = {{Routledge}},
  location = {{London ; New York, NY}},
  abstract = {The importance of social media as a way to monitor an electoral campaign is well established. Day-by-day, hour-by-hour evaluation of the evolution of online ideas and opinion allows observers and scholars to monitor trends and momentum in public opinion well before traditional polls. However, there are difficulties in recording and analyzing often brief, unverified comments while the unequal age, gender, social and racial representation among social media users can produce inaccurate forecasts of final polls. Reviewing the different techniques employed using social media to nowcast and forecast elections, this book assesses its achievements and limitations while presenting a new technique of "sentiment analysis" to improve upon them. The authors carry out a meta-analysis of the existing literature to show the conditions under which social media-based electoral forecasts prove most accurate while new case studies from France, the United States and Italy demonstrate how much more accurate "sentiment analysis" can prove.},
  isbn = {978-1-4724-6666-2},
  pagetotal = {188}
}

@article{cheng2014,
  title = {A {{Distribution}}-{{Free Multivariate Control Chart}} for {{Phase I Applications}}},
  author = {Cheng, Ching-Ren and Shiau, Jyh-Jen Horng},
  date = {2014},
  journaltitle = {Quality and Reliability Engineering International},
  volume = {31},
  pages = {97--111},
  issn = {1099-1638},
  doi = {10.1002/qre.1751},
  abstract = {The purpose of this paper is to provide a novel distribution-free control chart for monitoring the location parameter vector of a multivariate process in phase I analysis. To be robust to the process distribution, the spatial sign statistic that defines the multivariate direction of an observation is used to construct a Shewhart-type control chart for detecting out-of-control observations in historical phase I data. The proposed control chart is distribution free in the sense that the false-positive rate (or false alarm rate), the proportion of wrongly classified in-control samples, can be controlled at the specified value for elliptical-direction distributions. In addition, we demonstrate through simulation studies that the false-positive rate of the proposed chart is robust to the shift size of the out-of-control condition if we only delete the most extreme out-of-control observation at each iteration of phase I analysis. Compared with the traditional Hotelling's T2 control chart and some of its robust versions, the proposed chart is generally more powerful in detecting out-of-control observations and more robust to the normality assumption. Copyright © 2014 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.1751},
  keywords = {distribution free,done,multivariate control chart,NSPC,phase I analysis,spatial sign},
  langid = {english},
  number = {1}
}

@article{chiou2016,
  title = {Multivariate Functional Linear Regression and Prediction},
  author = {Chiou, Jeng-Min and Yang, Ya-Fang and Chen, Yu-Ting},
  date = {2016-04-01},
  journaltitle = {Journal of Multivariate Analysis},
  shortjournal = {Journal of Multivariate Analysis},
  volume = {146},
  pages = {301--312},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2015.10.003},
  abstract = {We propose a multivariate functional linear regression (mFLR) approach to analysis and prediction of multivariate functional data in cases in which both the response and predictor variables contain multivariate random functions. The mFLR model, coupled with the multivariate functional principal component analysis approach, takes the advantage of cross-correlation between component functions within the multivariate response and predictor variables, respectively. The estimate of the matrix of bivariate regression functions is consistent in the sense of the multi-dimensional Gram–Schmidt norm and is asymptotically normally distributed. The prediction intervals of the multivariate random trajectories are available for predictive inference. We show the finite sample performance of mFLR by a simulation study and illustrate the method through predicting multivariate traffic flow trajectories for up-to-date and partially observed traffic streams.},
  keywords = {doing,Functional prediction,Functional principal component analysis,Functional regression,Multivariate functional data,Stochastic processes},
  langid = {english},
  series = {Special {{Issue}} on {{Statistical Models}} and {{Methods}} for {{High}} or {{Infinite Dimensional Spaces}}}
}

@book{chopin2020,
  title = {An {{Introduction}} to {{Sequential Monte Carlo}}},
  author = {Chopin, Nicolas and Papaspiliopoulos, Omiros},
  date = {2020},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-47845-2},
  abstract = {This book provides a general introduction to Sequential Monte Carlo (SMC) methods, also known as particle filters. These methods have become a staple for the sequential analysis of data in such diverse fields as signal processing, epidemiology, machine learning, population ecology, quantitative finance, and robotics.The coverage is comprehensive, ranging from the underlying theory to computational implementation, methodology, and diverse applications in various areas of science. This is achieved by describing SMC algorithms as particular cases of a general framework, which involves concepts such as Feynman-Kac distributions, and tools such as importance sampling and resampling. This general framework is used consistently throughout the book.Extensive coverage is provided on sequential learning (filtering, smoothing) of state-space (hidden Markov) models, as this remains an important application of SMC methods. More recent applications, such as parameter estimation of these models (through e.g. particle Markov chain Monte Carlo techniques) and the simulation of challenging probability distributions (in e.g. Bayesian inference or rare-event problems), are also discussed.The book may be used either as a graduate text on Sequential Monte Carlo methods and state-space modeling, or as a general reference work on the area. Each chapter includes a set of exercises for self-study, a comprehensive bibliography, and a “Python corner,” which discusses the practical implementation of the methods covered. In addition, the book comes with an open source Python library, which implements all the algorithms described in the book, and contains all the programs that were used to perform the numerical experiments.},
  isbn = {978-3-030-47844-5},
  langid = {english},
  series = {Springer {{Series}} in {{Statistics}}}
}

@book{cifarelli1989,
  title = {Statistica Bayesiana: Appunti Ad Uso Degli Studenti},
  shorttitle = {Statistica Bayesiana},
  author = {Cifarelli, Donato Michele and Muliere, Pietro},
  date = {1989},
  publisher = {{Gianni Iuculano Editore}},
  isbn = {978-88-7072-114-0},
  keywords = {Statistica matematica bayesiana},
  pagetotal = {323}
}

@book{cinlar2011,
  title = {Probability and {{Stochastics}}},
  author = {Çinlar, Erhan},
  date = {2011},
  volume = {261},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-0-387-87859-1},
  isbn = {978-0-387-87858-4 978-0-387-87859-1},
  series = {Graduate {{Texts}} in {{Mathematics}}}
}

@book{cinlar2013,
  title = {Probability and Stochastics},
  author = {Çınlar, Erhan},
  date = {2013-04-19},
  publisher = {{Springer}},
  location = {{Place of publication not identified}},
  abstract = {Probability and Stochastics provides an introduction to the field, and begins by describing the fundamentals and basic principles of probability theory. Later chapters discuss more advanced topics, such as Martingales, Poisson Random Measures, Levy Processes and Brownian Motion.},
  isbn = {978-1-4614-2812-1},
  keywords = {todo},
  langid = {Inglese},
  pagetotal = {572}
}

@article{cook2006,
  title = {Validation of {{Software}} for {{Bayesian Models Using Posterior Quantiles}}},
  author = {Cook, Samantha R and Gelman, Andrew and Rubin, Donald B},
  date = {2006-09-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {null},
  volume = {15},
  pages = {675--692},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1198/106186006X136976},
  abstract = {This article presents a simulation-based method designed to establish the computational correctness of software developed to fit a specific Bayesian model, capitalizing on properties of Bayesian posterior distributions. We illustrate the validation technique with two examples. The validation method is shown to find errors in software when they exist and, moreover, the validation output can be informative about the nature and location of such errors. We also compare our method with that of an earlier approach.},
  keywords = {done},
  number = {3}
}

@book{cormen2009,
  title = {Introduction to {{Algorithms}}, 3rd {{Edition}}},
  author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
  date = {2009-07-31},
  edition = {3rd edition},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Mass}},
  abstract = {The latest edition of the essential text and professional reference, with substantial new material on such topics as vEB trees, multithreaded algorithms, dynamic programming, and edge-based flow.Some books on algorithms are rigorous but incomplete; others cover masses of material but lack rigor. Introduction to Algorithms uniquely combines rigor and comprehensiveness. The book covers a broad range of algorithms in depth, yet makes their design and analysis accessible to all levels of readers. Each chapter is relatively self-contained and can be used as a unit of study. The algorithms are described in English and in a pseudocode designed to be readable by anyone who has done a little programming. The explanations have been kept elementary without sacrificing depth of coverage or mathematical rigor.The first edition became a widely used text in universities worldwide as well as the standard reference for professionals. The second edition featured new chapters on the role of algorithms, probabilistic analysis and randomized algorithms, and linear programming. The third edition has been revised and updated throughout. It includes two completely new chapters, on van Emde Boas trees and multithreaded algorithms, substantial additions to the chapter on recurrence (now called “Divide-and-Conquer”), and an appendix on matrices. It features improved treatment of dynamic programming and greedy algorithms and a new notion of edge-based flow in the material on flow networks. Many exercises and problems have been added for this edition. The international paperback edition is no longer available; the hardcover is available worldwide.},
  isbn = {978-0-262-03384-8},
  langid = {english},
  pagetotal = {1292}
}

@article{cox1990,
  title = {Role of {{Models}} in {{Statistical Analysis}}},
  author = {Cox, D. R.},
  date = {1990},
  journaltitle = {Statistical Science},
  volume = {5},
  pages = {169--174},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  abstract = {A number of distinct roles are identified for probability models used in the analysis of data. Examples are outlined. Some general issues arising in the formulation of such models are discussed.},
  keywords = {done},
  number = {2}
}

@article{cox1994,
  title = {Some Comments on the Teaching of Stochastic Processes to Engineers},
  author = {Cox, David and Davison, Anthony},
  date = {1994-12-01},
  journaltitle = {International Journal of Continuing Engineering Education and Life-Long Learnin},
  shortjournal = {International Journal of Continuing Engineering Education and Life-Long Learnin},
  volume = {4},
  pages = {24--30},
  doi = {10.1504/IJCEELL.1994.030285},
  abstract = {The contribution of stochastic processes in modelling dynamic phenomenena is reviewed. The key ideas are identified, the importance of careful formulation is emphasised and the roles of analytical solution and simulation discussed. A simple example is investigated in a little detail.},
  keywords = {done}
}

@article{cox1997,
  title = {The {{Current Position}} of {{Statistics}}: {{A Personal View}}},
  shorttitle = {The {{Current Position}} of {{Statistics}}},
  author = {Cox, D. R.},
  date = {1997},
  journaltitle = {International Statistical Review},
  volume = {65},
  pages = {261--276},
  issn = {1751-5823},
  doi = {10.1111/j.1751-5823.1997.tb00305.x},
  abstract = {Some current aspects of statistical work are reviewed under three broad headings, applied probability modelling, design of investigations and statistical analysis and interpretation of data. The emphasis is on applications in science and science-based technology, although some incidental comments are made about statistics in public affairs. While no technical details are given, there is some discussion of potential fields for development. The choice of topics reflects the author's personal interests.},
  keywords = {Analysis of data,Applied probability models,Design of investigations,done,Formal inference,Government statistics,Stochastic models},
  langid = {english},
  number = {3}
}

@incollection{cressie2014,
  title = {Space-{{Time Kalman Filter}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Cressie, Noel and Wikle, Christopher K.},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2014-09-29},
  pages = {stat07813},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat07813},
  isbn = {978-1-118-44511-2},
  keywords = {done},
  langid = {english}
}

@article{cribari-neto2010,
  title = {Beta {{Regression}} in {{R}}},
  author = {Cribari-Neto, Francisco and Zeileis, Achim},
  date = {2010},
  journaltitle = {Journal of Statistical Software},
  volume = {34},
  pages = {1--24},
  issn = {1548-7660},
  doi = {10.18637/jss.v034.i02},
  keywords = {done},
  number = {1}
}

@article{crosier1988,
  title = {Multivariate {{Generalizations}} of {{Cumulative Sum Quality}}-{{Control Schemes}}},
  author = {Crosier, Ronald B.},
  date = {1988},
  journaltitle = {Technometrics},
  volume = {30},
  pages = {291--303},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1270083},
  abstract = {This article presents the design procedures and average run lengths for two multivariate cumulative sum (CUSUM) quality-control procedures. The first CUSUM procedure reduces each multivariate observation to a scalar and then forms a CUSUM of the scalars. The second CUSUM procedure forms a CUSUM vector directly from the observations. These two procedures are compared with each other and with the multivariate Shewhart chart. Other multivariate quality-control procedures are mentioned. Robustness, the fast initial response feature for CUSUM schemes, and combined Shewhart-CUSUM schemes are discussed.},
  eprint = {1270083},
  eprinttype = {jstor},
  keywords = {done},
  number = {3}
}

@book{dasgupta2011,
  title = {Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics},
  shorttitle = {Probability for Statistics and Machine Learning},
  author = {DasGupta, Anirban},
  date = {2011},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  abstract = {This book provides a versatile and lucid treatment of classic as well as modern probability theory, while integrating them with core topics in statistical theory and also some key tools in machine learning. It is written in an extremely accessible style, with elaborate motivating discussions and numerous worked out examples and exercises. The book has 20 chapters on a wide range of topics, 423 worked out examples, and 808 exercises. It is unique in its unification of probability and statistics, its coverage and its superb exercise sets, detailed bibliography, and in its substantive treatment of many topics of current importance.This book can be used as a text for a year long graduate course in statistics, computer science, or mathematics, for self-study, and as an invaluable research reference on probabiliity and its applications. Particularly worth mentioning are the treatments of distribution theory, asymptotics, simulation and Markov Chain Monte Carlo, Markov chains and martingales, Gaussian processes, VC theory, probability metrics, large deviations, bootstrap, the EM algorithm, confidence intervals, maximum likelihood and Bayes estimates, exponential families, kernels, and Hilbert spaces, and a self contained complete review of univariate probability.},
  isbn = {978-1-4419-9633-6}
}

@book{davison1997,
  title = {Bootstrap {{Methods}} and Their {{Application}}},
  author = {Davison, A. C. and Hinkley, D. V.},
  date = {1997},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9780511802843},
  abstract = {Bootstrap methods are computer-intensive methods of statistical analysis, which use simulation to calculate standard errors, confidence intervals, and significance tests. The methods apply for any level of modelling, and so can be used for fully parametric, semiparametric, and completely nonparametric analysis. This 1997 book gives a broad and up-to-date coverage of bootstrap methods, with numerous applied examples, developed in a coherent way with the necessary theoretical basis. Applications include stratified data; finite populations; censored and missing data; linear, nonlinear, and smooth regression models; classification; time series and spatial problems. Special features of the book include: extensive discussion of significance tests and confidence intervals; material on various diagnostic methods; and methods for efficient computation, including improved Monte Carlo simulation. Each chapter includes both practical and theoretical exercises. S-Plus programs for implementing the methods described in the text are available from the supporting website.},
  isbn = {978-0-521-57471-6},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}}
}

@book{davison2008,
  title = {Statistical Models},
  author = {Davison, A. C.},
  date = {2008-08-21},
  edition = {1 edizione},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  abstract = {Models and likelihood are the backbone of modern statistics. This 2003 book gives an integrated development of these topics that blends theory and practice, intended for advanced undergraduate and graduate students, researchers and practitioners. Its breadth is unrivaled, with sections on survival analysis, missing data, Markov chains, Markov random fields, point processes, graphical models, simulation and Markov chain Monte Carlo, estimating functions, asymptotic approximations, local likelihood and spline regressions as well as on more standard topics such as likelihood and linear and generalized linear models. Each chapter contains a wide range of problems and exercises. Practicals in the S language designed to build computing and data analysis skills, and a library of data sets to accompany the book, are available over the Web.},
  isbn = {978-0-521-73449-3},
  langid = {Inglese},
  pagetotal = {738}
}

@article{dean2008,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  date = {2008-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {51},
  pages = {107--113},
  issn = {0001-0782, 1557-7317},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper.},
  keywords = {done},
  langid = {english},
  number = {1}
}

@article{definetti1937,
  title = {La prévision : ses lois logiques, ses sources subjectives},
  shorttitle = {La prévision},
  author = {de Finetti, Bruno},
  date = {1937},
  journaltitle = {Annales de l'institut Henri Poincaré},
  volume = {7},
  pages = {1--68},
  keywords = {todo},
  langid = {french},
  number = {1},
  options = {useprefix=true}
}

@article{degroot1987,
  title = {A {{Conversation}} with {{George Box}}},
  author = {DeGroot, Morris H.},
  date = {1987},
  journaltitle = {Statistical Science},
  volume = {2},
  pages = {239--258},
  issn = {0883-4237},
  eprint = {2245757},
  eprinttype = {jstor},
  keywords = {conversation,discussion,done},
  number = {3}
}

@inproceedings{delathauwer2009,
  title = {A Survey of Tensor Methods},
  booktitle = {2009 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  author = {De Lathauwer, Lieven},
  date = {2009-05},
  pages = {2773--2776},
  issn = {2158-1525},
  doi = {10.1109/ISCAS.2009.5118377},
  abstract = {Matrix decompositions have always been at the heart of signal, circuit and system theory. In particular, the singular value decomposition (SVD) has been an important tool. There is currently a shift of paradigm in the algebraic foundations of these fields. Quite recently, nonnegative matrix factorization (NMF) has been shown to outperform SVD at a number of tasks. Increasing research efforts are spent on the study and application of decompositions of higher-order tensors or multi-way arrays. This paper is a partial survey on tensor generalizations of the SVD and their applications. We also touch on nonnegative tensor factorizations.},
  eventtitle = {2009 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  keywords = {Circuits and systems,Control systems,Data mining,Frequency,Heart,Matrices,Matrix decomposition,Signal processing,Singular value decomposition,Tensile stress,todo}
}

@article{delnegro2018,
  title = {A Bayesian Approach for Inference on Probabilistic Surveys},
  author = {Del Negro, Marco and Casarin, Roberto and Bassetti, Federico},
  date = {2018},
  keywords = {bayesian inference,bayesian nonparametric,casters,done,inflation credibility,survey of professional fore-},
  number = {2016}
}

@book{diggle2013,
  title = {Analysis of Longitudinal Data (Oxford Statistical Science): NCS P: 25},
  shorttitle = {Analysis of Longitudinal Data (Oxford Statistical Science)},
  author = {Diggle, Peter and Heagerty, Patrick and Liang, Kung-Yee and Zeger, Scott},
  date = {2013-08-05},
  publisher = {{Oxford University Press, Usa}},
  location = {{Oxford}},
  abstract = {The first edition of Analysis for Longitudinal Data has become a classic. Describing the statistical models and methods for the analysis of longitudinal data, it covers both the underlying statistical theory of each method, and its application to a range of examples from the agricultural and biomedical sciences. The main topics discussed are design issues, exploratory methods of analysis, linear models for continuous data, general linear models for discrete data, and models and methods for handling data and missing values. Under each heading, worked examples are presented in parallel with the methodological development, and sufficient detail is given to enable the reader to reproduce the author's results using the data-sets as an appendix. This second edition, published for the first time in paperback, provides a thorough and expanded revision of this important text. It includes two new chapters; the first discusses fully parametric models for discrete repeated measures data, and the second explores statistical models for time-dependent predictors.},
  isbn = {978-0-19-967675-0},
  langid = {Inglese},
  pagetotal = {400}
}

@book{docarmo1992,
  title = {Riemannian {{Geometry}}},
  author = {do Carmo, Manfredo Perdigao},
  date = {1992},
  edition = {1st edition},
  publisher = {{Birkhäuser}},
  location = {{Boston}},
  abstract = {Riemannian Geometry is an expanded edition of a highly acclaimed and successful textbook (originally published in Portuguese)~for first-year graduate students in mathematics and physics. The author's treatment goes very directly to the basic language of Riemannian geometry and immediately presents some of its most fundamental theorems. It is elementary, assuming only a modest background from readers, making it suitable for a wide variety of students and course structures. Its selection of topics has been deemed "superb" by teachers who have used the text. A significant feature of the book is its powerful and revealing structure, beginning simply with the definition of a differentiable manifold and ending with one of the most important results in Riemannian geometry, a proof of the Sphere Theorem. The text abounds with basic definitions and theorems, examples, applications, and numerous exercises to test the student's understanding and extend knowledge and insight into the subject. Instructors and students alike will find the work to be~a significant contribution to this highly applicable and stimulating subject.},
  isbn = {978-0-8176-3490-2},
  langid = {english},
  options = {useprefix=true},
  pagetotal = {300},
  translator = {Flaherty, Francis}
}

@article{doksum1974,
  title = {Tailfree and {{Neutral Random Probabilities}} and {{Their Posterior Distributions}}},
  author = {Doksum, Kjell},
  date = {1974-04},
  journaltitle = {The Annals of Probability},
  volume = {2},
  pages = {183--201},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0091-1798, 2168-894X},
  doi = {10.1214/aop/1176996703},
  abstract = {The random distribution function \$F\$ and its law is said to be neutral to the right if \$F(t\_1), \textbackslash lbrack F(t\_2) - F(t\_1) \textbackslash rbrack/\textbackslash lbrack 1 - F(t\_1)\textbackslash rbrack, \textbackslash cdots, \textbackslash lbrack F(t\_k) - F(t\_\{k-1\}) \textbackslash rbrack/\textbackslash lbrack 1 - F(t\_\{k-1\}) \textbackslash rbrack\$ are independent whenever \$t\_1 {$<$} \textbackslash cdots {$<$} t\_k\$. The posterior distribution of a random distribution function neutral to the right is shown to be neutral to the right. Characterizations of these random distribution functions and connections between neutrality to the right and general concepts of neutrality and tailfreeness (tailfreedom) are given.},
  keywords = {60K99,62C10,62G99,Bayes estimates,Dirichlet process,neutral,posterior distributions,posterior mean of a process,processes,Random probabilities,tailfree,todo},
  number = {2}
}

@incollection{drovandi2017,
  title = {Approximate {{Bayesian Computation}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Drovandi, Christopher C.},
  date = {2017},
  pages = {1--9},
  publisher = {{American Cancer Society}},
  doi = {10.1002/9781118445112.stat07974},
  abstract = {Bayesian statistics provides a principled framework for performing statistical inference for an unknown parameter of a stochastic model assumed to be responsible for generating some observed data. However, standard Bayesian algorithms to sample from the posterior require that the likelihood function, the probability density of the data given the parameter represented as a function of the parameter for fixed observed data, is computationally tractable. However, there are an increasing number of models across Science and Technology where the likelihood function is difficult or impossible to compute. When simulation from the model is comparatively cheaper, a class of likelihood-free methods called approximate Bayesian computation (ABC) can be used. However, ABC introduces an approximation to the posterior. This article gives an introduction to ABC, describes the approximation behavior of ABC, and provides advice on the successful implementation of ABC. Some current challenges facing ABC methods are also discussed.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat07974},
  isbn = {978-1-118-44511-2},
  keywords = {approximate Bayesian computation,done,Markov chain Monte Carlo,pseudo-marginal methods,sequential Monte Carlo,synthetic likelihood},
  langid = {english}
}

@book{dudley2002,
  title = {Real {{Analysis}} and {{Probability}}},
  author = {Dudley, R. M.},
  date = {2002-10-01},
  edition = {2nd edition},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge ; New York}},
  abstract = {This classic textbook, now reissued, offers a clear exposition of modern probability theory and of the interplay between the properties of metric spaces and probability measures. The new edition has been made even more self-contained than before; it now includes a foundation of the real number system and the Stone-Weierstrass theorem on uniform approximation in algebras of functions. Several other sections have been revised and improved, and the comprehensive historical notes have been further amplified. A number of new exercises have been added, together with hints for solution.},
  isbn = {978-0-521-00754-2},
  langid = {english},
  pagetotal = {568}
}

@article{dunson2018,
  title = {Statistics in the Big Data Era: {{Failures}} of the Machine},
  shorttitle = {Statistics in the Big Data Era},
  author = {Dunson, David B.},
  date = {2018-05-01},
  journaltitle = {Statistics \& Probability Letters},
  shortjournal = {Statistics \& Probability Letters},
  volume = {136},
  pages = {4--9},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2018.02.028},
  abstract = {There is vast interest in automated methods for complex data analysis. However, there is a lack of consideration of (1) interpretability, (2) uncertainty quantification, (3) applications with limited training data, and (4) selection bias. Statistical methods can achieve (1)-(4) with a change in focus.},
  keywords = {Deep learning,done,High-dimensional data,Large p small n,Machine learning,Scientific inference,Selection bias,Uncertainty quantification},
  langid = {english},
  series = {The Role of {{Statistics}} in the Era of Big Data}
}

@article{durbin2002,
  title = {A Simple and Efficient Simulation Smoother for State Space Time Series Analysis},
  author = {Durbin, J.},
  date = {2002-08-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {89},
  pages = {603--616},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/89.3.603},
  keywords = {todo},
  langid = {english},
  number = {3}
}

@book{durbin2012,
  title = {Time Series Analysis by State Space Methods},
  author = {Durbin, The late James and Koopman, Siem Jan},
  date = {2012-01-26},
  edition = {2 edizione},
  publisher = {{OUP Oxford}},
  location = {{Oxford}},
  abstract = {This new edition updates Durbin \& Koopman's important text on the state space approach to time series analysis. The distinguishing feature of state space time series models is that observations are regarded as made up of distinct components such as trend, seasonal, regression elements and disturbance terms, each of which is modelled separately. The techniques that emerge from this approach are very flexible and are capable of handling a much wider range of problems than the main analytical system currently in use for time series analysis, the Box-Jenkins ARIMA system. Additions to this second edition include the filtering of nonlinear and non-Gaussian series. Part I of the book obtains the mean and variance of the state, of a variable intended to measure the effect of an interaction and of regression coefficients, in terms of the observations. Part II extends the treatment to nonlinear and non-normal models. For these, analytical solutions are not available so methods are based on simulation.},
  isbn = {978-0-19-964117-8},
  langid = {Inglese},
  pagetotal = {368}
}

@article{eddelbuettel2011,
  title = {Rcpp: {{Seamless R}} and {{C}}++ {{Integration}}},
  shorttitle = {Rcpp},
  author = {Eddelbuettel, Dirk and Francois, Romain},
  date = {2011-04-13},
  journaltitle = {Journal of Statistical Software},
  volume = {40},
  pages = {1--18},
  issn = {1548-7660},
  doi = {10.18637/jss.v040.i08},
  issue = {1},
  keywords = {todo},
  langid = {english},
  number = {1}
}

@book{eddelbuettel2013,
  title = {Seamless {{R}} and {{C}}++ {{Integration}} with {{Rcpp}}},
  author = {Eddelbuettel, Dirk},
  date = {2013},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  doi = {10.1007/978-1-4614-6868-4},
  abstract = {Rcpp is the glue that binds the power and versatility of R with the speed and efficiency of C++. With Rcpp, the transfer of data between R and C++ is nearly seamless, and high-performance statistical computing is finally accessible to most R users. Rcpp should be part of every statistician's toolbox. -- Michael Braun, MIT Sloan School of Management "Seamless R and C++ integration with Rcpp" is simply a wonderful book. For anyone who uses C/C++ and R, it is an indispensable resource. The writing is outstanding. A huge bonus is the section on applications. This section covers the matrix packages Armadillo and Eigen and the GNU Scientific Library as well as RInside which enables you to use R inside C++. These applications are what most of us need to know to really do scientific programming with R and C++. I love this book. -- Robert McCulloch, University of Chicago Booth School of Business Rcpp is now considered an essential package for anybody doing serious computational research using R. Dirk's book is an excellent companion and takes the reader from a gentle introduction to more advanced applications via numerous examples and efficiency enhancing gems. The book is packed with all you might have ever wanted to know about Rcpp, its cousins (RcppArmadillo, RcppEigen .etc.), modules, package development and sugar. Overall, this book is a must-have on your shelf. -- Sanjog Misra, UCLA Anderson School of ManagementThe Rcpp package represents a major leap forward for scientific computations with R. With very few lines of C++ code, one has R's data structures readily at hand for further computations in C++. Hence, high-level numerical programming can be made in C++ almost as easily as in R, but often with a substantial speed gain. Dirk is a crucial person in these developments, and his book takes the reader from the first fragile steps on to using the full Rcpp machinery. A very recommended book! -- Søren Højsgaard, Department of Mathematical Sciences, Aalborg University, Denmark "Seamless R and C ++ Integration with Rcpp" provides the first comprehensive introduction to Rcpp. Rcpp has become the most widely-used language extension for R, and is deployed by over one-hundred different CRAN and BioConductor packages. Rcpp permits users to pass scalars, vectors, matrices, list or entire R objects back and forth between R and C++ with ease. This brings the depth of the R analysis framework together with the power, speed, and efficiency of C++.Dirk Eddelbuettel has been a contributor to CRAN for over a decade and maintains around twenty packages. He is the Debian/Ubuntu maintainer for R and other quantitative software, edits the CRAN Task Views for Finance and High-Performance Computing, is a co-founder of the annual R/Finance conference, and an editor of the Journal of Statistical Software. He holds a Ph.D. in Mathematical Economics from EHESS (Paris), and works in Chicago as a Senior Quantitative Analyst.},
  isbn = {978-1-4614-6867-7},
  langid = {english}
}

@article{eddelbuettel2014,
  title = {{{RcppArmadillo}}: {{Accelerating R}} with High-Performance {{C}}++~Linear Algebra},
  shorttitle = {{{RcppArmadillo}}},
  author = {Eddelbuettel, Dirk and Sanderson, Conrad},
  date = {2014-03-01},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  volume = {71},
  pages = {1054--1063},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2013.02.005},
  abstract = {The R statistical environment and language has demonstrated particular strengths for interactive development of statistical algorithms, as well as data modelling and visualisation. Its current implementation has an interpreter at its core which may result in a performance penalty in comparison to directly executing user algorithms in the native machine code of the host CPU. In contrast, the C++~language has no built-in visualisation capabilities, handling of linear algebra or even basic statistical algorithms; however, user programs are converted to high-performance machine code, ahead of execution. A new method avoids possible speed penalties in R by using the Rcpp extension package in conjunction with the Armadillo C++~matrix library. In addition to the inherent performance advantages of compiled code, Armadillo provides an easy-to-use template-based meta-programming framework, allowing the automatic pooling of several linear algebra operations into one, which in turn can lead to further speedups. With the aid of Rcpp and Armadillo, conversion of linear algebra centred algorithms from R to C++~becomes straightforward. The algorithms retain the overall structure as well as readability, all while maintaining a bidirectional link with the host R environment. Empirical timing comparisons of R and C++~implementations of a Kalman filtering algorithm indicate a speedup of several orders of magnitude.},
  keywords = {C++,Linear algebra,R,Software,todo},
  langid = {english}
}

@article{eddelbuettel2018,
  title = {Extending {{R}} with {{C}}++: {{A Brief Introduction}} to {{Rcpp}}},
  shorttitle = {Extending {{R}} with {{C}}++},
  author = {Eddelbuettel, Dirk and Balamuta, James Joseph},
  date = {2018-01-02},
  journaltitle = {The American Statistician},
  volume = {72},
  pages = {28--36},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2017.1375990},
  abstract = {R has always provided an application programming interface (API) for extensions. Based on the C language, it uses a number of macros and other low-level constructs to exchange data structures between the R process and any dynamically loaded component modules authors added to it. With the introduction of the Rcpp package, and its later refinements, this process has become considerably easier yet also more robust. By now, Rcpp has become the most popular extension mechanism for R. This article introduces Rcpp, and illustrates with several examples how the Rcpp Attributes mechanism in particular eases the transition of objects between R and C++ code. Supplementary materials for this article are available online.},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2017.1375990},
  keywords = {Applications and case studies,Computationally intensive methods,Simulation,Statistical computing,todo},
  number = {1}
}

@article{efron1986,
  title = {Why {{Isn}}'t {{Everyone}} a {{Bayesian}}?},
  author = {Efron, B.},
  date = {1986-02-01},
  journaltitle = {The American Statistician},
  volume = {40},
  pages = {1--5},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.1986.10475342},
  abstract = {Originally a talk delivered at a conference on Bayesian statistics, this article attempts to answer the following question: why is most scientific data analysis carried out in a non-Bayesian framework? The argument consists mainly of some practical examples of data analysis, in which the Bayesian approach is difficult but Fisherian/frequentist solutions are relatively easy. There is a brief discussion of objectivity in statistical analyses and of the difficulties of achieving objectivity within a Bayesian framework. The article ends with a list of practical advantages of Fisherian/frequentist methods, which so far seem to have outweighed the philosophical superiority of Bayesianism.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00031305.1986.10475342},
  keywords = {done,Fisherian inference,Frequentist theory,Neyman-Pearson-Wald,Objectivity},
  number = {1}
}

@book{efron1993,
  title = {An {{Introduction}} to the {{Bootstrap}}},
  author = {Efron, Bradley and Tibshirani, R. J.},
  date = {1993-01-01},
  edition = {1 edizione},
  publisher = {{Chapman and Hall/CRC}},
  location = {{New York}},
  abstract = {Statistics is a subject of many uses and surprisingly few effective practitioners. The traditional road to statistical knowledge is blocked, for most, by a formidable wall of mathematics. The approach in An Introduction to the Bootstrap avoids that wall. It arms scientists and engineers, as well as statisticians, with the computational techniques they need to analyze and understand complicated data sets.},
  isbn = {978-0-412-04231-7},
  pagetotal = {456}
}

@article{efron1996,
  title = {Empirical Bayes Methods for Combining Likelihoods},
  author = {Efron, Bradley},
  date = {1996},
  journaltitle = {Journal of the American Statistical Association},
  pages = {13},
  keywords = {done}
}

@article{efron1998,
  title = {R. {{A}}. {{Fisher}} in the 21st Century ({{Invited}} Paper Presented at the 1996 {{R}}. {{A}}. {{Fisher Lecture}})},
  author = {Efron, Bradley},
  date = {1998-05},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {13},
  pages = {95--122},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1028905930},
  abstract = {Fisher is the single most important figure in 20th century statistics. This talk examines his influence on modern statistical thinking, trying to predict how Fisherian we can expect the 21st century to be. Fisher's philosophy is characterized as a series of shrewd compromises between the Bayesian and frequentist viewpoints, augmented by some unique characteristics that are particularly useful in applied problems. Several current research topics are examined with an eye toward Fisherian influence, or the lack of it, and what this portends for future statistical developments. Based on the 1996 Fisher lecture, the article closely follows the text of that talk.},
  keywords = {Bayes,bootstrap,confidence intervals,done,empirical Bayes,fiducial,frequentist,model selection,Statistical inference},
  langid = {english},
  mrnumber = {MR1647499},
  number = {2},
  zmnumber = {1074.01536}
}

@article{efron2004,
  title = {Least Angle Regression},
  author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
  date = {2004-04},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {32},
  pages = {407--499},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053604000000067},
  abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
  keywords = {boosting,coefficient paths,done,Lasso,linear regression,variable selection},
  langid = {english},
  mrnumber = {MR2060166},
  number = {2},
  zmnumber = {1091.62054}
}

@book{efron2016,
  title = {Computer Age Statistical Inference: Algorithms, Evidence, and Data Science},
  shorttitle = {Computer Age Statistical Inference},
  author = {Efron, Bradley and Hastie, Trevor},
  date = {2016-07-20},
  publisher = {{Cambridge University Press}},
  location = {{New York}},
  abstract = {The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.},
  isbn = {978-1-107-14989-2},
  langid = {Inglese}
}

@article{efron2019,
  title = {Bayes, {{Oracle Bayes}} and {{Empirical Bayes}}},
  author = {Efron, Bradley},
  date = {2019-05},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {34},
  pages = {177--201},
  issn = {0883-4237},
  doi = {10.1214/18-STS674},
  abstract = {This article concerns the Bayes and frequentist aspects of empirical Bayes inference. Some of the ideas explored go back to Robbins in the 1950s, while others are current. Several examples are discussed, real and artificial, illustrating the two faces of empirical Bayes methodology: “oracle Bayes” shows empirical Bayes in its most frequentist mode, while “finite Bayes inference” is a fundamentally Bayesian application. In either case, modern theory and computation allow us to present a sharp finite-sample picture of what is at stake in an empirical Bayes analysis.},
  keywords = {done},
  langid = {english},
  number = {2}
}

@article{eklund2016,
  title = {Cluster Failure: {{Why fMRI}} Inferences for Spatial Extent Have Inflated False-Positive Rates},
  shorttitle = {Cluster Failure},
  author = {Eklund, Anders and Nichols, Thomas E. and Knutsson, Hans},
  date = {2016-07-12},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {113},
  pages = {7900--7905},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1602413113},
  abstract = {The most widely used task functional magnetic resonance imaging (fMRI) analyses use parametric statistical methods that depend on a variety of assumptions. In this work, we use real resting-state data and a total of 3 million random task group analyses to compute empirical familywise error rates for the fMRI software packages SPM, FSL, and AFNI, as well as a nonparametric permutation method. For a nominal familywise error rate of 5\%, the parametric statistical methods are shown to be conservative for voxelwise inference and invalid for clusterwise inference. Our results suggest that the principal cause of the invalid cluster inferences is spatial autocorrelation functions that do not follow the assumed Gaussian shape. By comparison, the nonparametric permutation test is found to produce nominal results for voxelwise as well as clusterwise inference. These findings speak to the need of validating the statistical methods being used in the field of neuroimaging.},
  eprint = {27357684},
  eprinttype = {pmid},
  keywords = {cluster inference,false positives,fMRI,permutation test,statistics,todo},
  langid = {english},
  number = {28}
}

@article{ernst2004,
  title = {Permutation {{Methods}}: {{A Basis}} for {{Exact Inference}}},
  shorttitle = {Permutation {{Methods}}},
  author = {Ernst, Michael D.},
  date = {2004-11},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {19},
  pages = {676--685},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/088342304000000396},
  abstract = {The use of permutation methods for exact inference dates back to Fisher in 1935. Since then, the practicality of such methods has increased steadily with computing power. They can now easily be employed in many situations without concern for computing difficulties. We discuss the reasoning behind these methods and describe situations when they are exact and distribution-free. We illustrate their use in several examples.},
  keywords = {Distribution-free,done,Monte Carlo,nonparametric,permutation tests,randomization tests},
  langid = {english},
  mrnumber = {MR2185589},
  number = {4},
  zmnumber = {1100.62563}
}

@article{escobar1995,
  title = {Bayesian {{Density Estimation}} and {{Inference Using Mixtures}}},
  author = {Escobar, Michael D. and West, Mike},
  date = {1995-06-01},
  journaltitle = {Journal of the American Statistical Association},
  volume = {90},
  pages = {577--588},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1995.10476550},
  abstract = {We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation and are exemplified by special cases where data are modeled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior, and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1995.10476550},
  keywords = {Kernel estimation,Mixtures of Dirichlet processes,Multimodality,Normal mixtures,Posterior sampling,Smoothing parameter estimation,todo},
  number = {430}
}

@book{evans2006,
  title = {Probability and Statistics: The Science of Uncertainty},
  shorttitle = {Probability and Statistics},
  author = {Evans, Michael J. and Rosenthal, Jeffrey S.},
  date = {2006},
  publisher = {{W H Freeman \& Co}},
  location = {{New York}},
  isbn = {978-0-7167-6219-5},
  langid = {Inglese},
  pagetotal = {147}
}

@book{fahrmeir2010,
  title = {Multivariate Statistical Modelling Based on Generalized Linear Models},
  author = {Fahrmeir, Ludwig and Tutz, Gerhard},
  date = {2010-12-01},
  publisher = {{Springer Nature}},
  location = {{New York; London}},
  abstract = {The book is aimed at applied statisticians, graduate students of statistics, and students and researchers with a strong interest in statistics and data analysis. This second edition is extensively revised, especially those sections relating with Bayesian concepts.},
  isbn = {978-1-4419-2900-6},
  langid = {Inglese},
  pagetotal = {574}
}

@article{fanelli2010,
  title = {“{{Positive}}” {{Results Increase Down}} the {{Hierarchy}} of the {{Sciences}}},
  author = {Fanelli, Daniele},
  date = {2010-04-07},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {5},
  pages = {e10068},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0010068},
  abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the “hardness” of scientific research—i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors—is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a “positive” (full or partial) or “negative” support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in “softer” sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
  keywords = {Forecasting,Mental health and psychiatry,Physical sciences,Scientists,Social psychology,Social research,Social sciences,Sociology,todo},
  langid = {english},
  number = {4}
}

@online{fasano2020,
  title = {Scalable and {{Accurate Variational Bayes}} for {{High}}-{{Dimensional Binary Regression Models}}},
  author = {Fasano, Augusto and Durante, Daniele and Zanella, Giacomo},
  date = {2020-10-22},
  abstract = {Modern methods for Bayesian regression with binary responses are either computationally impractical or inaccurate in high dimensions. In fact, as discussed in recent literature, bypassing this trade-off is still an open problem which is object of intense research. To cover this gap, we develop a novel variational approximation for the posterior distribution of the coefficients in high-dimensional probit regression with Gaussian priors. Our method leverages a representation with global and local variables but, unlike for classical mean-field assumptions, it avoids a fully factorized approximation, and instead assumes a factorization only for the local variables. We prove that the resulting variational approximation belongs to a tractable class of unified skew-normal densities that crucially incorporates skewness and, unlike for state-of-the-art variational Bayes solutions, converges to the exact posterior density as the number of predictors p goes to infinity. To solve the variational optimization problem, we develop a tractable coordinate ascent variational algorithm which easily scales to p in tens of thousands, and provably requires a number of iterations converging to 1 as p goes to infinity. These findings are also illustrated in extensive simulation studies and in real-world medical applications where our methods are shown to uniformly improve classical mean-field variational Bayes in terms of inference accuracy and predictive performance. The magnitude of such gains is especially remarkable in those high-dimensional p{$>$}n settings where state-of-the-art alternative strategies are computationally impractical.},
  archiveprefix = {arXiv},
  eprint = {1911.06743},
  eprinttype = {arxiv},
  keywords = {Statistics - Computation,Statistics - Methodology,todo},
  primaryclass = {stat}
}

@article{ferguson1973,
  title = {A {{Bayesian Analysis}} of {{Some Nonparametric Problems}}},
  author = {Ferguson, Thomas S.},
  date = {1973-03},
  journaltitle = {The Annals of Statistics},
  volume = {1},
  pages = {209--230},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176342360},
  abstract = {The Bayesian approach to statistical problems, though fruitful in many ways, has been rather unsuccessful in treating nonparametric problems. This is due primarily to the difficulty in finding workable prior distributions on the parameter space, which in nonparametric ploblems is taken to be a set of probability distributions on a given sample space. There are two desirable properties of a prior distribution for nonparametric problems. (I) The support of the prior distribution should be large--with respect to some suitable topology on the space of probability distributions on the sample space. (II) Posterior distributions given a sample of observations from the true probability distribution should be manageable analytically. These properties are antagonistic in the sense that one may be obtained at the expense of the other. This paper presents a class of prior distributions, called Dirichlet process priors, broad in the sense of (I), for which (II) is realized, and for which treatment of many nonparametric statistical problems may be carried out, yielding results that are comparable to the classical theory. In Section 2, we review the properties of the Dirichlet distribution needed for the description of the Dirichlet process given in Section 3. Briefly, this process may be described as follows. Let \$\textbackslash mathscr\{X\}\$ be a space and \$\textbackslash mathscr\{A\}\$ a \$\textbackslash sigma\$-field of subsets, and let \$\textbackslash alpha\$ be a finite non-null measure on \$(\textbackslash mathscr\{X\}, \textbackslash mathscr\{A\})\$. Then a stochastic process \$P\$ indexed by elements \$A\$ of \$\textbackslash mathscr\{A\}\$, is said to be a Dirichlet process on \$(\textbackslash mathscr\{X\}, \textbackslash mathscr\{A\})\$ with parameter \$\textbackslash alpha\$ if for any measurable partition \$(A\_1, \textbackslash cdots, A\_k)\$ of \$\textbackslash mathscr\{X\}\$, the random vector \$(P(A\_1), \textbackslash cdots, P(A\_k))\$ has a Dirichlet distribution with parameter \$(\textbackslash alpha(A\_1), \textbackslash cdots, \textbackslash alpha(A\_k)). P\$ may be considered a random probability measure on \$(\textbackslash mathscr\{X\}, \textbackslash mathscr\{A\})\$, The main theorem states that if \$P\$ is a Dirichlet process on \$(\textbackslash mathscr\{X\}, \textbackslash mathscr\{A\})\$ with parameter \$\textbackslash alpha\$, and if \$X\_1, \textbackslash cdots, X\_n\$ is a sample from \$P\$, then the posterior distribution of \$P\$ given \$X\_1, \textbackslash cdots, X\_n\$ is also a Dirichlet process on \$(\textbackslash mathscr\{X\}, \textbackslash mathscr\{A\})\$ with a parameter \$\textbackslash alpha + \textbackslash sum\^n\_1 \textbackslash delta\_\{x\_i\}\$, where \$\textbackslash delta\_x\$ denotes the measure giving mass one to the point \$x\$. In Section 4, an alternative definition of the Dirichlet process is given. This definition exhibits a version of the Dirichlet process that gives probability one to the set of discrete probability measures on \$(\textbackslash mathscr\{X\}, \textbackslash mathscr\{A\})\$. This is in contrast to Dubins and Freedman [2], whose methods for choosing a distribution function on the interval [0, 1] lead with probability one to singular continuous distributions. Methods of choosing a distribution function on [0, 1] that with probability one is absolutely continuous have been described by Kraft [7]. The general method of choosing a distribution function on [0, 1], described in Section 2 of Kraft and van Eeden [10], can of course be used to define the Dirichlet process on [0, 1]. Special mention must be made of the papers of Freedman and Fabius. Freedman [5] defines a notion of tailfree for a distribution on the set of all probability measures on a countable space \$\textbackslash mathscr\{X\}\$. For a tailfree prior, posterior distribution given a sample from the true probability measure may be fairly easily computed. Fabius [3] extends the notion of tailfree to the case where \$\textbackslash mathscr\{X\}\$ is the unit interval [0, 1], but it is clear his extension may be made to cover quite general \$\textbackslash mathscr\{X\}\$. With such an extension, the Dirichlet process would be a special case of a tailfree distribution for which the posterior distribution has a particularly simple form. There are disadvantages to the fact that \$P\$ chosen by a Dirichlet process is discrete with probability one. These appear mainly because in sampling from a \$P\$ chosen by a Dirichlet process, we expect eventually to see one observation exactly equal to another. For example, consider the goodness-of-fit problem of testing the hypothesis \$H\_0\$ that a distribution on the interval [0, 1] is uniform. If on the alternative hypothesis we place a Dirichlet process prior with parameter \$\textbackslash alpha\$ itself a uniform measure on [0, 1], and if we are given a sample of size \$n \textbackslash geqq 2\$, the only nontrivial nonrandomized Bayes rule is to reject \$H\_0\$ if and only if two or more of the observations are exactly equal. This is really a test of the hypothesis that a distribution is continuous against the hypothesis that it is discrete. Thus, there is still a need for a prior that chooses a continuous distribution with probability one and yet satisfies properties (I) and (II). Some applications in which the possible doubling up of the values of the observations plays no essential role are presented in Section 5. These include the estimation of a distribution function, of a mean, of quantiles, of a variance and of a covariance. A two-sample problem is considered in which the Mann-Whitney statistic, equivalent to the rank-sum statistic, appears naturally. A decision theoretic upper tolerance limit for a quantile is also treated. Finally, a hypothesis testing problem concerning a quantile is shown to yield the sign test. In each of these problems, useful ways of combining prior information with the statistical observations appear. Other applications exist. In his Ph. D. dissertation [1], Charles Antoniak finds a need to consider mixtures of Dirichlet processes. He treats several problems, including the estimation of a mixing distribution, bio-assay, empirical Bayes problems, and discrimination problems.},
  keywords = {todo},
  number = {2}
}

@book{ferguson2017,
  title = {A {{Course}} in {{Large Sample Theory}}},
  author = {Ferguson, Thomas S.},
  date = {2017-09-06},
  publisher = {{Routledge}},
  doi = {10.1201/9781315136288},
  abstract = {A Course in Large Sample Theory is presented in four parts. The first treats basic probabilistic notions, the second features the basic statistical tools for},
  isbn = {978-1-315-13628-8},
  langid = {english}
}

@article{fisher1936,
  title = {The {{Use}} of {{Multiple Measurements}} in {{Taxonomic Problems}}},
  author = {Fisher, R. A.},
  date = {1936},
  journaltitle = {Annals of Eugenics},
  volume = {7},
  pages = {179--188},
  issn = {2050-1439},
  doi = {10.1111/j.1469-1809.1936.tb02137.x},
  abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
  keywords = {done},
  langid = {english},
  number = {2}
}

@incollection{frankel2014,
  title = {Resampling {{Procedures}} for {{Sample Surveys}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Frankel, Martin R.},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2014-09-29},
  pages = {stat05043},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat05043},
  isbn = {978-1-118-44511-2},
  keywords = {todo},
  langid = {english}
}

@article{friedman2000,
  title = {Additive Logistic Regression: A Statistical View of Boosting ({{With}} Discussion and a Rejoinder by the Authors)},
  shorttitle = {Additive Logistic Regression},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  date = {2000-04},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {28},
  pages = {337--407},
  issn = {0090-5364},
  doi = {10.1214/aos/1016218223},
  keywords = {done},
  langid = {english},
  number = {2}
}

@article{friedman2010,
  title = {Regularization {{Paths}} for {{Generalized Linear Models}} via {{Coordinate Descent}}},
  author = {Friedman, Jerome H. and Hastie, Trevor and Tibshirani, Rob},
  date = {2010-02-02},
  journaltitle = {Journal of Statistical Software},
  volume = {33},
  pages = {1--22},
  issn = {1548-7660},
  doi = {10.18637/jss.v033.i01},
  issue = {1},
  keywords = {done},
  langid = {english},
  number = {1}
}

@report{frigyik2010,
  title = {Introduction to the {{Dirichlet}} Distribution and Related Processes},
  author = {Frigyik, Bela A. and Kapila, Amol and Gupta, Maya R.},
  date = {2010},
  abstract = {This tutorial covers the Dirichlet distribution, Dirichlet process, Pólya urn (and the associated Chinese restaurant process), hierarchical Dirichlet Process, and the Indian buffet process. Apart from basic properties, we describe and contrast three methods of generating samples: stick-breaking, the Pólya urn, and drawing gamma random variables. For the Dirichlet process we first present an informal introduction, and then a rigorous description for those more comfortable with probability theory.},
  keywords = {done}
}

@book{fristedt1996,
  title = {A Modern Approach to Probability Theory},
  author = {Fristedt, Bert and Gray, Lawrence},
  date = {1996-12-23},
  edition = {1997° edizione},
  publisher = {{Birkhauser}},
  location = {{Boston}},
  abstract = {Students and teachers of mathematics and related fields will find~this book~a comprehensive and modern approach to probability theory, providing the background and techniques to go from the beginning graduate level to the point of specialization in research areas of current interest. The book is designed for a two- or three-semester course, assuming only courses in undergraduate real analysis or rigorous advanced calculus, and some elementary linear algebra. A variety of applicationsBayesian statistics, financial mathematics, information theory, tomography, and signal processingappear as threads to both enhance the understanding of the relevant mathematics and motivate students whose main interests are outside of pure areas.},
  isbn = {978-0-8176-3807-8},
  langid = {Inglese},
  pagetotal = {756}
}

@article{friston1996,
  title = {Detecting Activations in {{PET}} and {{fMRI}}: Levels of Inference and Power},
  shorttitle = {Detecting Activations in {{PET}} and {{fMRI}}},
  author = {Friston, K. J. and Holmes, A. and Poline, J. B. and Price, C. J. and Frith, C. D.},
  date = {1996-12},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  volume = {4},
  pages = {223--235},
  issn = {1053-8119},
  doi = {10.1006/nimg.1996.0074},
  abstract = {This paper is about detecting activations in statistical parametric maps and considers the relative sensitivity of a nested hierarchy of tests that we have framed in terms of the level of inference (voxel level, cluster level, and set level). These tests are based on the probability of obtaining c, or more, clusters with k, or more, voxels, above a threshold u. This probability has a reasonably simple form and is derived using distributional approximations from the theory of Gaussian fields. The most important contribution of this work is the notion of set-level inference. Set-level inference refers to the statistical inference that the number of clusters comprising an observed activation profile is highly unlikely to have occurred by chance. This inference pertains to the set of activations reaching criteria and represents a new way of assigning P values to distributed effects. Cluster-level inferences are a special case of set-level inferences, which obtain when the number of clusters c = 1. Similarly voxel-level inferences are special cases of cluster-level inferences that result when the cluster can be very small (i.e., k = 0). Using a theoretical power analysis of distributed activations, we observed that set-level inferences are generally more powerful than cluster-level inferences and that cluster-level inferences are generally more powerful than voxel-level inferences. The price paid for this increased sensitivity is reduced localizing power: Voxel-level tests permit individual voxels to be identified as significant, whereas cluster-and set-level inferences only allow clusters or sets of clusters to be so identified. For all levels of inference the spatial size of the underlying signal f (relative to resolution) determines the most powerful thresholds to adopt. For set-level inferences if f is large (e.g., fMRI) then the optimum extent threshold should be greater than the expected number of voxels for each cluster. If f is small (e.g., PET) the extent threshold should be small. We envisage that set-level inferences will find a role in making statistical inferences about distributed activations, particularly in fMRI.},
  eprint = {9345513},
  eprinttype = {pmid},
  issue = {3 Pt 1},
  keywords = {Brain,Brain Mapping,Data Interpretation; Statistical,Humans,Magnetic Resonance Imaging,Mathematical Computing,Normal Distribution,Probability,ROC Curve,Sensitivity and Specificity,todo,Tomography; Emission-Computed},
  langid = {english}
}

@book{fu2015,
  title = {Handbook of {{Simulation Optimization}}},
  editor = {Fu, Michael C.},
  date = {2015},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  doi = {10.1007/978-1-4939-1384-8},
  abstract = {The Handbook of Simulation Optimization presents an overview of the state of the art of simulation optimization, providing a survey of the most well-established approaches for optimizing stochastic simulation models and a sampling of recent research advances in theory and methodology. Leading contributors cover such topics as discrete optimization via simulation, ranking and selection, efficient simulation budget allocation, random search methods, response surface methodology, stochastic gradient estimation, stochastic approximation, sample average approximation, stochastic constraints, variance reduction techniques, model-based stochastic search methods and Markov decision processes.This single volume should serve as a reference for those already in the field and as a means for those new to the field for understanding and applying the main approaches. The intended audience includes researchers, practitioners and graduate students in the business/engineering fields of operations research, management science, operations management and stochastic control, as well as in economics/finance and computer science.},
  isbn = {978-1-4939-1383-1},
  langid = {english},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}}
}

@article{gelman1992,
  title = {Inference from {{Iterative Simulation Using Multiple Sequences}}},
  author = {Gelman, Andrew and Rubin, Donald B.},
  date = {1992-11},
  journaltitle = {Statistical Science},
  volume = {7},
  pages = {457--472},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177011136},
  abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
  keywords = {Bayesian inference,Convergence of stochastic processes,done,ECM,EM,Gibbs sampler,importance sampling,Metropolis algorithm,multiple imputation,random-effects model,SIR},
  number = {4}
}

@article{gelman2006,
  title = {Multilevel ({{Hierarchical}}) {{Modeling}}: {{What It Can}} and {{Cannot Do}}},
  shorttitle = {Multilevel ({{Hierarchical}}) {{Modeling}}},
  author = {Gelman, Andrew},
  date = {2006-08-01},
  journaltitle = {Technometrics},
  volume = {48},
  pages = {432--435},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/004017005000000661},
  abstract = {Multilevel (hierarchical) modeling is a generalization of linear and generalized linear modeling in which regression coefficients are themselves given a model, whose parameters are also estimated from data. We illustrate the strengths and limitations of multilevel modeling through an example of the prediction of home radon levels in U.S. counties. The multilevel model is highly effective for predictions at both levels of the model, but could easily be misinterpreted for causal inference.},
  annotation = {\_eprint: https://doi.org/10.1198/004017005000000661},
  keywords = {Contextual effects,done,Hierarchical model,Multilevel regression},
  number = {3}
}

@book{gelman2007,
  title = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  date = {2007}
}

@article{gelman2008,
  title = {A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models},
  author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
  date = {2008-12},
  journaltitle = {The Annals of Applied Statistics},
  volume = {2},
  pages = {1360--1383},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/08-AOAS191},
  abstract = {We propose a new prior distribution for classical (nonhierarchical) logistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-t prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors. We recommend this prior distribution as a default choice for routine applied use. It has the advantage of always giving answers, even when there is complete separation in logistic regression (a common problem, even when the sample size is large and the number of predictors is small), and also automatically applying more shrinkage to higher-order interactions. This can be useful in routine data analysis as well as in automated procedures such as chained equations for missing-data imputation. We implement a procedure to fit generalized linear models in R with the Student-t prior distribution by incorporating an approximate EM algorithm into the usual iteratively weighted least squares. We illustrate with several applications, including a series of logistic regressions predicting voting preferences, a small bioassay experiment, and an imputation model for a public health data set.},
  keywords = {Bayesian inference,generalized linear model,hierarchical model,least squares,Linear regression,logistic regression,multilevel model,noninformative prior distribution,todo,weakly informative prior distribution},
  number = {4}
}

@book{gelman2013,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  date = {2013},
  edition = {3 edizione},
  publisher = {{Chapman and Hall/CRC}},
  isbn = {978-1-4398-4095-5},
  pagetotal = {675}
}

@article{gelman2017,
  title = {Beyond Subjective and Objective in Statistics},
  author = {Gelman, Andrew and Hennig, Christian},
  date = {2017},
  journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {180},
  pages = {967--1033},
  issn = {1467-985X},
  doi = {10.1111/rssa.12276},
  abstract = {Decisions in statistical data analysis are often justified, criticized or avoided by using concepts of objectivity and subjectivity. We argue that the words ‘objective’ and ‘subjective’ in statistics discourse are used in a mostly unhelpful way, and we propose to replace each of them with broader collections of attributes, with objectivity replaced by transparency, consensus, impartiality and correspondence to observable reality, and subjectivity replaced by awareness of multiple perspectives and context dependence. Together with stability, these make up a collection of virtues that we think is helpful in discussions of statistical foundations and practice. The advantage of these reformulations is that the replacement terms do not oppose each other and that they give more specific guidance about what statistical science strives to achieve. Instead of debating over whether a given statistical method is subjective or objective (or normatively debating the relative merits of subjectivity and objectivity in statistical practice), we can recognize desirable attributes such as transparency and acknowledgement of multiple perspectives as complementary goals. We demonstrate the implications of our proposal with recent applied examples from pharmacology, election polling and socio-economic stratification. The aim of the paper is to push users and developers of statistical methods towards more effective use of diverse sources of information and more open acknowledgement of assumptions and goals.},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12276},
  keywords = {Bayesian,done,Frequentist,Good practice,Philosophy of statistics,Virtues},
  langid = {english},
  number = {4}
}

@article{gelman2017a,
  title = {The {{Prior Can Often Only Be Understood}} in the {{Context}} of the {{Likelihood}}},
  author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
  date = {2017-10},
  journaltitle = {Entropy},
  volume = {19},
  pages = {555},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/e19100555},
  abstract = {A key sticking point of Bayesian analysis is the choice of prior distribution, and there is a vast literature on potential defaults including uniform priors, Jeffreys’ priors, reference priors, maximum entropy priors, and weakly informative priors. These methods, however, often manifest a key conceptual tension in prior modeling: a model encoding true prior information should be chosen without reference to the model of the measurement process, but almost all common prior modeling techniques are implicitly motivated by a reference likelihood. In this paper we resolve this apparent paradox by placing the choice of prior into the context of the entire Bayesian analysis, from inference to prediction to model evaluation.},
  issue = {10},
  keywords = {Bayesian inference,default priors,prior distribution,todo},
  langid = {english},
  number = {10}
}

@book{gelman2020,
  title = {Bayesian {{Workflow}}},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and Bürkner, Paul-Christian and Modrák, Martin},
  date = {2020-11-03},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.}
}

@article{genz1992,
  title = {Numerical {{Computation}} of {{Multivariate Normal Probabilities}}},
  author = {Genz, Alan},
  date = {1992-06-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  volume = {1},
  pages = {141--149},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.1992.10477010},
  abstract = {The numerical computation of a multivariate normal probability is often a difficult problem. This article describes a transformation that simplifies the problem and places it into a form that allows efficient calculation using standard numerical multiple integration algorithms. Test results are presented that compare implementations of two algorithms that use the transformation with currently available software.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/10618600.1992.10477010},
  keywords = {Adaptive integration,done,Monte Carlo,Multivariate normal distribution},
  number = {2}
}

@article{gerber2015,
  title = {Sequential Quasi {{Monte Carlo}}},
  author = {Gerber, Mathieu and Chopin, Nicolas},
  date = {2015-06-01},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {77},
  pages = {509--579},
  issn = {1467-9868},
  doi = {10.1111/rssb.12104},
  abstract = {We derive and study sequential quasi Monte Carlo (SQMC), a class of algorithms obtained by introducing QMC point sets in particle filtering. SQMC is related to, and may be seen as an extension of, th...},
  keywords = {todo},
  langid = {english},
  number = {3}
}

@book{ghiani2013,
  title = {Introduction to Logistics Systems Management},
  author = {Ghiani, Gianpaolo and Laporte, Gilbert and Musmanno, Roberto},
  date = {2013-03-11},
  edition = {2 edizione},
  publisher = {{John Wiley \& Sons Inc}},
  location = {{Chichester, West Sussex, United Kingdom}},
  abstract = {Introduction to Logistics Systems Management is the fully revised and enhanced version of the 2004 prize-winning textbook Introduction to Logistics Systems Planning and Control, used in universities around the world. This textbook offers an introduction to the methodological aspects of logistics systems management and is based on the rich experience of the authors in teaching, research and industrial consulting. This new edition puts more emphasis on the organizational context in which logistics systems operate and also covers several new models and techniques that have been developed over the past decade. Each topic is illustrated by a numerical example so that the reader can check his or her understanding of each concept before moving on to the next one. At the end of each chapter, case studies taken from the scientific literature are presented to illustrate the use of quantitative methods for solving complex logistics decision problems. An exhaustive set of exercises is also featured at the end of each chapter. The book targets an academic as well as a practitioner audience, and is appropriate for advanced undergraduate and graduate courses in logistics and supply chain management, and should also serve as a methodological reference for practitioners in consulting as well as in industry.},
  isbn = {978-1-119-94338-9},
  langid = {Inglese},
  pagetotal = {455}
}

@article{ghosal2007,
  title = {Convergence Rates of Posterior Distributions for Noniid Observations},
  author = {Ghosal, Subhashis and van der Vaart, Aad},
  date = {2007-02},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {35},
  pages = {192--223},
  issn = {0090-5364},
  doi = {10.1214/009053606000001172},
  abstract = {We consider the asymptotic behavior of posterior distributions and Bayes estimators based on observations which are required to be neither independent nor identically distributed. We give general results on the rate of convergence of the posterior measure relative to distances derived from a testing criterion. We then specialize our results to independent, nonidentically distributed observations, Markov processes, stationary Gaussian time series and the white noise model. We apply our general results to several examples of infinite-dimensional statistical models including nonparametric regression with normal errors, binary regression, Poisson regression, an interval censoring model, Whittle estimation of the spectral density of a time series and a nonlinear autoregressive model.},
  archiveprefix = {arXiv},
  eprint = {0708.0491},
  eprinttype = {arxiv},
  keywords = {62G20 (Primary) 62G08 (Secondary),Mathematics - Statistics Theory,todo},
  number = {1},
  options = {useprefix=true}
}

@book{ghosh2003,
  title = {Bayesian {{Nonparametrics}}},
  author = {Ghosh, J. K. and Ramamoorthi, R. V.},
  date = {2003},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  doi = {10.1007/b97842},
  abstract = {Bayesian nonparametrics has grown tremendously in the last three decades, especially in the last few years. This book is the first systematic treatment of Bayesian nonparametric methods and the theory behind them. While the book is of special interest to Bayesians, it will also appeal to statisticians in general because Bayesian nonparametrics offers a whole continuous spectrum of robust alternatives to purely parametric and purely nonparametric methods of classical statistics. The book is primarily aimed at graduate students and can be used as the text for a graduate course in Bayesian nonparametrics. Though the emphasis of the book is on nonparametrics, there is a substantial chapter on asymptotics of classical Bayesian parametric models. Jayanta Ghosh has been Director and Jawaharlal Nehru Professor at the Indian Statistical Institute and President of the International Statistical Institute. He is currently professor of statistics at Purdue University. He has been editor of Sankhya and served on the editorial boards of several journals including the Annals of Statistics. Apart from Bayesian analysis, his interests include asymptotics, stochastic modeling, high dimensional model selection, reliability and survival analysis and bioinformatics. R.V. Ramamoorthi is professor at the Department of Statistics and Probability at Michigan State University. He has published papers in the areas of sufficiency invariance, comparison of experiments, nonparametric survival analysis and Bayesian analysis. In addition to Bayesian nonparametrics, he is currently interested in Bayesian networks and graphical models. He is on the editorial board of Sankhya.},
  isbn = {978-0-387-95537-7},
  langid = {english},
  series = {Springer {{Series}} in {{Statistics}}}
}

@book{gill1982,
  title = {Practical Optimization},
  author = {Gill, Philip E. and Murray, Walter and Wright, Margaret H.},
  date = {1982-01-28},
  edition = {Reprint edizione},
  publisher = {{Academic Press}},
  location = {{London ; New York}},
  abstract = {This book is designed to help problem solvers make the best use of optimization software--i.e., to use existing methods most effectively whenever possible, and to adapt and modify techniques for particular problems if necessary. The contents of this book therefore include some topics that are essential for all those who wish to solve optimization problems. In addition, certain topics are treated that should be of special interest in most practical optimization problems. For example, advice is given to users who wish to take an active role in formulating their problems so as to enhance the chances of solving them successfully, and to users who need to understand why a certain method fails to solve their problem.},
  isbn = {978-0-12-283952-8},
  langid = {Inglese},
  pagetotal = {402}
}

@incollection{goldstein2016,
  title = {Multilevel {{Models}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Goldstein, Harvey and Browne, William J.},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2016-07-28},
  pages = {1--8},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat05764.pub2},
  isbn = {978-1-118-44511-2},
  keywords = {done},
  langid = {english}
}

@book{golub2013,
  title = {Matrix {{Computations}}},
  author = {Golub, Gene H. and Loan, Charles F. Van},
  date = {2013},
  publisher = {{JHU Press}},
  abstract = {The fourth edition of Gene H. Golub and Charles F. Van Loan's classic is an essential reference for computational scientists and engineers in addition to researchers in the numerical linear algebra community. Anyone whose work requires the solution to a matrix problem and an appreciation of its mathematical properties will find this book to be an indispensible tool. This revision is a cover-to-cover expansion and renovation of the third edition. It now includes an introduction to tensor computations and brand new sections on • fast transforms• parallel LU• discrete Poisson solvers• pseudospectra• structured linear equation problems• structured eigenvalue problems• large-scale SVD methods• polynomial eigenvalue problems Matrix Computations is packed with challenging problems, insightful derivations, and pointers to the literature—everything needed to become a matrix-savvy developer of numerical methods and software. The second most cited math book of 2012 according to MathSciNet, the book has placed in the top 10 for since 2005.},
  eprint = {X5YfsuCWpxMC},
  eprinttype = {googlebooks},
  isbn = {978-1-4214-0794-4},
  keywords = {Mathematics / Applied,Mathematics / General},
  langid = {english},
  pagetotal = {782}
}

@book{goodfellow2017,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2017-01-01},
  publisher = {{Mit Pr}},
  location = {{Cambridge, Massachusetts}},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  isbn = {978-0-262-03561-3},
  langid = {Inglese},
  pagetotal = {775}
}

@online{goodfellow2017a,
  title = {{{NIPS}} 2016 {{Tutorial}}: {{Generative Adversarial Networks}}},
  shorttitle = {{{NIPS}} 2016 {{Tutorial}}},
  author = {Goodfellow, Ian},
  date = {2017-04-03},
  abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
  archiveprefix = {arXiv},
  eprint = {1701.00160},
  eprinttype = {arxiv},
  keywords = {Computer Science - Machine Learning,todo},
  primaryclass = {cs}
}

@book{goodrich2011,
  title = {Data Structures and Algorithms in C++},
  author = {Goodrich, Michael T. and Tamassia, Roberto and Mount, David M.},
  date = {2011-01-31},
  edition = {2° edizione},
  publisher = {{John Wiley \& Sons Inc}},
  location = {{Hoboken, N.J}},
  abstract = {An updated, innovative approach to data structures and algorithms Written by an author team of experts in their fields, this authoritative guide demystifies even the most difficult mathematical concepts so that you can gain a clear understanding of data structures and algorithms in C++. The unparalleled author team incorporates the object-oriented design paradigm using C++ as the implementation language, while also providing intuition and analysis of fundamental algorithms.  Offers a unique multimedia format for learning the fundamentals of data structures and algorithms Allows you to visualize key analytic concepts, learn about the most recent insights in the field, and do data structure design Provides clear approaches for developing programs Features a clear, easy-to-understand writing style that breaks down even the most difficult mathematical concepts  Building on the success of the first edition, this new version offers you an innovative approach to fundamental data structures and algorithms.},
  isbn = {978-0-470-38327-8},
  langid = {Inglese},
  pagetotal = {714}
}

@software{goodrich2020,
  title = {Bayesian {{Applied Regression Modeling}} via {{Stan}}},
  author = {Goodrich, B. and Gabry, J. and Ali, I. and Brilleman, S.},
  date = {2020},
  abstract = {Estimates previously compiled regression models using the rstan     package, which provides the R interface to the Stan C++ library for Bayesian     estimation. Users specify models via the customary R syntax with a formula and     data.frame plus some additional arguments for priors.},
  version = {2.21.1}
}

@article{graham2010,
  title = {A {{Phase I}} Nonparametric {{Shewhart}}-Type Control Chart Based on the Median},
  author = {Graham, M. A. and Human, S. W. and Chakraborti, S.},
  date = {2010-11-01},
  journaltitle = {Journal of Applied Statistics},
  volume = {37},
  pages = {1795--1813},
  publisher = {{Taylor \& Francis}},
  issn = {0266-4763},
  doi = {10.1080/02664760903164913},
  abstract = {A nonparametric Shewhart-type control chart is proposed for monitoring the location of a continuous variable in a Phase I process control setting. The chart is based on the pooled median of the available Phase I samples and the charting statistics are the counts (number of observations) in each sample that are less than the pooled median. An exact expression for the false alarm probability (FAP) is given in terms of the multivariate hypergeometric distribution and this is used to provide tables for the control limits for a specified nominal FAP value (of 0.01, 0.05 and 0.10, respectively) and for some values of the sample size (n) and the number of Phase I samples (m). Some approximations are discussed in terms of the univariate hypergeometric and the normal distributions. A simulation study shows that the proposed chart performs as well as, and in some cases better than, an existing Shewhart-type chart based on the normal distribution. Numerical examples are given to demonstrate the implementation of the new chart.},
  annotation = {\_eprint: https://doi.org/10.1080/02664760903164913},
  keywords = {distribution-free,false alarm probability,false alarm rate,multivariate hypergeometric,prospective,retrospective,todo},
  number = {11}
}

@book{gramacy,
  title = {Chapter 1 {{Historical Perspective}} | {{Surrogates}}},
  author = {Gramacy, Robert B.},
  abstract = {Chapter 1 Historical Perspective | Surrogates: a new graduate level textbook on topics lying at the interface between machine learning, spatial statistics, computer simulation, meta-modeling (i.e., emulation), and design of experiments. Gaussian process emphasis facilitates flexible nonparametric and nonlinear modeling, with applications to uncertainty quantification, sensitivity analysis, calibration of computer models to field data, sequential design and (blackbox) optimization under uncertainty. Presentation targets numerically competent scientists in the engineering, physical, and biological sciences. Treatment includes historical perspective and canonical examples, but primarily concentrates on modern statistical methods, computation and implementation in R at modern scale. Rmarkdown facilitates a fully reproducible tour complete with motivation from, application to, and illustration with, compelling real-data examples.}
}

@book{gramacy2020,
  title = {Surrogates Gaussian Process Modeling, Design, and Optimization for the Applied Sciences},
  shorttitle = {Surrogates},
  author = {Gramacy, Robert B.},
  date = {2020-01-08},
  edition = {1° edizione},
  publisher = {{Chapman and Hall/CRC}},
  location = {{Boca Raton}},
  abstract = {Surrogates: a graduate textbook, or professional handbook, on topics at the interface between machine learning, spatial statistics, computer simulation, meta-modeling (i.e., emulation), design of experiments, and optimization. Experimentation through simulation, "human out-of-the-loop" statistical support (focusing on the science), management of dynamic processes, online and real-time analysis, automation, and practical application are at the forefront.   Topics include:    Gaussian process (GP) regression for flexible nonparametric and nonlinear modeling.   Applications to uncertainty quantification, sensitivity analysis, calibration of computer models to field data, sequential design/active learning and (blackbox/Bayesian) optimization under uncertainty.    Advanced topics include treed partitioning, local GP approximation, modeling of simulation experiments (e.g., agent-based models) with coupled nonlinear mean and variance (heteroskedastic) models.    Treatment appreciates historical response surface methodology (RSM) and canonical examples, but emphasizes contemporary methods and implementation in R at modern scale.   Rmarkdown facilitates a fully reproducible tour, complete with motivation from, application to, and illustration with, compelling real-data examples.   Presentation targets numerically competent practitioners in engineering, physical, and biological sciences. Writing is statistical in form, but the subjects are not about statistics. Rather, they’re about prediction and synthesis under uncertainty; about visualization and information, design and decision making, computing and clean code.},
  isbn = {978-0-367-41542-6},
  langid = {Inglese},
  pagetotal = {559}
}

@book{greene1999,
  title = {Econometric Analysis},
  author = {Greene, William H.},
  date = {1999-07-28},
  edition = {Subsequent edizione},
  publisher = {{Prentice Hall}},
  location = {{Upper Saddle River, N.J}},
  abstract = {For graduate-level courses in Introduction to Econometrics. A standard text/reference in courses that include basic techniques in regression analysis and extensions used when linear models prove inadequate or inappropriate. Areas of application include Economics, Sociology, Political Science, Medical Research, Transport Research, and Environmental Economics. This book introduces students to the broad field of applied econometrics. An effective bridge to both on-the-job problems and to the professional literature, it features extensive applications and presents sufficient theoretical background to enable students to recognize new variants of the models that they learn about here as merely natural extensions that fit within a common body of principles.},
  isbn = {978-0-13-013297-0},
  langid = {Inglese},
  pagetotal = {1004}
}

@article{griffin2021,
  title = {In Search of Lost Mixing Time: Adaptive {{Markov}} Chain {{Monte Carlo}} Schemes for {{Bayesian}} Variable Selection with Very Large {\emph{p}}},
  shorttitle = {In Search of Lost Mixing Time},
  author = {Griffin, J E and Łatuszyński, K G and Steel, M F J},
  date = {2021-03-02},
  journaltitle = {Biometrika},
  volume = {108},
  pages = {53--69},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asaa055},
  abstract = {Summary             The availability of datasets with large numbers of variables is rapidly increasing. The effective application of Bayesian variable selection methods for regression with these datasets has proved difficult since available Markov chain Monte Carlo methods do not perform well in typical problem sizes of interest. We propose new adaptive Markov chain Monte Carlo algorithms to address this shortcoming. The adaptive design of these algorithms exploits the observation that in large-\$p\$, small-\$n\$ settings, the majority of the \$p\$ variables will be approximately uncorrelated a posteriori. The algorithms adaptively build suitable nonlocal proposals that result in moves with squared jumping distance significantly larger than standard methods. Their performance is studied empirically in high-dimensional problems and speed-ups of up to four orders of magnitude are observed.},
  keywords = {todo},
  langid = {english},
  number = {1}
}

@article{gupta2011,
  title = {Theory and {{Use}} of the {{EM Algorithm}}},
  author = {Gupta, Maya R. and Chen, Yihua},
  date = {2011-04-14},
  journaltitle = {Foundations and Trends® in Signal Processing},
  shortjournal = {SIG},
  volume = {4},
  pages = {223--296},
  publisher = {{Now Publishers, Inc.}},
  issn = {1932-8346, 1932-8354},
  doi = {10.1561/2000000034},
  abstract = {Theory and Use of the EM Algorithm},
  keywords = {done},
  langid = {english},
  number = {3}
}

@book{haan,
  title = {Extreme {{Value Theory}}: {{An Introduction}}},
  shorttitle = {Extreme {{Value Theory}}},
  author = {de de Haan, Laurens and Ferreira, Ana},
  abstract = {Focuses on theoretical results along with applicationsAll the main topics covering the heart of the subject are introduced to the reader in a systematic fashionConcentration is on the probabilistic and statistical aspects of extreme valuesExcellent introduction to extreme value theory at the graduate level, requiring only some mathematical maturity}
}

@article{haario2001,
  title = {An Adaptive {{Metropolis}} Algorithm},
  author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
  date = {2001-04},
  journaltitle = {Bernoulli},
  volume = {7},
  pages = {223--242},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  abstract = {A proper choice of a proposal distribution for Markov chain Monte Carlo methods, for example for the Metropolis-Hastings algorithm, is well known to be a crucial factor for the convergence of the algorithm. In this paper we introduce an adaptive Metropolis (AM) algorithm, where the Gaussian proposal distribution is updated along the process using the full information cumulated so far. Due to the adaptive nature of the process, the AM algorithm is non-Markovian, but we establish here that it has the correct ergodic properties. We also include the results of our numerical tests, which indicate that the AM algorithm competes well with traditional Metropolis-Hastings algorithms, and demonstrate that the AM algorithm is easy to use in practical computation.},
  keywords = {Adaptive Markov chain Monte Carlo,Comparison,convergence,ergodicity,Markov chain Monte Carlo,Metropolis-Hastings algorithm,todo},
  number = {2}
}

@article{haizler2019,
  title = {Factorial {{Designs}} for {{Online Experiments}}},
  author = {Haizler, Tamar and Steinberg, David M.},
  date = {2019},
  journaltitle = {Technometrics},
  volume = {63},
  pages = {1--12},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2019.1701556},
  abstract = {Online experiments and specifically A/B testing are commonly used to identify whether a proposed change to a web page is in fact an effective one. This study focuses on basic settings in which a binary outcome is obtained from each user who visits the website and the probability of a response may be affected by numerous factors. We use Bayesian probit regression to model the factor effects and combine elements from traditional two-level factorial experiments and multiarmed bandits to construct sequential designs that embed attractive features of estimation and exploitation.},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2019.1701556},
  keywords = {todo},
  number = {1}
}

@book{hall1980,
  title = {Martingale {{Limit Theory}} and Its {{Application}}},
  author = {Hall, Peter and Heide, C.C.},
  date = {1980},
  publisher = {{Elsevier}},
  doi = {10.1016/C2013-0-10818-5},
  isbn = {978-0-12-319350-6},
  langid = {english}
}

@book{hall2012,
  title = {Macroeconomics: {{Principles}} and {{Applications}}},
  shorttitle = {Macroeconomics},
  author = {Hall, Robert and Lieberman, Marc},
  date = {2012-02-17},
  edition = {6 edition},
  publisher = {{Cengage Learning}},
  location = {{Mason, OH}},
  abstract = {Discover how today's macroeconomic policy issues, decisions, and applications impact you every day with the practical, accessible presentation in MACROECONOMICS. Written by acclaimed economists Hall and Lieberman, this straightforward contemporary text offers a presentation as current as the latest headlines. Fresh new cutting-edge examples throughout this edition as well as updated mini-cases clearly illustrate core macroeconomic principles and applications in action. This edition's streamlined chapters focus on today's most important macroeconomic theories and events. The latest thinking from leading economists helps equip readers with a solid foundation in macroeconomics necessary for success, no matter what the career.},
  isbn = {978-1-111-82235-4},
  langid = {english},
  pagetotal = {552}
}

@book{hamilton1994,
  title = {Time Series Analysis},
  author = {Hamilton, James D.},
  date = {1994-03-30},
  publisher = {{Princeton Univ Pr}},
  location = {{Princeton, N.J}},
  abstract = {The last decade has brought dramatic changes in the way that researchers analyze economic and financial time series. This book synthesizes these recent advances and makes them accessible to first-year graduate students. James Hamilton provides the first adequate text-book treatments of important innovations such as vector autoregressions, generalized method of moments, the economic and statistical consequences of unit roots, time-varying variances, and nonlinear time series models. In addition, he presents basic tools for analyzing dynamic systems (including linear representations, autocovariance generating functions, spectral analysis, and the Kalman filter) in a way that integrates economic theory with the practical difficulties of analyzing and interpreting real-world data. Time Series Analysis fills an important need for a textbook that integrates economic theory, econometrics, and new results. The book is intended to provide students and researchers with a self-contained survey of time series analysis. It starts from first principles and should be readily accessible to any beginning graduate student, while it is also intended to serve as a reference book for researchers.},
  isbn = {978-0-691-04289-3},
  langid = {Inglese},
  pagetotal = {799}
}

@inproceedings{hamilton2007,
  title = {Compact {{Hilbert Indices}} for {{Multi}}-{{Dimensional Data}}},
  booktitle = {First {{International Conference}} on {{Complex}}, {{Intelligent}} and {{Software Intensive Systems}}},
  author = {Hamilton, Chris H. and Rau-Chaplin, Andrew},
  date = {2007-04},
  pages = {139--146},
  publisher = {{IEEE}},
  doi = {10.1109/CISIS.2007.16},
  isbn = {978-0-7695-2823-6},
  keywords = {todo}
}

@article{hanagal2017,
  title = {Correlated Gamma Frailty Models for Bivariate Survival Data},
  author = {Hanagal, David D. and Pandey, Arvind and Ganguly, Ayon},
  date = {2017-05-28},
  journaltitle = {Communications in Statistics - Simulation and Computation},
  volume = {46},
  pages = {3627--3644},
  issn = {0361-0918},
  doi = {10.1080/03610918.2015.1085559},
  abstract = {Frailty models are used in the survival analysis to account for the unobserved heterogeneity in individual risks to disease and death. To analyze the bivariate data on related survival times (e.g., matched pairs experiments, twin or family data) the shared frailty models were suggested. Shared frailty models are used despite their limitations. To overcome their disadvantages correlated frailty models may be used. In this article, we introduce the gamma correlated frailty models with two different baseline distributions namely, the generalized log logistic, and the generalized Weibull. We introduce the Bayesian estimation procedure using Markov chain Monte Carlo (MCMC) technique to estimate the parameters involved in these models. We present a simulation study to compare the true values of the parameters with the estimated values. Also we apply these models to a real life bivariate survival dataset related to the kidney infection data and a better model is suggested for the data.},
  keywords = {62F15,62N01,62P10,Bayesian estimation,correlated frailty,Correlated gamma frailty,done,Generalized log-logistic distribution,Generalized Weibull distribution,survival},
  number = {5}
}

@article{hastie1987,
  title = {Generalized {{Additive Models}}: {{Some Applications}}},
  shorttitle = {Generalized {{Additive Models}}},
  author = {Hastie, Trevor and Tibshirani, Robert},
  date = {1987-06},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {82},
  pages = {371--386},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1987.10478440},
  keywords = {done},
  langid = {english},
  number = {398}
}

@article{hastie1989,
  title = {Principal {{Curves}}},
  author = {Hastie, Trevor and Stuetzle, Werner},
  date = {1989-06},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {84},
  pages = {502--516},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1989.10478797},
  keywords = {done},
  langid = {english},
  number = {406}
}

@book{hastie2013,
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  shorttitle = {The Elements of Statistical Learning},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  date = {2013-06-20},
  publisher = {{Springer Nature}},
  location = {{New York}},
  abstract = {This book describes the important ideas in a variety of fields such as medicine, biology, finance, and marketing in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of colour graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorisation, and spectral clustering. There is also a chapter on methods for "wide'' data (p bigger than n), including multiple testing and false discovery rates.},
  isbn = {978-0-387-84857-0},
  langid = {Inglese}
}

@article{hawkins2008,
  title = {Multivariate {{Exponentially Weighted Moving Covariance Matrix}}},
  author = {Hawkins, Douglas M. and Maboudou-Tchao, Edgard M.},
  date = {2008},
  journaltitle = {Technometrics},
  volume = {50},
  pages = {155--166},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  abstract = {Multivariate exponentially weighted moving average (MEWMA) charts are among the best control charts for detecting small changes in any direction. The well-known MEWMA is directed at changes in the mean vector. But changes can occur in either the location or the variability of the correlated multivariate quality characteristics, calling for parallel methodologies for detecting changes in the covariance matrix. This article discusses an exponentially weighted moving covariance matrix for monitoring the stability of the covariance matrix of a process. Used together with the location MEWMA, this chart provides a way to satisfy Shewhart's dictum that proper process control monitor both mean and variability. We show that the chart is competitive, generally outperforming current control charts for the covariance matrix.},
  eprint = {25471456},
  eprinttype = {jstor},
  keywords = {done},
  number = {2}
}

@online{he2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human}}-{{Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-02-06},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  archiveprefix = {arXiv},
  eprint = {1502.01852},
  eprinttype = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,todo},
  primaryclass = {cs}
}

@article{he2016,
  title = {Extensible Grids: Uniform Sampling on a Space Filling Curve},
  shorttitle = {Extensible Grids},
  author = {He, Zhijian and Owen, Art B.},
  date = {2016-09},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  shortjournal = {J. R. Stat. Soc. B},
  volume = {78},
  pages = {917--931},
  issn = {13697412},
  doi = {10.1111/rssb.12132},
  keywords = {done},
  langid = {english},
  number = {4}
}

@article{healy1987,
  title = {A {{Note}} on {{Multivariate CUSUM Procedures}}},
  author = {Healy, John D.},
  date = {1987},
  journaltitle = {Technometrics},
  volume = {29},
  pages = {409--412},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1269451},
  abstract = {Cumulative sum (CUSUM) procedures are among the most powerful tools for detecting a shift from a good quality distribution to a bad quality distribution. This article discusses the natural application of CUSUM procedures to the multivariate normal distribution. It discusses two cases, detecting a shift in the mean vector and detecting a shift in the covariance matrix. As an example, the procedure is applied to measurements taken on optical fibers.},
  eprint = {1269451},
  eprinttype = {jstor},
  keywords = {done},
  number = {4}
}

@article{heil2019,
  title = {Advantages of Fuzzy K-Means over k-Means Clustering in the Classification of Diffuse Reflectance Soil Spectra: {{A}} Case Study with {{West African}} Soils},
  shorttitle = {Advantages of Fuzzy K-Means over k-Means Clustering in the Classification of Diffuse Reflectance Soil Spectra},
  author = {Heil, Jannis and Häring, Volker and Marschner, Bernd and Stumpe, Britta},
  date = {2019-03},
  journaltitle = {Geoderma},
  shortjournal = {Geoderma},
  volume = {337},
  pages = {11--21},
  issn = {00167061},
  doi = {10.1016/j.geoderma.2018.09.004},
  keywords = {done},
  langid = {english}
}

@article{heinze2018,
  title = {Variable Selection – {{A}} Review and Recommendations for the Practicing Statistician},
  author = {Heinze, Georg and Wallisch, Christine and Dunkler, Daniela},
  date = {2018},
  journaltitle = {Biometrical Journal},
  volume = {60},
  pages = {431--449},
  issn = {1521-4036},
  doi = {10.1002/bimj.201700067},
  abstract = {Statistical models support medical research by facilitating individualized outcome prognostication conditional on independent variables or by estimating effects of risk factors adjusted for covariates. Theory of statistical models is well-established if the set of independent variables to consider is fixed and small. Hence, we can assume that effect estimates are unbiased and the usual methods for confidence interval estimation are valid. In routine work, however, it is not known a priori which covariates should be included in a model, and often we are confronted with the number of candidate variables in the range 10–30. This number is often too large to be considered in a statistical model. We provide an overview of various available variable selection methods that are based on significance or information criteria, penalized likelihood, the change-in-estimate criterion, background knowledge, or combinations thereof. These methods were usually developed in the context of a linear regression model and then transferred to more generalized linear models or models for censored survival data. Variable selection, in particular if used in explanatory modeling where effect estimates are of central interest, can compromise stability of a final model, unbiasedness of regression coefficients, and validity of p-values or confidence intervals. Therefore, we give pragmatic recommendations for the practicing statistician on application of variable selection methods in general (low-dimensional) modeling problems and on performing stability investigations and inference. We also propose some quantities based on resampling the entire variable selection process to be routinely reported by software packages offering automated variable selection algorithms.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201700067},
  keywords = {change-in-estimate criterion,done,penalized likelihood,resampling,statistical model,stepwise selection},
  langid = {english},
  number = {3}
}

@inproceedings{heller2008,
  title = {Statistical Models for Partial Membership},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  author = {Heller, Katherine A. and Williamson, Sinead and Ghahramani, Zoubin},
  date = {2008-07-05},
  pages = {392--399},
  publisher = {{Association for Computing Machinery}},
  location = {{Helsinki, Finland}},
  doi = {10.1145/1390156.1390206},
  abstract = {We present a principled Bayesian framework for modeling partial memberships of data points to clusters. Unlike a standard mixture model which assumes that each data point belongs to one and only one mixture component, or cluster, a partial membership model allows data points to have fractional membership in multiple clusters. Algorithms which assign data points partial memberships to clusters can be useful for tasks such as clustering genes based on microarray data (Gasch \& Eisen, 2002). Our Bayesian Partial Membership Model (BPM) uses exponential family distributions to model each cluster, and a product of these distibtutions, with weighted parameters, to model each datapoint. Here the weights correspond to the degree to which the datapoint belongs to each cluster. All parameters in the BPM are continuous, so we can use Hybrid Monte Carlo to perform inference and learning. We discuss relationships between the BPM and Latent Dirichlet Allocation, Mixed Membership models, Exponential Family PCA, and fuzzy clustering. Lastly, we show some experimental results and discuss nonparametric extensions to our model.},
  isbn = {978-1-60558-205-4},
  series = {{{ICML}} '08}
}

@article{hilbert1891,
  title = {Über die stetige Abbildung einer Line auf ein Flächenstück},
  author = {Hilbert, David},
  date = {1891-09-01},
  journaltitle = {Mathematische Annalen},
  shortjournal = {Math. Ann.},
  volume = {38},
  pages = {459--460},
  issn = {1432-1807},
  doi = {10.1007/BF01199431},
  keywords = {done},
  langid = {german},
  number = {3}
}

@book{hinkley1979,
  title = {Theoretical Statistics},
  author = {Hinkley, D. R. Cox {and} D. V.},
  date = {1979-09-06},
  publisher = {{Chapman \& Hall/CRC}},
  location = {{Boca Raton}},
  abstract = {A text that stresses the general concepts of the theory of statistics Theoretical Statistics provides a systematic statement of the theory of statistics, emphasizing general concepts rather than mathematical rigor. Chapters 1 through 3 provide an overview of statistics and discuss some of the basic philosophical ideas and problems behind statistical procedures. Chapters 4 and 5 cover hypothesis testing with simple and null hypotheses, respectively. Subsequent chapters discuss non-parametrics, interval estimation, point estimation, asymptotics, Bayesian procedure, and deviation theory. Student familiarity with standard statistical techniques is assumed.},
  isbn = {978-0-412-16160-5},
  langid = {Inglese},
  pagetotal = {528}
}

@online{hinton2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  date = {2012-07-03},
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  archiveprefix = {arXiv},
  eprint = {1207.0580},
  eprinttype = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,todo},
  primaryclass = {cs}
}

@book{hjort2010,
  title = {Bayesian {{Nonparametrics}}},
  editor = {Hjort, Nils Lid and Holmes, Chris and Müller, Peter and Walker, Stephen G.},
  date = {2010-04-12},
  publisher = {{Cambridge University Press}},
  abstract = {Bayesian nonparametrics works - theoretically, computationally. The theory provides highly flexible models whose complexity grows appropriately with the amount of data. Computational issues, though challenging, are no longer intractable. All that is needed is an entry point: this intelligent book is the perfect guide to what can seem a forbidding landscape. Tutorial chapters by Ghosal, Lijoi and Prünster, Teh and Jordan, and Dunson advance from theory, to basic models and hierarchical modeling, to applications and implementation, particularly in computer science and biostatistics. These are complemented by companion chapters by the editors and Griffin and Quintana, providing additional models, examining computational issues, identifying future growth areas, and giving links to related topics. This coherent text gives ready access both to underlying principles and to state-of-the-art practice. Specific examples are drawn from information retrieval, NLP, machine vision, computational biology, biostatistics, and bioinformatics.},
  isbn = {978-0-521-51346-3},
  langid = {english}
}

@book{hoff2009,
  title = {A First Course in Bayesian Statistical Methods},
  author = {Hoff, Peter D.},
  date = {2009},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  abstract = {This book provides a compact self-contained introduction to the theory and application of Bayesian statistical methods. The book is accessible to readers having a basic familiarity with probability, yet allows more advanced readers to quickly grasp the principles underlying Bayesian theory and methods. The examples and computer code allow the reader to understand and implement basic Bayesian data analyses using standard statistical models and to extend the standard models to specialized data analysis situations. The book begins with fundamental notions such as probability, exchangeability and Bayes' rule, and ends with modern topics such as variable selection in regression, generalized linear mixed effects models, and semiparametric copula estimation. Numerous examples from the social, biological and physical sciences show how to implement these methodologies in practice. Monte Carlo summaries of posterior distributions play an important role in Bayesian data analysis. The open-source R statistical computing environment provides sufficient functionality to make Monte Carlo estimation very easy for a large number of statistical models and example R-code is provided throughout the text. Much of the example code can be run ``as is'' in R, and essentially all of it can be run after downloading the relevant datasets from the companion website for this book. Peter Hoff is an Associate Professor of Statistics and Biostatistics at the University of Washington. He has developed a variety of Bayesian methods for multivariate data, including covariance and copula estimation, cluster analysis, mixture modeling and social network analysis. He is on the editorial board of the Annals of Applied Statistics.},
  isbn = {978-0-387-92299-7},
  series = {Springer {{Texts}} in {{Statistics}}}
}

@online{hoff2015,
  title = {Dyadic Data Analysis with Amen},
  author = {Hoff, Peter D.},
  date = {2015-06-26},
  abstract = {Dyadic data on pairs of objects, such as relational or social network data, often exhibit strong statistical dependencies. Certain types of second-order dependencies, such as degree heterogeneity and reciprocity, can be well-represented with additive random effects models. Higher-order dependencies, such as transitivity and stochastic equivalence, can often be represented with multiplicative effects. The "amen" package for the R statistical computing environment provides estimation and inference for a class of additive and multiplicative random effects models for ordinal, continuous, binary and other types of dyadic data. The package also provides methods for missing, censored and fixed-rank nomination data, as well as longitudinal dyadic data. This tutorial illustrates the "amen" package via example statistical analyses of several of these different data types.},
  archiveprefix = {arXiv},
  eprint = {1506.08237},
  eprinttype = {arxiv},
  keywords = {62-07; 62F15,heterogeneity,network model,social,Statistics - Computation,Statistics - Methodology,todo},
  primaryclass = {stat}
}

@online{hoff2018,
  title = {Additive and Multiplicative Effects Network Models},
  author = {Hoff, Peter D.},
  date = {2018-07-20},
  abstract = {Network datasets typically exhibit certain types of statistical dependencies, such as within-dyad correlation, row and column heterogeneity, and third-order dependence patterns such as transitivity and clustering. The first two of these can be well-represented statistically with a social relations model, a type of additive random effects model originally developed for continuous dyadic data. Third-order patterns can be represented with multiplicative random effects models, which are related to matrix decompositions commonly used for matrix-variate data analysis. Additionally, these multiplicative random effects models generalize other popular latent variable network models, such as the stochastic blockmodel and the latent space model. In this article we review a general regression framework for the analysis of network data that combines these two types of random effects and accommodates a variety of network data types, including continuous, binary and ordinal network relations.},
  archiveprefix = {arXiv},
  eprint = {1807.08038},
  eprinttype = {arxiv},
  keywords = {62H25; 62F15,bayes,network models,Statistics - Methodology,todo},
  primaryclass = {stat}
}

@article{hoffman2014,
  title = {The {{No}}-{{U}}-{{Turn Sampler}}: {{Adaptively Setting Path Lengths}} in {{Hamiltonian Monte Carlo}}},
  shorttitle = {The {{No}}-{{U}}-{{Turn Sampler}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  date = {2014},
  journaltitle = {Journal of Machine Learning Research},
  volume = {15},
  pages = {1593--1623},
  issn = {1533-7928},
  keywords = {doing},
  number = {47}
}

@article{hofmann2020,
  title = {{{lmSubsets}}: {{Exact Variable}}-{{Subset Selection}} in {{Linear Regression}} for {{R}}},
  shorttitle = {{{lmSubsets}}},
  author = {Hofmann, Marc and Gatu, Cristian and Kontoghiorghes, Erricos J. and Colubi, Ana and Zeileis, Achim},
  date = {2020-04-28},
  journaltitle = {Journal of Statistical Software},
  volume = {93},
  pages = {1--21},
  issn = {1548-7660},
  doi = {10.18637/jss.v093.i03},
  issue = {1},
  keywords = {best-subset regression,linear regression,model selection,R,todo,variable selection},
  langid = {english},
  number = {1}
}

@article{horrace2005,
  title = {Some Results on the Multivariate Truncated Normal Distribution},
  author = {Horrace, William C.},
  date = {2005-05-01},
  journaltitle = {Journal of Multivariate Analysis},
  shortjournal = {Journal of Multivariate Analysis},
  volume = {94},
  pages = {209--221},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2004.10.007},
  abstract = {This note formalizes some analytical results on the n-dimensional multivariate truncated normal distribution where truncation is one-sided and at an arbitrary point. Results on linear transformations, marginal and conditional distributions, and independence are provided. Also, results on log-concavity, A-unimodality and the MTP2 property are derived.},
  keywords = {A-unimodality,Characteristic function,done,Log-concavity},
  langid = {english},
  number = {1}
}

@book{hubbard2015,
  title = {Data {{Structures}} and {{Algorithms}} with {{Python}}},
  author = {Hubbard, Kent D. Lee;Steve},
  date = {2015},
  publisher = {{Springer}}
}

@article{hullait2020,
  title = {Robust {{Function}}-on-{{Function Regression}}},
  author = {Hullait, Harjit and Leslie, David S. and Pavlidis, Nicos G. and King, Steve},
  date = {2020-07-29},
  journaltitle = {Technometrics},
  volume = {0},
  pages = {1--14},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2020.1802350},
  abstract = {Functional linear regression is a widely used approach to model functional responses with respect to functional inputs. However, classical functional linear regression models can be severely affected by outliers. We therefore introduce a Fisher-consistent robust functional linear regression model that is able to effectively fit data in the presence of outliers. The model is built using robust functional principal component and least squares regression estimators. The performance of the functional linear regression model depends on the number of principal components used. We therefore introduce a consistent robust model selection procedure to choose the number of principal components. Our robust functional linear regression model can be used alongside an outlier detection procedure to effectively identify abnormal functional responses. A simulation study shows our method is able to effectively capture the regression behavior in the presence of outliers, and is able to find the outliers with high accuracy. We demonstrate the usefulness of our method on jet engine sensor data. We identify outliers that would not be found if the functional responses were modeled independently of the functional input, or using nonrobust methods.},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2020.1802350},
  keywords = {doing,Outlier detection,Robust functional data analysis,Robust model selection},
  number = {0}
}

@article{hyvarinen2000,
  title = {Independent Component Analysis: Algorithms and Applications},
  shorttitle = {Independent Component Analysis},
  author = {Hyvärinen, A. and Oja, E.},
  date = {2000-06-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {13},
  pages = {411--430},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(00)00026-5},
  abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
  keywords = {Blind signal separation,done,Factor analysis,Independent component analysis,Projection pursuit,Representation,Source separation},
  langid = {english},
  number = {4}
}

@book{imbens2015,
  title = {Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction},
  shorttitle = {Causal Inference for Statistics, Social, and Biomedical Sciences},
  author = {Imbens, Guido W. and Rubin, Donald B.},
  date = {2015-04-06},
  publisher = {{Cambridge University Press}},
  location = {{New York}},
  abstract = {Most questions in social and biomedical sciences are causal in nature: what would happen to individuals, or to groups, if part of their environment were changed? In this groundbreaking text, two world-renowned experts present statistical methods for studying such questions. This book starts with the notion of potential outcomes, each corresponding to the outcome that would be realized if a subject were exposed to a particular treatment or regime. In this approach, causal effects are comparisons of such potential outcomes. The fundamental problem of causal inference is that we can only observe one of the potential outcomes for a particular subject. The authors discuss how randomized experiments allow us to assess causal effects and then turn to observational studies. They lay out the assumptions needed for causal inference and describe the leading analysis methods, including matching, propensity-score methods, and instrumental variables. Many detailed applications are included, with special focus on practical aspects for the empirical researcher.},
  isbn = {978-0-521-88588-1},
  langid = {Inglese},
  pagetotal = {644}
}

@book{irizarry,
  title = {Data {{Analysis}} for the {{Life Sciences}} with {{R}}},
  author = {Irizarry, Rafael A. and Love, Michael I.},
  abstract = {This book covers several of the statistical concepts and data analytic skills needed to succeed in data-driven life science research. The authors proceed from relatively basic concepts related to computed p-values to advanced topics related to analyzing highthroughput data. They include the R code that performs this analysis and connect the lines of code to the statistical and mathematical concepts explained.}
}

@article{ishwaran2001,
  title = {Gibbs {{Sampling Methods}} for {{Stick}}-{{Breaking Priors}}},
  author = {Ishwaran, Hemant and James, Lancelot F.},
  date = {2001-03-01},
  journaltitle = {Journal of the American Statistical Association},
  volume = {96},
  pages = {161--173},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214501750332758},
  abstract = {A rich and flexible class of random probability measures, which we call stick-breaking priors, can be constructed using a sequence of independent beta random variables. Examples of random measures that have this characterization include the Dirichlet process, its two-parameter extension, the two-parameter Poisson–Dirichlet process, finite dimensional Dirichlet priors, and beta two-parameter processes. The rich nature of stick-breaking priors offers Bayesians a useful class of priors for nonparametric problems, while the similar construction used in each prior can be exploited to develop a general computational procedure for fitting them. In this article we present two general types of Gibbs samplers that can be used to fit posteriors of Bayesian hierarchical models based on stick-breaking priors. The first type of Gibbs sampler, referred to as a Pólya urn Gibbs sampler, is a generalized version of a widely used Gibbs sampling method currently employed for Dirichlet process computing. This method applies to stick-breaking priors with a known Pólya urn characterization, that is, priors with an explicit and simple prediction rule. Our second method, the blocked Gibbs sampler, is based on an entirely different approach that works by directly sampling values from the posterior of the random measure. The blocked Gibbs sampler can be viewed as a more general approach because it works without requiring an explicit prediction rule. We find that the blocked Gibbs avoids some of the limitations seen with the Pólya urn approach and should be simpler for nonexperts to use.},
  annotation = {\_eprint: https://doi.org/10.1198/016214501750332758},
  keywords = {Blocked Gibbs sampler,Dirichlet process,Generalized Dirichlet distribution,Pitman–Yor process,Pólya urn Gibbs sampler,Prediction rule,Random probability measure,Random weights,Stable law,todo},
  number = {453}
}

@article{ishwaran2002,
  title = {Exact and {{Approximate Sum Representations}} for the {{Dirichlet Process}}},
  author = {Ishwaran, Hemant and Zarepour, Mahmoud},
  date = {2002},
  journaltitle = {The Canadian Journal of Statistics},
  volume = {30},
  pages = {269--283},
  publisher = {{[Statistical Society of Canada, Wiley]}},
  issn = {0319-5724},
  doi = {10.2307/3315951},
  abstract = {The Dirichlet process can be regarded as a random probability measure for which the authors examine various sum representations. They consider in particular the gamma process construction of Ferguson (1973) and the "stick-breaking" construction of Sethuraman (1994). They propose a Dirichlet finite sum representation that strongly approximates the Dirichlet process. They assess the accuracy of this approximation and characterize the posterior that this new prior leads to in the context of Bayesian nonparametric hierarchical models. /// Le processus de Dirichlet constitue une mesure de probabilité aléatoire dont les auteurs examinent différentes représentations à l'aide de sommes. Ils s'intéressent en particulier à la construction de Ferguson (1973) fondée sur la loi gamma et à la construction dite à "bâtons rompus" de Sethuraman (1994). Ils proposent une approximation forte du processus de Dirichlet par somme finie de type Dirichlet. Ils évaluent la qualité de cette approximation qui conduit à une loi a priori dont ils caractérisent la loi a posteriori dans le cadre des modèles bayésiens hiérarchiques non paramétriques.},
  eprint = {3315951},
  eprinttype = {jstor},
  number = {2}
}

@article{ishwaran2005,
  title = {Spike and Slab Variable Selection: {{Frequentist}} and {{Bayesian}} Strategies},
  shorttitle = {Spike and Slab Variable Selection},
  author = {Ishwaran, Hemant and Rao, J. Sunil},
  date = {2005-04},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {33},
  pages = {730--773},
  issn = {0090-5364},
  doi = {10.1214/009053604000001147},
  keywords = {doing},
  langid = {english},
  number = {2}
}

@book{jackman2009,
  title = {Bayesian Analysis for the Social Sciences},
  author = {Jackman, Simon},
  date = {2009-10-23},
  publisher = {{John Wiley \& Sons Inc}},
  location = {{Chichester, U.K}},
  abstract = {Bayesian methods are increasingly being used in the social sciences, as the problems encountered lend themselves so naturally to the subjective qualities of Bayesian methodology. This book provides an accessible introduction to Bayesian methods, tailored specifically for social science students. It contains lots of real examples from political science, psychology, sociology, and economics, exercises in all chapters, and detailed descriptions of all the key concepts, without assuming any background in statistics beyond a first course. It features examples of how to implement the methods using WinBUGS \&; the most-widely used Bayesian analysis software in the world \&; and R \&; an open-source statistical software. The book is supported by a Website featuring WinBUGS and R code, and data sets.},
  isbn = {978-0-470-01154-6},
  langid = {Inglese},
  pagetotal = {564}
}

@online{jacob2017,
  title = {Better Together? {{Statistical}} Learning in Models Made of Modules},
  shorttitle = {Better Together?},
  author = {Jacob, Pierre E. and Murray, Lawrence M. and Holmes, Chris C. and Robert, Christian P.},
  date = {2017},
  abstract = {In modern applications, statisticians are faced with integrating heterogeneous data modalities relevant for an inference, prediction, or decision problem. In such circumstances, it is convenient to use a graphical model to represent the statistical dependencies, via a set of connected "modules", each relating to a specific data modality, and drawing on specific domain expertise in their development. In principle, given data, the conventional statistical update then allows for coherent uncertainty quantification and information propagation through and across the modules. However, misspecification of any module can contaminate the estimate and update of others, often in unpredictable ways. In various settings, particularly when certain modules are trusted more than others, practitioners have preferred to avoid learning with the full model in favor of approaches that restrict the information propagation between modules, for example by restricting propagation to only particular directions along the edges of the graph. In this article, we investigate why these modular approaches might be preferable to the full model in misspecified settings. We propose principled criteria to choose between modular and full-model approaches. The question arises in many applied settings, including large stochastic dynamical systems, meta-analysis, epidemiological models, air pollution models, pharmacokinetics-pharmacodynamics, and causal inference with propensity scores.},
  archiveprefix = {arXiv},
  eprint = {1708.08719},
  eprinttype = {arxiv},
  keywords = {done,modules statistical learning,Statistics - Methodology}
}

@book{james2013,
  title = {An Introduction to Statistical Learning: With Applications in R},
  shorttitle = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  date = {2013-06-25},
  publisher = {{Springer Verlag}},
  location = {{New York}},
  abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
  isbn = {978-1-4614-7137-0},
  langid = {Inglese},
  pagetotal = {426}
}

@incollection{jensen2016,
  title = {Bayesian {{Graphical Models}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Jensen, Finn V. and Nielsen, Thomas D.},
  date = {2016},
  pages = {1--9},
  publisher = {{American Cancer Society}},
  doi = {10.1002/9781118445112.stat07360.pub2},
  abstract = {Mathematically, a Bayesian graphical model is a compact representation of the joint probability distribution for a set of variables. The most frequently used type of Bayesian graphical models are Bayesian networks. The structural part of a Bayesian graphical model is a graph consisting of nodes and edges. The nodes represent variables, which may be either discrete or continuous. An edge between two nodes A and B indicates a direct influence between the state of A and the state of B, which in some domains can also be interpreted as a causal relation. The widespread use of Bayesian networks is largely due to the availability of efficient inference algorithms for answering probabilistic queries about the states of the variables in the network. Furthermore, to support the construction of Bayesian network models, learning algorithms are also available. We give an overview of the Bayesian network formalism and some of the algorithmic developments in the area.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat07360.pub2},
  isbn = {978-1-118-44511-2},
  keywords = {Bayesian networks,inference algorithms,learning algorithms,probabilistic graphical models,todo},
  langid = {english}
}

@misc{jia2019,
  title = {Parametric {{Curves}}},
  author = {Jia, Yan-Bin},
  date = {2019-08-10},
  langid = {english}
}

@article{jiang2004,
  title = {The {{Indirect Method}}: {{Inference Based}} on {{Intermediate Statistics}}—{{A Synthesis}} and {{Examples}}},
  shorttitle = {The {{Indirect Method}}},
  author = {Jiang, Wenxin and Turnbull, Bruce},
  date = {2004-05},
  journaltitle = {Statistical Science},
  volume = {19},
  pages = {239--263},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/088342304000000152},
  abstract = {This article presents an exposition and synthesis of the theory and some applications of the so-called indirect method of inference. These ideas have been exploited in the field of econometrics, but less so in other fields such as biostatistics and epidemiology. In the indirect method, statistical inference is based on an intermediate statistic, which typically follows an asymptotic normal distribution, but is not necessarily a consistent estimator of the parameter of interest. This intermediate statistic can be a naive estimator based on a convenient but misspecified model, a sample moment or a solution to an estimating equation. We review a procedure of indirect inference based on the generalized method of moments, which involves adjusting the naive estimator to be consistent and asymptotically normal. The objective function of this procedure is shown to be interpretable as an “indirect likelihood” based on the intermediate statistic. Many properties of the ordinary likelihood function can be extended to this indirect likelihood. This method is often more convenient computationally than maximum likelihood estimation when handling such model complexities as random effects and measurement error, for example, and it can also serve as a basis for robust inference and model selection, with less stringent assumptions on the data generating mechanism. Many familiar estimation techniques can be viewed as examples of this approach. We describe applications to measurement error, omitted covariates and recurrent events. A dataset concerning prevention of mammary tumors in rats is analyzed using a Poisson regression model with overdispersion. A second dataset from an epidemiological study is analyzed using a logistic regression model with mismeasured covariates. A third dataset of exam scores is used to illustrate robust covariance selection in graphical models.},
  keywords = {asymptotic normality,bias correction,consistency,efficiency,estimating equations,generalized method of moments,graphical models,Indirect inference,indirect likelihood,measurement error,missing data,Model selection,naive estimators,omitted covariates,overdispersion,quasi-likelihood,random effects,robustness,todo},
  number = {2}
}

@article{jones-farmer2009,
  title = {Distribution-{{Free Phase I Control Charts}} for {{Subgroup Location}}},
  author = {Jones-Farmer, L. Allison and Jordan, Victoria and Champ, Charles W.},
  date = {2009-07-01},
  journaltitle = {Journal of Quality Technology},
  volume = {41},
  pages = {304--316},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2009.11917784},
  abstract = {Much of the work in statistical quality control is dependent on the proper completion of a Phase I study. Many Phase I control charts are based on an implicit assumption of normally distributed process observations. In the beginning stages of process control, little information is available about the process and the normality assumption may not be reasonable. Existing robust and distribution-free control charts are concerned with the establishment of Phase II control limits that are robust to nonnormality or outliers from the Phase I sample. Our literature review revealed no purely distribution-free Phase I control-chart methods. We propose a distribution-free method for defining the in-control state of a process and identifying an in-control reference sample. The resultant reference sample can be used to estimate the process parameters for the Phase II procedure of choice. The proposed rank-based method is compared with the traditional X chart using Monte Carlo simulation. The rank-based method compares favorably to the X chart when the process is normally distributed and performs better than the X chart in many situations when the process distribution is skewed or heavy tailed.},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2009.11917784},
  keywords = {done,Nonparametric,Shewhart Chart,Statistical Process Control},
  number = {3}
}

@article{jones-farmer2010,
  title = {A {{Distribution}}-{{Free Phase I Control Chart}} for {{Subgroup Scale}}},
  author = {Jones-Farmer, L. Allison and Champ, Charles W.},
  date = {2010-10},
  journaltitle = {Journal of Quality Technology},
  shortjournal = {Journal of Quality Technology},
  volume = {42},
  pages = {373--387},
  issn = {0022-4065, 2575-6230},
  doi = {10.1080/00224065.2010.11917834},
  keywords = {done},
  langid = {english},
  number = {4}
}

@article{jr2010,
  title = {An {{Evaluation}} of a {{GLR Control Chart}} for {{Monitoring}} the {{Process Mean}}},
  author = {Jr, Marion R. Reynolds and Lou, Jianying},
  date = {2010-07-01},
  journaltitle = {Journal of Quality Technology},
  volume = {42},
  pages = {287--310},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2010.11917825},
  abstract = {This paper considers the problem of monitoring the mean of a normally distributed process variable when the objective is to effectively detect both small and large shifts in this mean. The performance of a generalized likelihood ratio (GLR) control chart is evaluated, where the likelihood ratio is based on a moving window of past observations. The performance of the GLR chart is compared with the performance of other options, such as combinations of Shewhart and cumulative sum (CUSUM) charts and an adaptive CUSUM chart, that have been proposed for detecting a wide range of shift sizes. Performance is evaluated for sustained shifts, transient shifts, and drifts in the mean. It is shown that the overall performance of the GLR chart is at least as good as these other options. These other options have multiple control-chart parameters that allow for the charts to be tuned to be more sensitive to certain shifts that may be of interest. However, the GLR chart does not require users to specify the values of any control-chart parameters other than the size of the window and the control limit. We recommend a specific window size and provide a table of control limits corresponding to specified values of the false-alarm rate, so it is very easy to design the GLR chart for use in applications. Simulating the performance of the GLR chart is time consuming, but approximating the GLR chart with a set of CUSUM charts provides a much faster way of evaluating the performance of the GLR chart for research purposes.},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2010.11917825},
  keywords = {Adaptive Control Chart,Average Time to Signal,CUSUM Chart,done,Generalized Likelihood Ratio,Shewhart Chart,Statistical Process Control,Steady State,Surveillance},
  number = {3}
}

@article{kalli2011,
  title = {Slice Sampling Mixture Models},
  author = {Kalli, Maria and Griffin, Jim E. and Walker, Stephen G.},
  date = {2011-01},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {21},
  pages = {93--105},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-009-9150-y},
  keywords = {todo},
  langid = {english},
  number = {1}
}

@book{keener2010,
  title = {Theoretical {{Statistics}}: {{Topics}} for a {{Core Course}}},
  shorttitle = {Theoretical {{Statistics}}},
  author = {Keener, Robert W.},
  date = {2010-09-08},
  edition = {2010 edition},
  publisher = {{Springer}},
  abstract = {Intended as the text for a sequence of advanced courses, this book covers major topics in theoretical statistics in a concise and rigorous fashion. The discussion assumes a background in advanced calculus, linear algebra, probability, and some analysis and topology. Measure theory is used, but the notation and basic results needed are presented in an initial chapter on probability, so prior knowledge of these topics is not essential.The presentation is designed to expose students to as many of the central ideas and topics in the discipline as possible, balancing various approaches to inference as well as exact, numerical, and large sample methods. Moving beyond more standard material, the book includes chapters introducing bootstrap methods, nonparametric regression, equivariant estimation, empirical Bayes, and sequential design and analysis.The book has a rich collection of exercises. Several of them illustrate how the theory developed in the book may be used in various applications. Solutions to many of the exercises are included in an appendix.},
  langid = {english},
  pagetotal = {538}
}

@article{kiefer1952,
  title = {Stochastic {{Estimation}} of the {{Maximum}} of a {{Regression Function}}},
  author = {Kiefer, J. and Wolfowitz, J.},
  date = {1952-09},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {23},
  pages = {462--466},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729392},
  abstract = {Let \$M(x)\$ be a regression function which has a maximum at the unknown point \$\textbackslash theta. M(x)\$ is itself unknown to the statistician who, however, can take observations at any level \$x\$. This paper gives a scheme whereby, starting from an arbitrary point \$x\_1\$, one obtains successively \$x\_2, x\_3, \textbackslash cdots\$ such that \$x\_n\$ converges to \$\textbackslash theta\$ in probability as \$n \textbackslash rightarrow \textbackslash infty\$.},
  keywords = {todo},
  number = {3}
}

@online{kingma2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  keywords = {Computer Science - Machine Learning,done}
}

@article{kolouri2017,
  title = {Optimal {{Mass Transport}}: {{Signal}} Processing and Machine-Learning Applications},
  shorttitle = {Optimal {{Mass Transport}}},
  author = {Kolouri, Soheil and Park, Se Rim and Thorpe, Matthew and Slepcev, Dejan and Rohde, Gustavo K.},
  date = {2017-07},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {34},
  pages = {43--59},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2695801},
  abstract = {Transport-based techniques for signal and data analysis have recently received increased interest. Given their ability to provide accurate generative models for signal intensities and other data distributions, they have been used in a variety of applications, including content-based retrieval, cancer detection, image superresolution, and statistical machine learning, to name a few, and they have been shown to produce state-of-the-art results. Moreover, the geometric characteristics of transport-related metrics have inspired new kinds of algorithms for interpreting the meaning of data distributions. Here, we provide a practical overview of the mathematical underpinnings of mass transport-related methods, including numerical implementation, as well as a review, with demonstrations, of several applications. Software accompanying this article is available from [43].},
  keywords = {Analytical models,data analysis,data distributions,Data models,Estimation,generative models,geometric characteristics,image processing,learning (artificial intelligence),Linear programming,machine-learning applications,mass transport-related methods,metric space,Morphology,optimal transport,Probability density function,signal analysis,signal intensities,signal processing,todo,transport-based techniques,transport-related metrics,Transportation,wasserstein distance},
  number = {4}
}

@inproceedings{kotecha1999,
  title = {Gibbs Sampling Approach for Generation of Truncated Multivariate {{Gaussian}} Random Variables},
  booktitle = {1999 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}.},
  author = {Kotecha, J.H. and Djuric, P.M.},
  date = {1999},
  pages = {1757-1760 vol.3},
  publisher = {{IEEE}},
  doi = {10.1109/ICASSP.1999.756335},
  isbn = {978-0-7803-5041-0},
  keywords = {todo}
}

@article{kowal2020,
  title = {Simultaneous Transformation and Rounding ({{STAR}}) Models for Integer-Valued Data},
  author = {Kowal, Daniel R. and Canale, Antonio},
  date = {2020},
  journaltitle = {Electronic Journal of Statistics},
  shortjournal = {Electron. J. Statist.},
  volume = {14},
  pages = {1744--1772},
  publisher = {{The Institute of Mathematical Statistics and the Bernoulli Society}},
  issn = {1935-7524},
  doi = {10.1214/20-EJS1707},
  abstract = {We propose a simple yet powerful framework for modeling integer-valued data, such as counts, scores, and rounded data. The data-generating process is defined by Simultaneously Transforming and Rounding (STAR) a continuous-valued process, which produces a flexible family of integer-valued distributions capable of modeling zero-inflation, bounded or censored data, and over- or underdispersion. The transformation is modeled as unknown for greater distributional flexibility, while the rounding operation ensures a coherent integer-valued data-generating process. An efficient MCMC algorithm is developed for posterior inference and provides a mechanism for adaptation of successful Bayesian models and algorithms for continuous data to the integer-valued data setting. Using the STAR framework, we design a new Bayesian Additive Regression Tree model for integer-valued data, which demonstrates impressive predictive distribution accuracy for both synthetic data and a large healthcare utilization dataset. For interpretable regression-based inference, we develop a STAR additive model, which offers greater flexibility and scalability than existing integer-valued models. The STAR additive model is applied to study the recent decline in Amazon river dolphins.},
  keywords = {Additive models,BART,count data,done,nonparametric regression,prediction},
  langid = {english},
  mrnumber = {MR4083734},
  number = {1},
  zmnumber = {07200242}
}

@article{kreiss2011,
  title = {Bootstrap Methods for Dependent Data: {{A}} Review},
  shorttitle = {Bootstrap Methods for Dependent Data},
  author = {Kreiss, Jens-Peter and Paparoditis, Efstathios},
  date = {2011-12-01},
  journaltitle = {Journal of the Korean Statistical Society},
  shortjournal = {Journal of the Korean Statistical Society},
  volume = {40},
  pages = {357--378},
  issn = {1226-3192},
  doi = {10.1016/j.jkss.2011.08.009},
  abstract = {This paper gives a review on a variety of bootstrap methods for dependent data. The main focus is not on an exhaustive listing and description of bootstrap procedures but on general principles which should be taken into account when selecting a particular bootstrap procedure in order to approximate the (properly standardized) distribution of a statistic of interest. Questions are considered related to which dependence properties of the underlying data generating process asymptotically influence the distribution of the statistic of interest and which dependence properties (or even which process) a particular bootstrap method really mimics. For answering these questions we introduce the concept of a companion stochastic process. As statistics we consider generalized means, and integrated periodogram statistics (including ratio statistics) as well as nonparametric estimators.},
  keywords = {Bootstrap methods,Stochastic processes,Time series,todo},
  langid = {english},
  number = {4}
}

@book{kreyszig1989,
  title = {Introductory {{Functional Analysis}} with {{Applications}}},
  author = {Kreyszig, Erwin},
  date = {1989-02-23},
  edition = {1 edition},
  publisher = {{Wiley}},
  location = {{New York}},
  abstract = {Provides avenues for applying functional analysis to the practical study of natural sciences as well as mathematics. Contains worked problems on Hilbert space theory and on Banach spaces and emphasizes concepts, principles, methods and major applications of functional analysis.},
  isbn = {978-0-471-50459-7},
  langid = {english},
  pagetotal = {704}
}

@book{kroese2013,
  title = {Statistical Modeling and Computation},
  author = {Kroese, Dirk P. and Chan, Joshua C. C.},
  date = {2013-11-15},
  publisher = {{Springer-Nature New York Inc}},
  location = {{New York}},
  abstract = {This textbook on statistical modeling and statistical inference will assist advanced undergraduate and graduate students. Statistical Modeling and Computation\&nbsp;provides a unique introduction to modern Statistics from both classical and Bayesian perspectives. It also offers\&nbsp;an integrated treatment of Mathematical Statistics and modern statistical computation, emphasizing statistical modeling, computational techniques, and applications. Each of the three parts will cover topics essential to university courses. Part I covers the fundamentals of probability theory. In Part II, the authors introduce a wide variety of classical models that include, among others, linear regression and ANOVA models. In Part III,\&nbsp;the authors\&nbsp;address the statistical analysis and computation of various advanced models, such as generalized linear, state-space and Gaussian models. Particular attention is paid to fast Monte Carlo techniques for Bayesian inference on these models. Throughout the book the authors\&nbsp;include a large number of illustrative examples and solved problems. The book also features a section with solutions, an appendix that serves as a MATLAB primer, and a mathematical supplement.?},
  isbn = {978-1-4614-8774-6},
  langid = {Inglese},
  pagetotal = {400}
}

@online{kucukelbir2015,
  title = {Automatic {{Variational Inference}} in {{Stan}}},
  author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
  date = {2015-06-12},
  abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult to automate. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI). The user only provides a Bayesian model and a dataset; nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We implement ADVI in Stan (code available now), a probabilistic programming framework. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.},
  archiveprefix = {arXiv},
  eprint = {1506.03431},
  eprinttype = {arxiv},
  keywords = {done,Statistics - Machine Learning},
  primaryclass = {stat}
}

@book{kushner1997,
  title = {Stochastic {{Approximation}} and {{Recursive Algorithms}} and {{Applications}}},
  author = {Kushner, Harold and Yin, George},
  date = {1997},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  doi = {10.1007/978-1-4899-2696-8},
  abstract = {In recent years algorithms of the stochastic approximation type have found applications in new and diverse areas, and new techniques have been developed for proofs of convergence and rate of convergence. The actual and potential applications in signal processing have exploded. New challenges have arisen in applications to adaptive control. This book presents a thorough coverage of the ODE method used to analyze these algorithms.},
  isbn = {978-1-4899-2696-8},
  langid = {english},
  series = {Stochastic {{Modelling}} and {{Applied Probability}}}
}

@book{kwon2016,
  title = {Julia Programming for Operations Research: A Primer on Computing},
  shorttitle = {Julia Programming for Operations Research},
  author = {Kwon, Changhyun},
  date = {2016-05-29},
  publisher = {{CreateSpace Independent Publishing Platform}},
  location = {{Charleston, SC}},
  abstract = {Last Updated: May 2018The main motivation of writing this book was to help the author himself. He is a professor in the field of operations research, and his daily activities involve building models of mathematical optimization, developing algorithms for solving the problems, implementing those algorithms using computer programming languages, experimenting with data, etc. Three languages are involved: human language, mathematical language, and computer language. His team of students need to go over three different languages, which requires "translation" among the three languages. As this book was written to teach his research group how to translate, this book will also be useful for anyone who needs to learn how to translate in a similar situation.The Julia Language is as fast as C, as convenient as MATLAB, and as general as Python with a flexible algebraic modeling language for mathematical optimization problems. With the great support from Julia developers, especially the developers of the JuMP—Julia for Mathematical Programming—package, Julia makes a perfect tool for students and professionals in operations research and related areas such as industrial engineering, management science, transportation engineering, economics, and regional science.For more information, visit: http://www.chkwon.net/julia},
  isbn = {978-1-5333-2879-3},
  langid = {Inglese},
  pagetotal = {248}
}

@book{kwong2020,
  title = {Hands-On Design Patterns and Best Practices with Julia: Proven solutions to common problems in software design for Julia 1.x},
  shorttitle = {Hands-On Design Patterns and Best Practices with Julia},
  author = {Kwong, Tom and Karpinski, Stefan},
  date = {2020-01-17},
  edition = {1 edizione},
  publisher = {{Packt Publishing}},
  abstract = {Design and develop high-performance, reusable, and maintainable applications using traditional and modern Julia patterns with this comprehensive guideKey FeaturesExplore useful design patterns along with object-oriented programming in Julia 1.0Implement macros and metaprogramming techniques to make your code faster, concise, and efficientDevelop the skills necessary to implement design patterns for creating robust and maintainable applicationsBook DescriptionDesign patterns are fundamental techniques for developing reusable and maintainable code. They provide a set of proven solutions that allow developers to solve problems in software development quickly. This book will demonstrate how to leverage design patterns with real-world applications.Starting with an overview of design patterns and best practices in application design, you'll learn about some of the most fundamental Julia features such as modules, data types, functions/interfaces, and metaprogramming. You'll then get to grips with the modern Julia design patterns for building large-scale applications with a focus on performance, reusability, robustness, and maintainability. The book also covers anti-patterns and how to avoid common mistakes and pitfalls in development. You'll see how traditional object-oriented patterns can be implemented differently and more effectively in Julia. Finally, you'll explore various use cases and examples, such as how expert Julia developers use design patterns in their open source packages.By the end of this Julia programming book, you'll have learned methods to improve software design, extensibility, and reusability, and be able to use design patterns efficiently to overcome common challenges in software development.What you will learnMaster the Julia language features that are key to developing large-scale software applicationsDiscover design patterns to improve overall application architecture and designDevelop reusable programs that are modular, extendable, performant, and easy to maintainWeigh up the pros and cons of using different design patterns for use casesExplore methods for transitioning from object-oriented programming to using equivalent or more advanced Julia techniquesWho this book is forThis book is for beginner to intermediate-level Julia programmers who want to enhance their skills in designing and developing large-scale applications. Table of ContentsDesign Patterns and Related PrinciplesModules, Packages, and Data Type ConceptsDesigning Functions and InterfacesMacros and Meta Programming TechniquesReusability PatternsPerformance PatternsMaintainability PatternsRobustness PatternsMiscellaneous PatternsAnti-PatternsObject Oriented Traditional PatternsInheritance and Variance},
  keywords = {julia,todo},
  langid = {Inglese},
  pagetotal = {532}
}

@incollection{lad2016,
  title = {Stable {{Estimation}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Lad, Frank},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2016-05-17},
  pages = {1--2},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat00248.pub2},
  isbn = {978-1-118-44511-2},
  keywords = {todo},
  langid = {english}
}

@inproceedings{lakshminarayanan2015,
  title = {Particle {{Gibbs}} for {{Bayesian Additive Regression Trees}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Lakshminarayanan, Balaji and Roy, Daniel and Teh, Yee Whye},
  date = {2015-02-21},
  pages = {553--561},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Additive regression trees are flexible non-parametric models and popular off-the-shelf tools for real-world non-linear regression. In application domains, such as bioinformatics, where there is als...},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  keywords = {todo},
  langid = {english}
}

@article{lall2021,
  title = {The {{MIDAS Touch}}: {{Accurate}} and {{Scalable Missing}}-{{Data Imputation}} with {{Deep Learning}}},
  shorttitle = {The {{MIDAS Touch}}},
  author = {Lall, Ranjit and Robinson, Thomas},
  date = {2021-02-26},
  journaltitle = {Political Analysis},
  pages = {1--18},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2020.49},
  abstract = {Principled methods for analyzing missing values, based chiefly on multiple imputation, have become increasingly popular yet can struggle to handle the kinds of large and complex data that are also becoming common. We propose an accurate, fast, and scalable approach to multiple imputation, which we call MIDAS (Multiple Imputation with Denoising Autoencoders). MIDAS employs a class of unsupervised neural networks known as denoising autoencoders, which are designed to reduce dimensionality by corrupting and attempting to reconstruct a subset of data. We repurpose denoising autoencoders for multiple imputation by treating missing values as an additional portion of corrupted data and drawing imputations from a model trained to minimize the reconstruction error on the originally observed portion. Systematic tests on simulated as well as real social science data, together with an applied example involving a large-scale electoral survey, illustrate MIDAS’s accuracy and efficiency across a range of settings. We provide open-source software for implementing MIDAS.},
  keywords = {done,imputation methods,machine learning,missing data,multiple imputation},
  langid = {english}
}

@software{lall2021a,
  title = {{{MIDASverse}}/{{rMIDAS}}},
  author = {Lall, Ranjit and Robinson, Thomas},
  date = {2021-03-03T14:27:18Z},
  origdate = {2020-09-03T13:43:45Z},
  abstract = {R package for missing-data imputation with deep learning},
  keywords = {deep-learning,done,imputation-methods,neural-network,r,reticulate,tensorflow},
  organization = {{MIDASverse}}
}

@book{lawler2006,
  title = {Introduction to {{Stochastic Processes}}},
  author = {Lawler, Gregory},
  date = {2006},
  edition = {2},
  publisher = {{Chapman and Hall/CRC}},
  series = {Chapman \& {{Hall}}/{{CRC Probability Series}}}
}

@article{lawless2012,
  title = {Monitoring {{Warranty Claims With Cusums}}},
  author = {Lawless, J. F. and Crowder, M. J. and Lee, K.-A.},
  date = {2012},
  journaltitle = {Technometrics},
  volume = {54},
  pages = {269--278},
  publisher = {{Taylor \& Francis, Ltd.}},
  issn = {0040-1706},
  abstract = {Manufacturers monitor products in field use with respect to the quality and reliability of their performance, and warranty claims data are one valuable source of information. Updated warranty claims data are examined periodically by manufacturers, and formal monitoring procedures are a useful adjunct that can help identify emerging problems. This article presents cusum procedures for monitoring claims, designed to allow changes in claim rates to be detected in as timely a manner as possible. The determination of plans with given signal probabilities under nominal and increased rates is based on Markov chain calculations which are easy to implement. The procedures are motivated by and illustrated on warranty claims for North American automobiles. This article has online supplementary material.},
  eprint = {41714895},
  eprinttype = {jstor},
  keywords = {done},
  number = {3}
}

@book{lax2002,
  title = {Functional Analysis},
  author = {Lax, Peter D.},
  date = {2002-03-21},
  edition = {1. edizione},
  publisher = {{Wiley-Interscience}},
  location = {{New York}},
  abstract = {Includes sections on the spectral resolution and spectral representation of self adjoint operators, invariant subspaces, strongly continuous one-parameter semigroups, the index of operators, the trace formula of Lidskii, the Fredholm determinant, and more. * Assumes prior knowledge of Naive set theory, linear algebra, point set topology, basic complex variable, and real variables. * Includes an appendix on the Riesz representation theorem.},
  isbn = {978-0-471-55604-6},
  langid = {Inglese},
  pagetotal = {580}
}

@book{lazar2010,
  title = {The Statistical Analysis of Functional MRI Data},
  author = {Lazar, Nicole A.},
  date = {2010-11-23},
  publisher = {{Springer}},
  abstract = {This book is a primer for readers interested in learning more about this fascinating subject and the many statistical challenges inherent in functional neuroimaging data. It presents the basics of technique and surveys popular statistical approaches.},
  isbn = {978-1-4419-2679-1},
  langid = {Inglese},
  pagetotal = {316}
}

@incollection{lecun2012,
  title = {Efficient {{BackProp}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author = {LeCun, Yann A. and Bottou, Léon and Orr, Genevieve B. and Müller, Klaus-Robert},
  editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  date = {2012},
  pages = {9--48},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_3},
  abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
  isbn = {978-3-642-35289-8},
  keywords = {Conjugate Gradient,done,Gradient Descent,Handwritten Digit,Neural Information Processing System,Newton Algorithm},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{lefkimmiatis2009,
  title = {Bayesian Inference on Multiscale Models for {{Poisson}} Intensity Estimation: Applications to Photon-Limited Image Denoising},
  shorttitle = {Bayesian Inference on Multiscale Models for Poisson Intensity Estimation},
  author = {Lefkimmiatis, S. and Maragos, P. and Papandreou, G.},
  date = {2009},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {18},
  pages = {1724--1741},
  issn = {1057-7149},
  doi = {10.1109/TIP.2009.2022008},
  abstract = {We present an improved statistical model for analyzing Poisson processes, with applications to photon-limited imaging. We build on previous work, adopting a multiscale representation of the Poisson process in which the ratios of the underlying Poisson intensities (rates) in adjacent scales are modeled as mixtures of conjugate parametric distributions. Our main contributions include: 1) a rigorous and robust regularized expectation-maximization (EM) algorithm for maximum-likelihood estimation of the rate-ratio density parameters directly from the noisy observed Poisson data (counts); 2) extension of the method to work under a multiscale hidden Markov tree model (HMT) which couples the mixture label assignments in consecutive scales, thus modeling interscale coefficient dependencies in the vicinity of image edges; 3) exploration of a 2-D recursive quad-tree image representation, involving Dirichlet-mixture rate-ratio densities, instead of the conventional separable binary-tree image representation involving beta-mixture rate-ratio densities; and 4) a novel multiscale image representation, which we term Poisson-Haar decomposition, that better models the image edge structure, thus yielding improved performance. Experimental results on standard images with artificially simulated Poisson noise and on real photon-limited images demonstrate the effectiveness of the proposed techniques.},
  keywords = {2D recursive quad-tree image,Additive noise,Algorithms,artificially simulated Poisson noise,Bayes methods,Bayes Theorem,Bayesian inference,Bayesian methods,conjugate parametric distribution,Degradation,Dirichlet-mixture rate-ratio densities,done,expectation-maximisation algorithm,expectation-maximization (EM) algorithm,hidden Markov models,Hidden Markov models,hidden Markov tree (HMT),Image analysis,image denoising,Image denoising,image edge structure,image edges,Image Processing; Computer-Assisted,image representation,Image representation,interscale coefficient,Markov Chains,Maximum likelihood estimation,maximum-likelihood estimation,mixture label assignment,Models; Statistical,multiscale hidden Markov tree model,multiscale image representation,Noise level,Optics and Photonics,photon-limited image denoising,photon-limited imaging,Poisson Distribution,Poisson intensity estimation,Poisson processes,Poisson-Haar decomposition,quadtrees,rate-ratio density parameters,recursive estimation,regularized expectation-maximization algorithm,Robustness,statistical model,stochastic processes},
  number = {8}
}

@article{lehmann1990,
  title = {Model {{Specification}}: {{The Views}} of {{Fisher}} and {{Neyman}}, and {{Later Developments}}},
  shorttitle = {Model {{Specification}}},
  author = {Lehmann, E. L.},
  date = {1990},
  journaltitle = {Statistical Science},
  volume = {5},
  pages = {160--168},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  abstract = {Since Fisher's formulation in 1922 of a framework for theoretical statistics, statistical theory has been concerned primarily with the derivation and properties of suitable statistical procedures on the basis of an assumed statistical model (including sensitivity to deviations from this model). Until relatively recently, the theory has paid little attention to the question of how such a model should be chosen. In the present paper, we consider first what Fisher and Neyman had to say about this problem and in Section 2 survey some contributions statistical theory has made to it. In Section 3 we study a distinction between two types of models (empirical and explanatory) which has been discussed by Neyman, Box, and others. A concluding section considers some lines of further work.},
  eprint = {2245675},
  eprinttype = {jstor},
  keywords = {done},
  number = {2}
}

@book{lehmann2003,
  title = {Theory of Point Estimation},
  author = {Lehmann, E. L. and Casella, George},
  date = {2003-09-09},
  publisher = {{Springer Verlag}},
  location = {{New York}},
  abstract = {This second, much enlarged edition by Lehmann and Casella of Lehmann's classic text on point estimation maintains the outlook and general style of the first edition. All of the topics are updated, while an entirely new chapter on Bayesian and hierarchical Bayesian approaches is provided, and there is much new material on simultaneous estimation. Each chapter concludes with a Notes section which contains suggestions for further study. This is a companion volume to the second edition of Lehmann's "Testing Statistical Hypotheses".},
  isbn = {978-0-387-98502-2},
  langid = {Inglese},
  pagetotal = {589}
}

@book{lehmann2005,
  title = {Testing {{Statistical Hypotheses}}},
  author = {Lehmann, Erich L. and Romano, Joseph P.},
  date = {2005},
  edition = {3},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  doi = {10.1007/0-387-27605-X},
  abstract = {The third edition of Testing Statistical Hypotheses updates and expands upon the classic graduate text, emphasizing optimality theory for hypothesis testing and confidence sets. The principal additions include a rigorous treatment of large sample optimality, together with the requisite tools. In addition, an introduction to the theory of resampling methods such as the bootstrap is developed. The sections on multiple testing and goodness of fit testing are expanded. The text is suitable for Ph.D. students in statistics and includes over 300 new problems out of a total of more than 760. E.L. Lehmann is Professor of Statistics Emeritus at the University of California, Berkeley. He is a member of the National Academy of Sciences and the American Academy of Arts and Sciences, and the recipient of honorary degrees from the University of Leiden, The Netherlands and the University of Chicago. He is the author of Elements of Large-Sample Theory and (with George Casella) he is also the author of Theory of Point Estimation, Second Edition. Joseph P. Romano is Professor of Statistics at Stanford University. He is a recipient of a Presidential Young Investigator Award and a Fellow of the Institute of Mathematical Statistics. He has coauthored two other books, Subsampling with Dimitris Politis and Michael Wolf, and Counterexamples in Probability and Statistics with Andrew Siegel.},
  isbn = {978-0-387-98864-1},
  langid = {english},
  series = {Springer {{Texts}} in {{Statistics}}}
}

@book{leskovic2020,
  title = {Mining of Massive Datasets},
  author = {Leskovic, Jure and Anand, Rajaraman and Ullman, Jeffrey David},
  date = {2020},
  publisher = {{John Wiley \& Sons}},
  location = {{New Delhi}},
  isbn = {978-1-108-47634-8},
  langid = {Inglese}
}

@incollection{li2016,
  title = {Model {{Selection}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Li, Ang and Pericchi, Luis},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2016-08-05},
  pages = {1--10},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat06424.pub2},
  isbn = {978-1-118-44511-2},
  keywords = {done},
  langid = {english}
}

@article{li2017,
  title = {Simple, Scalable and Accurate Posterior Interval Estimation},
  author = {Li, Cheng and Srivastava, Sanvesh and Dunson, David B.},
  date = {2017-09-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {104},
  pages = {665--680},
  issn = {0006-3444},
  doi = {10.1093/biomet/asx033},
  abstract = {Standard posterior sampling algorithms, such as Markov chain Monte Carlo procedures, face major challenges in scaling up to massive datasets. We propose a simple and general posterior interval estimation algorithm to rapidly and accurately estimate quantiles of the posterior distributions for one-dimensional functionals. Our algorithm runs Markov chain Monte Carlo in parallel for subsets of the data, and then averages quantiles estimated from each subset. We provide strong theoretical guarantees and show that the credible intervals from our algorithm asymptotically approximate those from the full posterior in the leading parametric order. Our algorithm has a better balance of accuracy and efficiency than its competitors across a variety of simulations and a real-data example.},
  keywords = {done},
  number = {3}
}

@article{li2019,
  ids = {li2019a},
  title = {Spatial {{Bayesian}} Modeling of {{GLCM}} with Application to Malignant Lesion Characterization},
  author = {Li, Xiao and Guindani, Michele and Ng, Chaan S. and Hobbs, Brian P.},
  date = {2019-01-25},
  journaltitle = {Journal of Applied Statistics},
  shortjournal = {Journal of Applied Statistics},
  volume = {46},
  pages = {230--246},
  publisher = {{Taylor \& Francis}},
  issn = {0266-4763, 1360-0532},
  doi = {10.1080/02664763.2018.1473348},
  abstract = {The emerging field of cancer radiomics endeavors to characterize intrinsic patterns of tumor phenotypes and surrogate markers of response by transforming medical images into objects that yield quantifiable summary statistics to which regression and machine learning algorithms may be applied for statistical interrogation. Recent literature has identified clinicopathological association based on textural features deriving from gray-level co-occurrence matrices (GLCM) which facilitate evaluations of gray-level spatial dependence within a delineated region of interest. GLCM-derived features, however, tend to contribute highly redundant information. Moreover, when reporting selected feature sets, investigators often fail to adjust for multiplicities and commonly fail to convey the predictive power of their findings. This article presents a Bayesian probabilistic modeling framework for the GLCM as a multivariate object as well as describes its application within a cancer detection context based on computed tomography. The methodology, which circumvents processing steps and avoids evaluations of reductive and highly correlated feature sets, uses latent Gaussian Markov random field structure to characterize spatial dependencies among GLCM cells and facilitates classification via predictive probability. Correctly predicting the underlying pathology of 81\% of the adrenal lesions in our case study, the proposed method outperformed current practices which achieved a maximum accuracy of only 59\%. Simulations and theory are presented to further elucidate this comparison as well as ascertain the utility of applying multivariate Gaussian spatial processes to GLCM objects.},
  keywords = {Bayesian prediction,cancer detection,done,gray-level co-occurrence matrix,Markov random field,radiomics,texture analysis},
  langid = {english},
  number = {2}
}

@online{li2020,
  title = {A {{Bayesian Nonparametric}} Model for Textural Pattern Heterogeneity},
  author = {Li, Xiao and Guindani, Michele and Ng, Chaan S. and Hobbs, Brian P.},
  date = {2020-11-11},
  abstract = {Cancer radiomics is an emerging discipline promising to elucidate lesion phenotypes and tumor heterogeneity through patterns of enhancement, texture, morphology, and shape. The prevailing technique for image texture analysis relies on the construction and synthesis of Gray-Level Co-occurrence Matrices (GLCM). Practice currently reduces the structured count data of a GLCM to reductive and redundant summary statistics for which analysis requires variable selection and multiple comparisons for each application, thus limiting reproducibility. In this article, we develop a Bayesian multivariate probabilistic framework for the analysis and unsupervised clustering of a sample of GLCM objects. By appropriately accounting for skewness and zero-inflation of the observed counts and simultaneously adjusting for existing spatial autocorrelation at nearby cells, the methodology facilitates estimation of texture pattern distributions within the GLCM lattice itself. The techniques are applied to cluster images of adrenal lesions obtained from CT scans with and without administration of contrast. We further assess whether the resultant subtypes are clinically oriented by investigating their correspondence with pathological diagnoses. Additionally, we compare performance to a class of machine-learning approaches currently used in cancer radiomics with simulation studies.},
  archiveprefix = {arXiv},
  eprint = {2011.05548},
  eprinttype = {arxiv},
  keywords = {done,Statistics - Applications},
  primaryclass = {stat}
}

@article{lin,
  title = {Data-{{Intensive Text Processing}} with {{MapReduce}}},
  author = {Lin, Jimmy and Dyer, Chris},
  pages = {175},
  keywords = {done},
  langid = {english}
}

@article{lindquist2008,
  title = {The {{Statistical Analysis}} of {{fMRI Data}}},
  author = {Lindquist, Martin A.},
  date = {2008-11},
  journaltitle = {Statistical Science},
  volume = {23},
  pages = {439--464},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/09-STS282},
  abstract = {In recent years there has been explosive growth in the number of neuroimaging studies performed using functional Magnetic Resonance Imaging (fMRI). The field that has grown around the acquisition and analysis of fMRI data is intrinsically interdisciplinary in nature and involves contributions from researchers in neuroscience, psychology, physics and statistics, among others. A standard fMRI study gives rise to massive amounts of noisy data with a complicated spatio-temporal correlation structure. Statistics plays a crucial role in understanding the nature of the data and obtaining relevant results that can be used and interpreted by neuroscientists. In this paper we discuss the analysis of fMRI data, from the initial acquisition of the raw data to its use in locating brain activity, making inference about brain connectivity and predictions about psychological or disease states. Along the way, we illustrate interesting and important issues where statistics already plays a crucial role. We also seek to illustrate areas where statistics has perhaps been underutilized and will have an increased role in the future.},
  keywords = {brain imaging,challenges,fMRI,statistical analysis,todo},
  number = {4}
}

@unpublished{liseo2010,
  title = {Introduzione Alla Statistica Bayesiana},
  author = {Liseo, Brunero},
  date = {2010}
}

@book{little2019,
  title = {Statistical {{Analysis}} with {{Missing Data}}},
  author = {Little, Roderick J. A. and Rubin, Donald B.},
  date = {2019-04-23},
  edition = {3rd edition},
  publisher = {{Wiley}},
  location = {{Hoboken, NJ}},
  abstract = {An up-to-date, comprehensive treatment of a classic text on missing data in statisticsThe topic of missing data has gained considerable attention in recent decades. This new edition by two acknowledged experts on the subject offers an up-to-date account of practical methodology for handling missing data problems. Blending theory and application, authors Roderick Little and Donald Rubin review historical approaches to the subject and describe simple methods for multivariate analysis with missing values. They then provide a coherent theory for analysis of problems based on likelihoods derived from statistical models for the data and the missing data mechanism, and then they apply the theory to a wide range of important missing data problems.Statistical Analysis with Missing Data, Third Edition starts by introducing readers to the subject and approaches toward solving it. It looks at the patterns and mechanisms that create the missing data, as well as a taxonomy of missing data. It then goes on to examine missing data in experiments, before discussing complete-case and available-case analysis, including weighting methods. The new edition expands its coverage to include recent work on topics such as nonresponse in sample surveys, causal inference, diagnostic methods, and sensitivity analysis, among a host of other topics.  An updated “classic” written by renowned authorities on the subject Features over 150 exercises (including many new ones) Covers recent work on important methods like multiple imputation, robust alternatives to weighting, and Bayesian methods Revises previous topics based on past student feedback and class experience Contains an updated and expanded bibliography  The authors were awarded The Karl Pearson Prize in 2017 by the International Statistical Institute, for a research contribution that has had profound influence on statistical theory, methodology or applications. Their work "has been no less than defining and transforming." (ISI)Statistical Analysis with Missing Data, Third Edition is an ideal textbook for upper undergraduate and/or beginning graduate level students of the subject. It is also an excellent source of information for applied statisticians and practitioners in government and industry.},
  isbn = {978-0-470-52679-8},
  keywords = {todo},
  langid = {english},
  pagetotal = {464}
}

@article{lowry1992,
  title = {A {{Multivariate Exponentially Weighted Moving Average Control Chart}}},
  author = {Lowry, Cynthia A. and Woodall, William H. and Champ, Charles W. and Rigdon, Steven E.},
  date = {1992-02-01},
  journaltitle = {Technometrics},
  volume = {34},
  pages = {46--53},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1992.10485232},
  abstract = {A multivariate extension of the exponentially weighted moving average (EWMA) control chart is presented, and guidelines given for designing this easy-to-implement multivariate procedure. A comparison shows that the average run length (ARL) performance of this chart is similar to that of multivariate cumulative sum (CUSUM) control charts in detecting a shift in the mean vector of a multivariate normal distribution. As with the Hotelling's χ2 and multivariate CUSUM charts, the ARL performance of the multivariate EWMA chart depends on the underlying mean vector and covariance matrix only through the value of the noncentrality parameter. Worst-case scenarios show that Hotelling's χ2 charts should always be used in conjunction with multivariate CUSUM and EWMA charts to avoid potential inertia problems. Examples are given to illustrate the use of the proposed procedure.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1992.10485232},
  keywords = {Average run length,done,Hotelling's T 2 chart,Multivariate CUSUM,Statistical process control},
  number = {1}
}

@book{luck2014,
  title = {An Introduction to the Event-Related Potential Technique},
  author = {Luck, Steven J.},
  date = {2014-05-30},
  edition = {Seconda edizione},
  publisher = {{A Bradford Book}},
  abstract = {An essential guide to designing, conducting, and analyzing event-related potential (ERP) experiments, completely updated for this edition.The event-related potential (ERP) technique, in which neural responses to specific events are extracted from the EEG, provides a powerful noninvasive tool for exploring the human brain. This volume describes practical methods for ERP research along with the underlying theoretical rationale. It offers researchers and students an essential guide to designing, conducting, and analyzing ERP experiments. This second edition has been completely updated, with additional material, new chapters, and more accessible explanations. Freely available supplementary material, including several online-only chapters, offer expanded or advanced treatment of selected topics.The first half of the book presents essential background information, describing the origins of ERPs, the nature of ERP components, and the design of ERP experiments. The second half of the book offers a detailed treatment of the main steps involved in conducting ERP experiments, covering such topics as recording the EEG, filtering the EEG and ERP waveforms, and quantifying amplitudes and latencies. Throughout, the emphasis is on rigorous experimental design and relatively simple analyses. New material in the second edition includes entire chapters devoted to components, artifacts, measuring amplitudes and latencies, and statistical analysis; updated coverage of recording technologies; concrete examples of experimental design; and many more figures. Online chapters cover such topics as overlap, localization, writing and reviewing ERP papers, and setting up and running an ERP lab.},
  isbn = {978-0-262-52585-5},
  langid = {Inglese},
  pagetotal = {416}
}

@article{ludwig2015,
  title = {{{MapReduce}}-Based Fuzzy c-Means Clustering Algorithm: Implementation and Scalability},
  shorttitle = {{{MapReduce}}-Based Fuzzy c-Means Clustering Algorithm},
  author = {Ludwig, Simone},
  date = {2015},
  journaltitle = {International Journal of Machine Learning and Cybernetics},
  shortjournal = {International Journal of Machine Learning and Cybernetics},
  volume = {6},
  abstract = {The management and analysis of big data has been identified as one of the most important emerging needs in recent years. This is because of the sheer volume and increasing complexity of data being created or collected. Current clustering algorithms can not handle big data, and therefore, scalable solutions are necessary. Since fuzzy clustering algorithms have shown to outperform hard clustering approaches in terms of accuracy, this paper investigates the parallelization and scalability of a common and effective fuzzy clustering algorithm named fuzzy c-means (FCM) algorithm. The algorithm is parallelized using the MapReduce paradigm outlining how the Map and Reduce primitives are implemented. A validity analysis is conducted in order to show that the implementation works correctly achieving competitive purity results compared to state-of-the art clustering algorithms. Furthermore, a scalability analysis is conducted to demonstrate the performance of the parallel FCM implementation with increasing number of computing nodes used.},
  keywords = {done}
}

@incollection{lukas2014,
  title = {Regularization {{Methods}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Lukas, Mark A.},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2014-09-29},
  pages = {stat07415},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat07415},
  isbn = {978-1-118-44511-2},
  keywords = {todo},
  langid = {english}
}

@book{lutkepohl2005,
  title = {New {{Introduction}} to {{Multiple Time Series Analysis}}},
  author = {Lütkepohl, Helmut},
  date = {2005},
  publisher = {{Springer-Verlag}},
  location = {{Berlin Heidelberg}},
  doi = {10.1007/978-3-540-27752-1},
  abstract = {This reference work and graduate level textbook considers a wide range of models and methods for analyzing and forecasting multiple time series. The models covered include vector autoregressive, cointegrated,vector autoregressive moving average, multivariate ARCH and periodic processes as well as dynamic simultaneous equations and state space models. Least squares, maximum likelihood and Bayesian methods are considered for estimating these models. Different procedures for model selection and model specification are treated and a wide range of tests and criteria for model checking are introduced. Causality analysis, impulse response analysis and innovation accounting are presented as tools for structural analysis. The book is accessible to graduate students in business and economics. In addition, multiple time series courses in other fields such as statistics and engineering may be based on it. Applied researchers involved in analyzing multiple time series may benefit from the book as it provides the background and tools for their tasks. It bridges the gap to the difficult technical literature on the topic.},
  isbn = {978-3-540-40172-8},
  langid = {english}
}

@article{maaten2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  date = {2008},
  journaltitle = {Journal of Machine Learning Research},
  volume = {9},
  pages = {2579--2605},
  issn = {ISSN 1533-7928},
  issue = {Nov},
  keywords = {done}
}

@book{marin2007a,
  title = {Bayesian {{Core}}: {{A Practical Approach}} to {{Computational Bayesian Statistics}}},
  shorttitle = {Bayesian {{Core}}},
  author = {Marin, Jean-Michel and Robert, Christian},
  date = {2007},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  doi = {10.1007/978-0-387-38983-7},
  abstract = {This Bayesian modeling book is intended for practitioners and applied statisticians looking for a self-contained entry to computational Bayesian statistics. Focusing on standard statistical models and backed up by discussed real datasets available from the book website, it provides an operational methodology for conducting Bayesian inference, rather than focusing on its theoretical justifications. Special attention is paid to the derivation of prior distributions in each case and specific reference solutions are given for each of the models. Similarly, computational details are worked out to lead the reader towards an effective programming of the methods given in the book. While R programs are provided on the book website and R hints are given in the computational sections of the book, The Bayesian Core requires no knowledge of the R language and it can be read and used with any other programming language. The Bayesian Core can be used as a textbook at both undergraduate and graduate levels, as exemplified by courses given at Université Paris Dauphine (France), University of Canterbury (New Zealand), and University of British Columbia (Canada). It serves as a unique textbook for a service course for scientists aiming at analyzing data the Bayesian way as well as an introductory course on Bayesian statistics. The prerequisites for the book are a basic knowledge of probability theory and of statistics. Methodological and data-based exercises are included within the main text and students are expected to solve them as they read the book. Those exercises can obviously serve as assignments, as was done in the above courses. Datasets, R codes and course slides all are available on the book website. Jean-Michel Marin is currently senior researcher at INRIA, the French Computer Science research institute, and located at Université Paris-Sud, Orsay. He has previously been Assistant Professor at Université Paris Dauphine for four years. He has written numerous papers on Bayesian methodology and computing, and is currently a member of the council of the French Statistical Society. Christian Robert is Professor of Statistics at Université Paris Dauphine and Head of the Statistics Research Laboratory at CREST-INSEE, Paris. He has written over a hundred papers on Bayesian Statistics and computational methods and is the author or co-author of seven books on those topics, including The Bayesian Choice (Springer, 2001), winner of the ISBA DeGroot Prize in 2004. He is a Fellow and member of the council of the Institute of Mathematical Statistics, and a Fellow and member of the research committee of the Royal Statistical Society. He is currently co-editor of the Journal of the Royal Statistical Society, Series B, after taking part in the editorial boards of the Journal of the American Statistical Society, the Annals of Statistics, Statistical Science, and Bayesian Analysis. He is also the winner of the Young Statistician prize of the Paris Statistical Society in 1996 and a recipient of an Erskine Fellowship from the University of Canterbury (NZ) in 2006.},
  isbn = {978-0-387-38983-7},
  langid = {english},
  series = {Springer {{Texts}} in {{Statistics}}}
}

@article{mechelli2005,
  title = {Voxel-{{Based Morphometry}} of the {{Human Brain}}: {{Methods}} and {{Applications}}},
  shorttitle = {Voxel-{{Based Morphometry}} of the {{Human Brain}}},
  author = {Mechelli, Andrea and Price, Cathy J. and Ashburner, John and Friston, Karl J.},
  date = {2005-05-31},
  journaltitle = {Current Medical Imaging},
  shortjournal = {Current Medical Imaging},
  volume = {1},
  pages = {105--113},
  abstract = {In recent years, a whole-brain unbiased objective technique, known as voxel-based morphometry (VBM), has been developed to characterise brain differences i...},
  keywords = {todo},
  langid = {english},
  number = {2}
}

@book{medema2011,
  title = {The {{Hesitant Hand}}: {{Taming Self}}-{{Interest}} in the {{History}} of {{Economic Ideas}}},
  shorttitle = {The {{Hesitant Hand}}},
  author = {Medema, Steven G.},
  date = {2011-03-06},
  edition = {Reprint edition},
  publisher = {{Princeton University Press}},
  location = {{Princeton, New Jersey Oxford}},
  abstract = {Adam Smith turned economic theory on its head in 1776 when he declared that the pursuit of self-interest mediated by the market itself--not by government--led, via an invisible hand, to the greatest possible welfare for society as a whole. The Hesitant Hand examines how subsequent economic thinkers have challenged or reaffirmed Smith's doctrine, some contending that society needs government to intervene on its behalf when the marketplace falters, others arguing that government interference ultimately benefits neither the market nor society. Steven Medema explores what has been perhaps the central controversy in modern economics from Smith to today. He traces the theory of market failure from the 1840s through the 1950s and subsequent attacks on this view by the Chicago and Virginia schools. Medema follows the debate from John Stuart Mill through the Cambridge welfare tradition of Henry Sidgwick, Alfred Marshall, and A. C. Pigou, and looks at Ronald Coase's challenge to the Cambridge approach and the rise of critiques affirming Smith's doctrine anew. He shows how, following the marginal revolution, neoclassical economists, like the preclassical theorists before Smith, believed government can mitigate the adverse consequences of self-interested behavior, yet how the backlash against this view, led by the Chicago and Virginia schools, demonstrated that self-interest can also impact government, leaving society with a choice among imperfect alternatives. The Hesitant Hand demonstrates how government's economic role continues to be bound up in questions about the effects of self-interest on the greater good.},
  isbn = {978-0-691-15000-0},
  langid = {english},
  pagetotal = {248}
}

@article{menardi2014,
  title = {Training and Assessing Classification Rules with Imbalanced Data},
  author = {Menardi, Giovanna and Torelli, Nicola},
  date = {2014-01-01},
  journaltitle = {Data Mining and Knowledge Discovery},
  shortjournal = {Data Min Knowl Disc},
  volume = {28},
  pages = {92--122},
  issn = {1573-756X},
  doi = {10.1007/s10618-012-0295-5},
  abstract = {The problem of modeling binary responses by using cross-sectional data has been addressed with a number of satisfying solutions that draw on both parametric and nonparametric methods. However, there exist many real situations where one of the two responses (usually the most interesting for the analysis) is rare. It has been largely reported that this class imbalance heavily compromises the process of learning, because the model tends to focus on the prevalent class and to ignore the rare events. However, not only the estimation of the classification model is affected by a skewed distribution of the classes, but also the evaluation of its accuracy is jeopardized, because the scarcity of data leads to poor estimates of the model’s accuracy. In this work, the effects of class imbalance on model training and model assessing are discussed. Moreover, a unified and systematic framework for dealing with the problem of imbalanced classification is proposed, based on a smoothed bootstrap re-sampling technique. The proposed technique is founded on a sound theoretical basis and an extensive empirical study shows that it outperforms the main other remedies to face imbalanced learning problems.},
  keywords = {todo},
  langid = {english},
  number = {1}
}

@article{meng2018,
  title = {Statistical Paradises and Paradoxes in Big Data ({{I}}): {{Law}} of Large Populations, Big Data Paradox, and the 2016 {{US}} Presidential Election},
  shorttitle = {Statistical Paradises and Paradoxes in Big Data ({{I}})},
  author = {Meng, Xiao-Li},
  date = {2018-06},
  journaltitle = {The Annals of Applied Statistics},
  shortjournal = {Ann. Appl. Stat.},
  volume = {12},
  pages = {685--726},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/18-AOAS1161SF},
  abstract = {Statisticians are increasingly posed with thought-provoking and even paradoxical questions, challenging our qualifications for entering the statistical paradises created by Big Data. By developing measures for data quality, this article suggests a framework to address such a question: “Which one should I trust more: a 1\% survey with 60\% response rate or a self-reported administrative dataset covering 80\% of the population?” A 5-element Euler-formula-like identity shows that for any dataset of size nnn, probabilistic or not, the difference between the sample average X⎯⎯⎯⎯nX¯n\textbackslash overline\{X\}\_\{n\} and the population average X⎯⎯⎯⎯NX¯N\textbackslash overline\{X\}\_\{N\} is the product of three terms: (1) a data quality measure, ρR,XρR,X\textbackslash rho\_\{\{R,X\}\}, the correlation between XjXjX\_\{j\} and the response/recording indicator RjRjR\_\{j\}; (2) a data quantity measure, (N−n)/n‾‾‾‾‾‾‾‾‾√(N−n)/n\textbackslash sqrt\{(N-n)/n\}, where NNN is the population size; and (3) a problem difficulty measure, σXσX\textbackslash sigma\_\{X\}, the standard deviation of XXX. This decomposition provides multiple insights: (I) Probabilistic sampling ensures high data quality by controlling ρR,XρR,X\textbackslash rho\_\{\{R,X\}\} at the level of N−1/2N−1/2N\^\{-1/2\}; (II) When we lose this control, the impact of NNN is no longer canceled by ρR,XρR,X\textbackslash rho\_\{\{R,X\}\}, leading to a Law of Large Populations (LLP), that is, our estimation error, relative to the benchmarking rate 1/n‾√1/n1/\textbackslash sqrt\{n\}, increases with N‾‾√N\textbackslash sqrt\{N\}; and (III) the “bigness” of such Big Data (for population inferences) should be measured by the relative size f=n/Nf=n/Nf=n/N, not the absolute size nnn; (IV) When combining data sources for population inferences, those relatively tiny but higher quality ones should be given far more weights than suggested by their sizes. Estimates obtained from the Cooperative Congressional Election Study (CCES) of the 2016 US presidential election suggest a ρR,X≈−0.005ρR,X≈−0.005\textbackslash rho\_\{\{R,X\}\}\textbackslash approx-0.005 for self-reporting to vote for Donald Trump. Because of LLP, this seemingly minuscule data defect correlation implies that the simple sample proportion of the self-reported voting preference for Trump from 1\%1\%1\textbackslash\% of the US eligible voters, that is, n≈2,300,000n≈2,300,000n\textbackslash approx2\textbackslash mbox\{,\}300\textbackslash mbox\{,\}000, has the same mean squared error as the corresponding sample proportion from a genuine simple random sample of size n≈400n≈400n\textbackslash approx400, a 99.98\%99.98\%99.98\textbackslash\% reduction of sample size (and hence our confidence). The CCES data demonstrate LLP vividly: on average, the larger the state’s voter populations, the further away the actual Trump vote shares from the usual 95\%95\%95\textbackslash\% confidence intervals based on the sample proportions. This should remind us that, without taking data quality into account, population inferences with Big Data are subject to a Big Data Paradox: the more the data, the surer we fool ourselves.},
  keywords = {Bias-variance tradeoff,data confidentiality and privacy,data defect correlation,data defect index (d.d.i.),data quality-quantity tradeoff,done,Euler identity,Monte Carlo and Quasi Monte Carlo (MCQMC),non-response bias},
  langid = {english},
  mrnumber = {MR3834282},
  number = {2},
  zmnumber = {06980472}
}

@book{meyers2005,
  title = {Effective C++: 55 Specific Ways to Improve Your Programs and Designs},
  shorttitle = {Effective C++},
  author = {Meyers, Scott},
  date = {2005-05-12},
  edition = {3 edizione},
  publisher = {{Addison-Wesley Professional}},
  location = {{Upper Saddle River, NJ}},
  abstract = {Presents a collection of tips for programmers on ways to improve programming skills.},
  isbn = {978-0-321-33487-9},
  langid = {Inglese},
  pagetotal = {297}
}

@book{mitchell,
  title = {IELTS Academic Writing Task 1: The Ultimate Guide with Practice to Get a Target Band Score of 8.0+ In 10 Minutes a Day},
  shorttitle = {IELTS Academic Writing Task 1},
  author = {Mitchell, Rachel},
  abstract = {Downloaded by over 15,000 people...\#1 Release 2018Hurry up and get YOUR copy today for 2.99 only❗~Regular price at 6.99❗IELTS Academic Writing Task 1: The Ultimate Guide with Practice to Get a Target Band Score of 8.0+ in 10 Minutes a Day!Are you finding task 1 writing difficult and struggling with it?Are you looking for a book that helps you achieve an 8.0+ in an effortless way?Would you like to learn all strategies and structures in Task 1 writing in just 10 minutes a day?If your answer is~“yes”~to these above questions, then this book is perfect for you.This book is~well designed~and written~by~an experienced native teacher~from the USA who has been teaching IELTS for over 10 years. She really is the expert in training IELTS for students at each level. In this book, she will provide you all~proven formulas, tips, strategies, explanations, structures, task 1 language, vocabulary and model essays~to help you easily achieve an~8.0+~in the IELTS Writing section (Academic), even if your English is not excellent. This book will also walk you through step-by-step on how to develop your~well-organised answers~for the Task 1 Writing; clearly explains the different types of questions that are asked for Task 1; provide you step-by-step instructions on how to write each type of report excellently.As the author of this book, I believe that this book will be~an indispensable reference and trusted guide~for you who may want to~maximize your band score~in IELTS academic task 1 writing. Once you read this book, I guarantee you that you will have learned an extraordinarily wide range of useful, and practical IELTS WRITNG TASK 1 strategies and formulas that will help you become a successful IELTS taker as well as you will even become a successful English user in work and in life within a short period of time only.Don’t delay any more seconds, scroll back up,~DOWNLOAD~your copy~TODAY~and start learning to get an 8.0+ in IELTS Academic Task 1 Writing tomorrow!Tags:ielts writing task 1 and 2, academic ielts task 1 writing, ielts academic writing, ielts essay writing, ielts writing books, ielts essay, ielts academic books, ielts guide, ielts grammar, ielts vocabulary book, ielts writing skills, ielts writing practice, ielts academic writing book, ielts foundation, ielts prep book, ielts practice exams, ielts success, ielts training, ielts academic module, ielts academic 2017, ielts preparation books, ielts ebook, ielts academic vocabulary, ielts preparation 2017, ielts vocabulary, ielts academic, ielts preparation, ielts writing, ielts practice tests, ielts writing task 1},
  langid = {Inglese},
  pagetotal = {92}
}

@article{mockus1975,
  title = {On the {{Bayes Methods}} for {{Seeking}} the {{Extremal Point}}},
  author = {Mockus, J.},
  date = {1975-08},
  journaltitle = {IFAC Proceedings Volumes},
  shortjournal = {IFAC Proceedings Volumes},
  volume = {8},
  pages = {428--431},
  issn = {14746670},
  doi = {10.1016/S1474-6670(17)67769-3},
  keywords = {todo},
  langid = {english},
  number = {1}
}

@book{mockus1989,
  title = {Bayesian {{Approach}} to {{Global Optimization}}: {{Theory}} and {{Applications}}},
  shorttitle = {Bayesian {{Approach}} to {{Global Optimization}}},
  author = {Mockus, Jonas},
  date = {1989},
  publisher = {{Springer Netherlands}},
  doi = {10.1007/978-94-009-0909-0},
  abstract = {`Bayesian Approach to Global Optimization is an excellent reference book in the field. As a text it is probably most appropriate in a mathematics or computer science department or at an advanced graduate level in engineering departments ...' A. Belegundu, Applied Mechanics Review, Vol. 43, no. 4, April 1990},
  isbn = {978-94-010-6898-7},
  langid = {english},
  series = {Mathematics and Its {{Applications}}}
}

@book{mockus2000,
  title = {A {{Set}} of {{Examples}} of {{Global}} and {{Discrete Optimization}}: {{Applications}} of {{Bayesian Heuristic Approach}}},
  shorttitle = {A {{Set}} of {{Examples}} of {{Global}} and {{Discrete Optimization}}},
  author = {Mockus, Jonas},
  date = {2000},
  publisher = {{Springer US}},
  doi = {10.1007/978-1-4615-4671-9},
  abstract = {This book shows how the Bayesian Approach (BA) improves well­ known heuristics by randomizing and optimizing their parameters. That is the Bayesian Heuristic Approach (BHA). The ten in-depth examples are designed to teach Operations Research using Internet. Each example is a simple representation of some impor­ tant family of real-life problems. The accompanying software can be run by remote Internet users. The supporting web-sites include software for Java, C++, and other lan­ guages. A theoretical setting is described in which one can discuss a Bayesian adaptive choice of heuristics for discrete and global optimization prob­ lems. The techniques are evaluated in the spirit of the average rather than the worst case analysis. In this context, "heuristics" are understood to be an expert opinion defining how to solve a family of problems of dis­ crete or global optimization. The term "Bayesian Heuristic Approach" means that one defines a set of heuristics and fixes some prior distribu­ tion on the results obtained. By applying BHA one is looking for the heuristic that reduces the average deviation from the global optimum. The theoretical discussions serve as an introduction to examples that are the main part of the book. All the examples are interconnected. Dif­ ferent examples illustrate different points of the general subject. How­ ever, one can consider each example separately, too.},
  isbn = {978-0-7923-6359-0},
  langid = {english},
  series = {Applied {{Optimization}}}
}

@article{mokbel2003,
  title = {Analysis of {{Multi}}-{{Dimensional Space}}-{{Filling Curves}}},
  author = {Mokbel, Mohamed F. and Aref, Walid G. and Kamel, Ibrahim},
  date = {2003},
  journaltitle = {GeoInformatica},
  volume = {7},
  pages = {179--209},
  issn = {13846175},
  doi = {10.1023/A:1025196714293},
  keywords = {todo},
  number = {3}
}

@incollection{montgomery1972,
  title = {Some {{Techniques}} for {{Multivariate Quality Control Applications}}},
  author = {Montgomery, D.C. and Wadsworth, H.M.},
  date = {1972},
  location = {{Washington, DC}},
  series = {Transactions of the {{ASQC}}}
}

@online{mukhopadhyay2019,
  title = {Nonparametric {{Universal Copula Modeling}}},
  author = {Mukhopadhyay, Subhadeep and Parzen, Emanuel},
  date = {2019-12-11},
  abstract = {To handle the ubiquitous problem of "dependence learning," copulas are quickly becoming a pervasive tool across a wide range of data-driven disciplines encompassing neuroscience, finance, econometrics, genomics, social science, machine learning, healthcare and many more. Copula (or connection) functions were invented in 1959 by Abe Sklar in response to a query of Maurice Frechet. After 60 years, where do we stand now? This article provides a history of the key developments and offers a unified perspective.},
  archiveprefix = {arXiv},
  eprint = {1912.05503},
  eprinttype = {arxiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology,todo},
  primaryclass = {stat}
}

@book{murphy2012,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  date = {2012-09-07},
  edition = {1 edizione},
  publisher = {{The MIT Press}},
  abstract = {A comprehensive introduction to machine learning that uses probabilistic models and inference as a unifying approach.Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach.The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package—PMTK (probabilistic modeling toolkit)—that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
  langid = {Inglese},
  pagetotal = {1104}
}

@book{murray1993,
  title = {Differential {{Geometry}} and {{Statistics}}},
  author = {Murray, M.K. and Rice, J.W.},
  date = {1993-01-01},
  edition = {1 edition},
  publisher = {{Chapman and Hall/CRC}},
  location = {{London ; New York}},
  abstract = {This book explains why geometry should enter into parametric statistics and how the theory of asymptotic expansions involves a form of higher-order differential geometry. It gives some new explanations of geometric ideas from a first principles point of view as far as geometry is concerned.},
  isbn = {978-0-412-39860-5},
  langid = {english},
  pagetotal = {288}
}

@article{neal2003,
  title = {Slice Sampling},
  author = {Neal, Radford M.},
  date = {2003-06},
  journaltitle = {The Annals of Statistics},
  volume = {31},
  pages = {705--767},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1056562461},
  abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can ample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
  keywords = {65C05,65C60,Adaptive methods,auxiliary variables,doing,dynamical methods,Gibbs sampling,Markov chain Monte Carlo,Metropolis algorithm,overrelaxation},
  number = {3}
}

@book{nesterov2004,
  title = {Introductory {{Lectures}} on {{Convex Optimization}}: {{A Basic Course}}},
  shorttitle = {Introductory {{Lectures}} on {{Convex Optimization}}},
  author = {Nesterov, Yurii},
  date = {2004},
  publisher = {{Springer US}},
  doi = {10.1007/978-1-4419-8853-9},
  abstract = {It was in the middle of the 1980s, when the seminal paper by Kar­ markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op­ timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre­ diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc­ tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop­ ing field, which got the name "polynomial-time interior-point methods", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].},
  isbn = {978-1-4020-7553-7},
  langid = {english},
  series = {Applied {{Optimization}}}
}

@book{nesterov2018,
  title = {Lectures on {{Convex Optimization}}},
  author = {Nesterov, Yurii},
  date = {2018},
  edition = {2},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-91578-4},
  abstract = {This book provides a comprehensive, modern introduction to convex optimization, a field that is becoming increasingly important in applied mathematics, economics and finance, engineering, and computer science, notably in data science and machine learning. Written by a leading expert in the field, this book includes recent advances in the algorithmic theory of convex optimization, naturally complementing the existing literature. It contains a unified and rigorous presentation of the acceleration techniques for minimization schemes of first- and second-order. It provides readers with a full treatment of the smoothing technique, which has tremendously extended the abilities of gradient-type methods. Several powerful approaches in structural optimization, including optimization in relative scale and polynomial-time interior-point methods, are also discussed in detail. Researchers in theoretical optimization as well as professionals working on optimization problems will find this book very useful. It presents many successful examples of how to develop very fast specialized minimization algorithms. Based on the author’s lectures, it can naturally serve as the basis for introductory and advanced courses in convex optimization for students in engineering, economics, computer science and mathematics.},
  isbn = {978-3-319-91577-7},
  langid = {english},
  series = {Springer {{Optimization}} and {{Its Applications}}}
}

@book{newman2010,
  title = {Networks: {{An Introduction}}},
  shorttitle = {Networks},
  author = {Newman, Mark},
  date = {2010-03-25},
  publisher = {{Oxford University Press}},
  abstract = {The scientific study of networks, including computer networks, social networks, and biological networks, has received an enormous amount of interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on a large scale, and the development of a variety of new theoretical tools has allowed us to extract new knowledge from many different kinds of networks. The study of networks is broadly interdisciplinary and important developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas. Subjects covered include the measurement and structure of networks in many branches of science, methods for analyzing network data, including methods developed in physics, statistics, and sociology, the fundamentals of graph theory, computer algorithms, and spectral methods, mathematical models of networks, including random graph models and generative models, and theories of dynamical processes taking place on networks.},
  isbn = {978-0-19-159417-5},
  langid = {american}
}

@book{newman2018,
  title = {Networks},
  author = {Newman, Mark},
  date = {2018-07-26},
  edition = {Second Edition},
  publisher = {{Oxford University Press}},
  location = {{Oxford, New York}},
  abstract = {The study of networks, including computer networks, social networks, and biological networks, has attracted enormous interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on an unprecedented scale, and the development of new theoretical tools has allowed us to extract knowledge from networks of many different kinds. The study of networks is broadly interdisciplinary and central developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas.Topics covered include the measurement of networks; methods for analyzing network data, including methods developed in physics, statistics, and sociology; fundamentals of graph theory; computer algorithms; mathematical models of networks, including random graph models and generative models; and theories of dynamical processes taking place on networks.},
  isbn = {978-0-19-880509-0},
  keywords = {networks},
  pagetotal = {800}
}

@article{neyman1933,
  title = {On the Problem of the Most Efficient Tests of Statistical Hypotheses},
  author = {Neyman, Jerzy and Pearson, Egon},
  date = {1933-02-16},
  journaltitle = {Philosophical Transactions of the Royal Society of London Series A},
  shortjournal = {Phil. Trans. R. Soc. Lond. A},
  volume = {231},
  pages = {289--337},
  issn = {0264-3952, 2053-9258},
  doi = {10.1098/rsta.1933.0009},
  keywords = {done},
  langid = {english}
}

@online{nielsen2018,
  title = {An Elementary Introduction to Information Geometry},
  author = {Nielsen, Frank},
  date = {2018-08-16},
  abstract = {We describe the fundamental differential-geometric structures of information manifolds, state the fundamental theorem of information geometry, and illustrate some uses of these information manifolds in information sciences. The exposition is self-contained by concisely introducing the necessary concepts of differential geometry with proofs omitted for brevity.},
  archiveprefix = {arXiv},
  eprint = {1808.08271},
  eprinttype = {arxiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning,todo},
  primaryclass = {cs, math, stat}
}

@book{oja2010,
  title = {Multivariate {{Nonparametric Methods}} with {{R}}: {{An}} Approach Based on Spatial Signs and Ranks},
  shorttitle = {Multivariate {{Nonparametric Methods}} with {{R}}},
  author = {Oja, Hannu},
  date = {2010},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  doi = {10.1007/978-1-4419-0468-3},
  abstract = {This book offers a new, fairly efficient, and robust alternative to analyzing multivariate data. The analysis of data based on multivariate spatial signs and ranks proceeds very much as does a traditional multivariate analysis relying on the assumption of multivariate normality; the regular L2 norm is just replaced by different L1 norms, observation vectors are replaced by spatial signs and ranks, and so on. A unified methodology starting with the simple one-sample multivariate location problem and proceeding to the general multivariate multiple linear regression case is presented. Companion estimates and tests for scatter matrices are considered as well. The R package MNM is available for computation of the procedures. This monograph provides an up-to-date overview of the theory of multivariate nonparametric methods based on spatial signs and ranks. The classical book by Puri and Sen (1971) uses marginal signs and ranks and different type of L1 norm. The book may serve as a textbook and a general reference for the latest developments in the area. Readers are assumed to have a good knowledge of basic statistical theory as well as matrix theory. Hannu Oja is an academy professor and a professor in biometry in the University of Tampere. He has authored and coauthored numerous research articles in multivariate nonparametrical and robust methods as well as in biostatistics.},
  isbn = {978-1-4419-0467-6},
  langid = {english},
  series = {Lecture {{Notes}} in {{Statistics}}}
}

@article{opensciencecollaboration2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  date = {2015-08-28},
  journaltitle = {Science},
  volume = {349},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  abstract = {Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 Structured Abstract INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
  eprint = {26315443},
  eprinttype = {pmid},
  keywords = {todo},
  langid = {english},
  number = {6251}
}

@unpublished{orbanz2014,
  title = {Lecture {{Notes}} on {{Bayesian Nonparametrics}}},
  author = {Orbanz, Peter},
  date = {2014},
  keywords = {done}
}

@book{pace1997,
  title = {Principles of Statistical Inference: From a Neo-Fisherian Perspective},
  shorttitle = {Principles of Statistical Inference},
  author = {Pace, Luigi and Salvan, Alessandra},
  date = {1997-08-05},
  publisher = {{World Scientific Pub}},
  location = {{Singapore}},
  abstract = {In this book, an integrated introduction to statistical inference is provided from a frequentist likelihood-based viewpoint. Classical results are presented together with recent developments, largely built upon ideas due to R.A. Fisher. The term "neo-Fisherian" highlights this.After a unified review of background material (statistical models, likelihood, data and model reduction, first-order asymptotics) and inference in the presence of nuisance parameters (including pseudo-likelihoods), a self-contained introduction is given to exponential families, exponential dispersion models, generalized linear models, and group families. Finally, basic results of higher-order asymptotics are introduced (index notation, asymptotic expansions for statistics and distributions, and major applications to likelihood inference).The emphasis is more on general concepts and methods than on regularity conditions. Many examples are given for specific statistical models. Each chapter is supplemented with problems and bibliographic notes. This volume can serve as a textbook in intermediate-level undergraduate and postgraduate courses in statistical inference.},
  isbn = {978-981-02-3066-1},
  langid = {Inglese},
  pagetotal = {535}
}

@book{pace2001,
  title = {Introduzione Alla Statistica: {{Inferenza}}, Verosimiglianza, Modelli},
  author = {Pace, Luigi and Salvan, Alessandra},
  date = {2001},
  volume = {2},
  publisher = {{CEDAM}}
}

@article{pace2020,
  title = {Likelihood, {{Replicability}} and {{Robbins Confidence Sequences}}},
  author = {Pace, Luigi and Salvan, Alessandra},
  date = {2020-12},
  journaltitle = {International Statistical Review},
  shortjournal = {International Statistical Review},
  volume = {88},
  pages = {599--615},
  issn = {0306-7734, 1751-5823},
  doi = {10.1111/insr.12355},
  keywords = {doing},
  langid = {english},
  number = {3}
}

@article{page1954,
  title = {Continuous {{Inspection Schemes}}},
  author = {Page, E. S.},
  date = {1954-06},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {41},
  pages = {100},
  issn = {00063444},
  doi = {10.2307/2333009},
  eprint = {2333009},
  eprinttype = {jstor},
  keywords = {done},
  number = {1/2}
}

@incollection{palmgren2014,
  title = {Transition ({{Markov}}) {{Models}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Palmgren, Juni},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2014-09-29},
  pages = {stat00360},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat00360},
  isbn = {978-1-118-44511-2},
  keywords = {todo},
  langid = {english}
}

@article{panaretos2019,
  title = {Statistical {{Aspects}} of {{Wasserstein Distances}}},
  author = {Panaretos, Victor M. and Zemel, Yoav},
  date = {2019-03-07},
  journaltitle = {Annual Review of Statistics and Its Application},
  shortjournal = {Annu. Rev. Stat. Appl.},
  volume = {6},
  pages = {405--431},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-030718-104938},
  abstract = {Wasserstein distances are metrics on probability distributions inspired by the problem of optimal mass transportation. Roughly speaking, they measure the minimal effort required to reconfigure the probability mass of one distribution in order to recover the other distribution. They are ubiquitous in mathematics, with a long history that has seen them catalyse core developments in analysis, optimization, and probability. Beyond their intrinsic mathematical richness, they possess attractive features that make them a versatile tool for the statistician: they can be used to derive weak convergence and convergence of moments, and can be easily bounded; they are well-adapted to quantify a natural notion of perturbation of a probability distribution; and they seamlessly incorporate the geometry of the domain of the distributions in question, thus being useful for contrasting complex objects. Consequently, they frequently appear in the development of statistical theory and inferential methodology, and have recently become an object of inference in themselves. In this review, we provide a snapshot of the main concepts involved in Wasserstein distances and optimal transportation, and a succinct overview of some of their many statistical aspects.},
  archiveprefix = {arXiv},
  eprint = {1806.05500},
  eprinttype = {arxiv},
  keywords = {62-00 (primary); 62G99; 62M99 (secondary),deformation map,empirical optimal transport,Fréchet mean,goodness-of-fit,inference,Kantorovich distance,Monge–Kantorovich problem,optimal coupling,probability metric,Statistics - Methodology,todo,transportation of measure,warping and registration,Wasserstein distance,Wasserstein space},
  number = {1}
}

@article{peano1890,
  title = {Sur une courbe, qui remplit toute une aire plane},
  author = {Peano, G.},
  date = {1890-03},
  journaltitle = {Mathematische Annalen},
  shortjournal = {Math. Ann.},
  volume = {36},
  pages = {157--160},
  issn = {0025-5831, 1432-1807},
  doi = {10.1007/BF01199438},
  keywords = {done},
  langid = {french},
  number = {1}
}

@book{petris2009,
  title = {Dynamic Linear Models with R},
  author = {Petris, Giovanni and Petrone, Sonia and Campagnoli, Patrizia},
  date = {2009-06-02},
  edition = {2009 ed. edizione},
  publisher = {{Springer}},
  location = {{Dordrecht ; New York}},
  abstract = {This text introduces general state space models in detail before focusing on dynamic linear models, emphasizing their Bayesian analysis. It illustrates all the fundamental steps needed to use dynamic linear models in practice, using R.},
  isbn = {978-0-387-77237-0},
  langid = {Inglese},
  pagetotal = {268}
}

@online{pewsey2020,
  title = {Recent Advances in Directional Statistics},
  author = {Pewsey, Arthur and García-Portugués, Eduardo},
  date = {2020-05-14},
  abstract = {Mainstream statistical methodology is generally applicable to data observed in Euclidean space. There are, however, numerous contexts of considerable scientific interest in which the natural supports for the data under consideration are Riemannian manifolds like the unit circle, torus, sphere and their extensions. Typically, such data can be represented using one or more directions, and directional statistics is the branch of statistics that deals with their analysis. In this paper we provide a review of the many recent developments in the field since the publication of Mardia and Jupp (1999), still the most comprehensive text on directional statistics. Many of those developments have been stimulated by interesting applications in fields as diverse as astronomy, medicine, genetics, neurology, aeronautics, acoustics, image analysis, text mining, environmetrics, and machine learning. We begin by considering developments for the exploratory analysis of directional data before progressing to distributional models, general approaches to inference, hypothesis testing, regression, nonparametric curve estimation, methods for dimension reduction, classification and clustering, and the modelling of time series, spatial, and spatio-temporal data. We end with an overview of presently available software for analyzing directional data.},
  archiveprefix = {arXiv},
  eprint = {2005.06889},
  eprinttype = {arxiv},
  keywords = {62H11,Statistics - Methodology,todo},
  primaryclass = {stat}
}

@online{peyre2020,
  title = {Computational {{Optimal Transport}}},
  author = {Peyré, Gabriel and Cuturi, Marco},
  date = {2020-03-18},
  abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archiveprefix = {arXiv},
  eprint = {1803.00567},
  eprinttype = {arxiv},
  keywords = {Statistics - Machine Learning,todo},
  primaryclass = {stat}
}

@article{piironen2017,
  title = {Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors},
  author = {Piironen, Juho and Vehtari, Aki},
  date = {2017},
  journaltitle = {Electronic Journal of Statistics},
  shortjournal = {Electron. J. Statist.},
  volume = {11},
  pages = {5018--5051},
  issn = {1935-7524},
  doi = {10.1214/17-EJS1337SI},
  abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but has previously suffered from two problems. First, there has been no systematic way of specifying a prior for the global shrinkage hyperparameter based on the prior information about the degree of sparsity in the parameter vector. Second, the horseshoe prior has the undesired property that there is no possibility of specifying separately information about sparsity and the amount of regularization for the largest coefficients, which can be problematic with weakly identified parameters, such as the logistic regression coefficients in the case of data separation. This paper proposes solutions to both of these problems. We introduce a concept of effective number of nonzero parameters, show an intuitive way of formulating the prior for the global hyperparameter based on the sparsity assumptions, and argue that the previous default choices are dubious based on their tendency to favor solutions with more unshrunk parameters than we typically expect a priori. Moreover, we introduce a generalization to the horseshoe prior, called the regularized horseshoe, that allows us to specify a minimum level of regularization to the largest values. We show that the new prior can be considered as the continuous counterpart of the spike-and-slab prior with a finite slab width, whereas the original horseshoe resembles the spike-and-slab with an infinitely wide slab. Numerical experiments on synthetic and real world data illustrate the benefit of both of these theoretical advances.},
  archiveprefix = {arXiv},
  eprint = {1707.01694},
  eprinttype = {arxiv},
  keywords = {done,Statistics - Methodology},
  number = {2}
}

@book{poldrack2011,
  title = {Handbook of Functional MRI Data Analysis},
  author = {Poldrack, Russell A. and Mumford, Jeanette A. and Nichols, Thomas E.},
  date = {2011-08-22},
  publisher = {{Cambridge University Press}},
  abstract = {Functional magnetic resonance imaging (fMRI) has become the most popular method for imaging brain function. Handbook of Functional MRI Data Analysis provides a comprehensive and practical introduction to the methods used for fMRI data analysis. Using minimal jargon, this book explains the concepts behind processing fMRI data, focusing on the techniques that are most commonly used in the field. This book provides background about the methods employed by common data analysis packages including FSL, SPM and AFNI. Some of the newest cutting-edge techniques, including pattern classification analysis, connectivity modeling and resting state network analysis, are also discussed. Readers of this book, whether newcomers to the field or experienced researchers, will obtain a deep and effective knowledge of how to employ fMRI analysis to ask scientific questions and become more sophisticated users of fMRI analysis software.},
  isbn = {978-0-521-51766-9},
  langid = {Inglese},
  pagetotal = {238}
}

@online{polson2013,
  title = {Bayesian Inference for Logistic Models Using {{Polya}}-{{Gamma}} Latent Variables},
  author = {Polson, Nicholas G. and Scott, James G. and Windle, Jesse},
  date = {2013-07-22},
  abstract = {We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods. The approach appeals to a new class of Polya-Gamma distributions, which are constructed in detail. A variety of examples are presented to show the versatility of the method, including logistic regression, negative binomial regression, nonlinear mixed-effects models, and spatial models for count data. In each case, our data-augmentation strategy leads to simple, effective methods for posterior inference that: (1) circumvent the need for analytic approximations, numerical integration, or Metropolis-Hastings; and (2) outperform other known data-augmentation strategies, both in ease of use and in computational efficiency. All methods, including an efficient sampler for the Polya-Gamma distribution, are implemented in the R package BayesLogit. In the technical supplement appended to the end of the paper, we provide further details regarding the generation of Polya-Gamma random variables; the empirical benchmarks reported in the main manuscript; and the extension of the basic data-augmentation framework to contingency tables and multinomial outcomes.},
  archiveprefix = {arXiv},
  eprint = {1205.0310},
  eprinttype = {arxiv},
  keywords = {bayes,data augmentation,latent variables,logistic regression,Polya-Gamma,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology,todo},
  primaryclass = {stat},
  version = {3}
}

@book{prado2010,
  title = {Time Series: Modeling, Computation, and Inference},
  shorttitle = {Time Series},
  author = {Prado, Raquel and West, Mike},
  date = {2010-05-21},
  publisher = {{Chapman and Hall/CRC}},
  location = {{Boca Raton}},
  abstract = {Focusing on Bayesian approaches and computations using simulation-based methods for inference, Time Series: Modeling, Computation, and Inference integrates mainstream approaches for time series modeling with significant recent developments in methodology and applications of time series analysis. It encompasses a graduate-level account of Bayesian time series modeling and analysis, a broad range of references to state-of-the-art approaches to univariate and multivariate time series analysis, and emerging topics at research frontiers.  The book presents overviews of several classes of models and related methodology for inference, statistical computation for model fitting and assessment, and forecasting. The authors also explore the connections between time- and frequency-domain approaches and develop various models and analyses using Bayesian tools, such as Markov chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC) methods. They illustrate the models and methods with examples and case studies from a variety of fields, including signal processing, biomedicine, and finance. Data sets, R and MATLAB® code, and other material are available on the authors’ websites.  Along with core models and methods, this text offers sophisticated tools for analyzing challenging time series problems. It also demonstrates the growth of time series analysis into new application areas.},
  isbn = {978-1-4200-9336-0},
  langid = {Inglese},
  pagetotal = {368}
}

@incollection{prado2016,
  title = {Dynamic {{Bayesian Models}}: {{Inference}} and {{Forecasting}}},
  shorttitle = {Dynamic {{Bayesian Models}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Prado, Raquel},
  date = {2016},
  pages = {1--18},
  publisher = {{American Cancer Society}},
  doi = {10.1002/9781118445112.stat00219.pub2},
  abstract = {Bayesian forecasting encompasses statistical theory and methods in time-series analysis and time-series forecasting, particularly approaches using dynamic and state-space models, although the underlying concepts and theoretical foundation relate to probability modeling and inference more generally. This entry focuses specifically on Bayesian time-series modeling, inference, and forecasting via dynamic models, with mention of related areas.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat00219.pub2},
  isbn = {978-1-118-44511-2},
  keywords = {Bayesian inference,dynamic linear models,forecasting,time series,todo},
  langid = {english}
}

@book{qiu2014,
  title = {Introduction to {{Statistical Process Control}}},
  author = {Qiu, Peihua},
  date = {2014},
  publisher = {{CRC Press}},
  abstract = {A major tool for quality control and management, statistical process control (SPC) monitors sequential processes, such as production lines and Internet traffic, to ensure that they work stably and satisfactorily. Along with covering traditional methods, Introduction to Statistical Process Control describes many recent SPC methods that improve upon the more established techniques. The author—a leading researcher on SPC—shows how these methods can handle new applications.After exploring the role of SPC and other statistical methods in quality control and management, the book covers basic statistical concepts and methods useful in SPC. It then systematically describes traditional SPC charts, including the Shewhart, CUSUM, and EWMA charts, as well as recent control charts based on change-point detection and fundamental multivariate SPC charts under the normality assumption. The text also introduces novel univariate and multivariate control charts for cases when the normality assumption is invalid and discusses control charts for profile monitoring. All computations in the examples are solved using R, with R functions and datasets available for download on the author’s website. Offering a systematic description of both traditional and newer SPC methods, this book is ideal as a primary textbook for a one-semester course in disciplines concerned with process quality control, such as statistics, industrial and systems engineering, and management sciences. It can also be used as a supplemental textbook for courses on quality improvement and system management. In addition, the book provides researchers with many useful, recent research results on SPC and gives quality control practitioners helpful guidelines on implementing up-to-date SPC techniques.}
}

@article{qiu2019,
  title = {Big {{Data}}? {{Statistical Process Control Can Help}}!},
  shorttitle = {Big {{Data}}?},
  author = {Qiu, Peihua},
  date = {2019-12-02},
  journaltitle = {The American Statistician},
  volume = {0},
  pages = {1--16},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2019.1700163},
  abstract = {“Big data” is a buzzword these days due to an enormous amount of data-rich applications in different industries and research projects. In practice, big data often take the form of data streams in the sense that new batches of data keep being collected over time. One fundamental research problem when analyzing big data in a given application is to monitor the underlying sequential process of the observed data to see whether it is longitudinally stable, or how its distribution changes over time. To monitor a sequential process, one major statistical tool is the statistical process control (SPC) charts, which have been developed and used mainly for monitoring production lines in the manufacturing industries during the past several decades. With many new and versatile SPC methods developed in the recent research, it is our belief that SPC can become a powerful tool for handling many big data applications that are beyond the production line monitoring. In this article, we introduce some recent SPC methods, and discuss their potential to solve some big data problems. Certain challenges in the interface between the current SPC research and some big data applications are also discussed.},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2019.1700163},
  keywords = {Covariates,Data-rich applications,done,Dynamic processes,Feature extraction,Image data,Spatio-temporal data},
  number = {0}
}

@article{qu2019,
  title = {Boundary {{Detection Using}} a {{Bayesian Hierarchical Model}} for {{Multiscale Spatial Data}}},
  author = {Qu, Kai and Bradley, Jonathan R. and Niu, Xufeng},
  date = {2019},
  journaltitle = {Technometrics},
  volume = {63},
  pages = {64--76},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2019.1677268},
  abstract = {Spatial boundary analysis has attained considerable attention in several disciplines including engineering, shape analysis, spatial statistics, and computer science. The inferential question of interest is often to identify rapid surface change of an unobserved latent process. Curvilinear wombling and crisp wombling (or fuzzy) are two major approaches that have emerged in Bayesian spatial statistics literature. These methods are limited to a single spatial scale even though data with multiple spatial scales are often accessible. Thus, we propose a multiscale representation of the directional derivative Karhunen–Loéve expansion to perform directionally based boundary detection. Taking a multiscale spatial perspective allows us, for the first time, to consider the concept of curvilinear boundary fallacy (CBF) error, which is a boundary detection analog to the ecological fallacy that is often studied in spatial change of support literature. Furthermore, we propose a directionally based multiscale curvilinear boundary error criterion to quantify CBF. We refer to this metric as the criterion for boundary aggregation error (BAGE), and use it to perform boundary detection. Several theoretical results are derived to motivate BAGE. In particular, we show that no BAGE exists when the directional derivatives of eigenfunctions of a KL expansion are constant across spatial scales. We illustrate the use of our model through a simulated example and an analysis of Mediterranean wind measurements data. Supplementary materials for this article are available online.},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2019.1677268},
  keywords = {Change of support,Gradient,Image segmentation,todo},
  number = {1}
}

@book{ramsay2002,
  title = {Applied {{Functional Data Analysis}}: {{Methods}} and {{Case Studies}}},
  shorttitle = {Applied {{Functional Data Analysis}}},
  author = {Ramsay, J. O. and Silverman, B. W.},
  date = {2002},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  doi = {10.1007/b98886},
  abstract = {Almost as soon as we had completed our previous book Functional Data Analysis in 1997, it became clear that potential interest in the ?eld was far wider than the audience for the thematic presentation we had given there. At the same time, both of us rapidly became involved in relevant new research involving many colleagues in ?elds outside statistics. This book treats the ?eld in a di?erent way, by considering case st- ies arising from our own collaborative research to illustrate how functional data analysis ideas work out in practice in a diverse range of subject areas. These include criminology, economics, archaeology, rheumatology, psych- ogy, neurophysiology, auxology (the study of human growth), meteorology, biomechanics, and education—and also a study of a juggling statistician. Obviously such an approach will not cover the ?eld exhaustively, and in any case functional data analysis is not a hard-edged closed system of thought. Nevertheless we have tried to give a ?avor of the range of meth- ology we ourselves have considered. We hope that our personal experience, including the fun we had working on these projects, will inspire others to extend “functional” thinking to many other statistical contexts. Of course, manyofourcasestudiesrequireddevelopmentofexistingmethodology,and readersshouldgaintheabilitytoadaptmethodstotheirownproblemstoo.},
  isbn = {978-0-387-95414-1},
  langid = {english},
  series = {Springer {{Series}} in {{Statistics}}}
}

@book{ramsay2005,
  title = {Functional {{Data Analysis}}},
  author = {Ramsay, James and Silverman, B. W.},
  date = {2005},
  edition = {2},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  doi = {10.1007/b98888},
  abstract = {Scientists and others today often collect samples of curves and other functional observations. This monograph presents many ideas and techniques for such data. Included are expressions in the functional domain of such classics as linear regression, principal components analysis, linear modeling, and canonical correlation analysis, as well as specifically functional techniques such as curve registration and principal differential analysis. Data arising in real applications are used throughout for both motivation and illustration, showing how functional approaches allow us to see new things, especially by exploiting the smoothness of the processes generating the data. The data sets exemplify the wide scope of functional data analysis; they are drawn from growth analysis, meteorology, biomechanics, equine science, economics, and medicine. The book presents novel statistical technology, much of it based on the authors’ own research work, while keeping the mathematical level widely accessible. It is designed to appeal to students, to applied data analysts, and to experienced researchers; it will have value both within statistics and across a broad spectrum of other fields. This second edition is aimed at a wider range of readers, and especially those who would like to apply these techniques to their research problems. It complements the authors' other recent volume Applied Functional Data Analysis: Methods and Case Studies. In particular, there is an extended coverage of data smoothing and other matters arising in the preliminaries to a functional data analysis. The chapters on the functional linear model and modeling of the dynamics of systems through the use of differential equations and principal differential analysis have been completely rewritten and extended to include new developments. Other chapters have been revised substantially, often to give more weight to examples and practical considerations. Jim Ramsay is Professor of Psychology at McGill University and is an international authority on many aspects of multivariate analysis. He was President of the Statistical Society of Canada in 2002-3 and holds the Society’s Gold Medal for his work in functional data analysis. Bernard Silverman is Master of St Peter’s College and Professor of Statistics at Oxford University. He was President of the Institute of Mathematical Statistics in 2000–1. He is a Fellow of the Royal Society. His main specialty is in computational statistics, and he is the author or editor of several highly regarded books in this area.},
  isbn = {978-0-387-40080-8},
  langid = {english},
  series = {Springer {{Series}} in {{Statistics}}}
}

@online{ranganath2013,
  title = {Black {{Box Variational Inference}}},
  author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
  date = {2013-12-31},
  abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
  archiveprefix = {arXiv},
  eprint = {1401.0118},
  eprinttype = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology,todo},
  primaryclass = {cs, stat}
}

@article{rao2007,
  title = {Has Statistics a Future? {{If}} so in What Form?},
  shorttitle = {Has Statistics a Future?},
  author = {Rao, C.Radhakrishna},
  date = {2007-01-01},
  journaltitle = {Journal of the Indian Society of Agricultural Statistics},
  shortjournal = {Journal of the Indian Society of Agricultural Statistics},
  volume = {61},
  doi = {10.1142/9789812776372_0022},
  abstract = {The mathematical foundations of statistics as a separate discipline were laid by Fisher, Neyman and Wald during the second quarter of the last century. Subsequent research in statistics and the courses taught in the universities are mostly based on the guidelines set by these pioneers. Statistics is used in some form or other in all areas of human endeavor from scientific research to optimum use of resources for social welfare, prediction and decision-making. However, there are controversies in statistics, especially in the choice of a model for data, use of prior probabilities and subject-matter judgments by experts. The same data analyzed by different consulting statisticians may lead to different conclusions. What is the future of statistics in the present millennium dominated by information technology encompassing the whole of communications, interaction with intelligent systems, massive data bases, and complex information processing networks? The current statistical methodology based on simple probabilistic models developed for the analysis of small data sets appears to be inadequate to meet the needs of customers for quick on line processing of data and making the information available for practical use. Some methods are being put forward in the name of data mining for such purposes. A broad review of the current state of the art in statistics, its merits and demerits, and possible future developments is presented.},
  keywords = {done}
}

@book{rasmussen2005,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  date = {2005-11-23},
  publisher = {{Mit Pr}},
  location = {{Cambridge, Mass}},
  abstract = {A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines.Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.},
  isbn = {978-0-262-18253-9},
  langid = {Inglese},
  pagetotal = {2448}
}

@book{resnick2013,
  title = {Adventures in Stochastic Processes},
  author = {Resnick, Sidney I.},
  date = {2013-12-11},
  edition = {Reprint edizione},
  publisher = {{Birkhauser}},
  location = {{Erscheinungsort nicht ermittelbar}},
  abstract = {Stochastic processes are necessary ingredients for building models of a wide variety of phenomena exhibiting time varying randomness. This text offers easy access to this fundamental topic for many students of applied sciences at many levels. It includes examples, exercises, applications, and computational procedures. It is uniquely useful for beginners and non-beginners in the field. No knowledge of measure theory is presumed.},
  isbn = {978-1-4612-6738-6},
  langid = {Inglese},
  pagetotal = {640}
}

@article{reynolds2006,
  title = {Multivariate {{Control Charts}} for {{Monitoring}} the {{Mean Vector}} and {{Covariance Matrix}} with {{Variable Sampling Intervals}}},
  author = {Reynolds, Marion R. Jr. and Cho, Gyo-Young},
  date = {2006-01-21},
  journaltitle = {Sequential Analysis},
  volume = {30},
  pages = {1--40},
  publisher = {{Taylor \& Francis}},
  issn = {0747-4946},
  doi = {10.1080/07474946.2010.520627},
  abstract = {Traditional control charts for process monitoring are fixed sampling rate (FSR) control charts based on taking samples of fixed size with a fixed sampling interval between samples. Variable sampling rate (VSR) control charts vary the sampling rate as a function of the data from the process, and can detect most process changes significantly faster than traditional FSR control charts. The main focus of this article is on a type of VSR control chart called a variable sampling interval (VSI) control chart, but another type of VSR control chart based on sequential sampling is also considered. We consider the problem of detecting sustained or transient shifts in the mean or variability of a process. We investigate Shewhart control charts and multivariate exponentially weighted moving average (MEWMA) control charts based on sample means and on the sum of the squared deviations from target. We show that the combination of the standard MEWMA chart and an MEWMA-type chart based on squared deviations from target gives excellent overall performance. When the VSI feature is used with this combination, the performance is dramatically better than the performance of the standard FSR multivariate control charts that have traditionally been used.},
  annotation = {\_eprint: https://doi.org/10.1080/07474946.2010.520627},
  keywords = {62D99,62L10,62P30,Average time to signal,done,Exponentially weighted moving average control chart,Multivariate exponentially weighted moving average control chart,Regression adjustment of variables,Sequential sampling,Squared deviations from target,Statistical process control,Steady-state average time to signal},
  number = {1}
}

@article{rhudy2017,
  title = {A {{Kalman Filtering Tutorial}} for {{Undergraduate Students}}},
  author = {Rhudy, Matthew B and Salguero, Roger A and Holappa, Keaton},
  date = {2017-02-28},
  journaltitle = {International Journal of Computer Science \& Engineering Survey},
  shortjournal = {IJCSES},
  volume = {08},
  pages = {01--18},
  issn = {09763252, 09762760},
  doi = {10.5121/ijcses.2017.8101},
  keywords = {done},
  langid = {english},
  number = {01}
}

@book{riker1988,
  title = {Liberalism {{Against Populism}}: {{A Confrontation Between}} the {{Theory}} of {{Democracy}} and the {{Theory}} of {{Social Choice}}},
  shorttitle = {Liberalism {{Against Populism}}},
  author = {Riker, William H.},
  date = {1988-07-01},
  publisher = {{Waveland Pr Inc}},
  location = {{Prospect Heights, Ill}},
  abstract = {The discoveries of social choice theory have undermined the simple and unrealistic 19th century notions of democracy, especially the expectation that electoral institutions smoothly translate popular will directly into public policy. One response to these discoveries is to reject democracy out of hand. Another, which is the program of this book, is to save democracy by formulating more realistic expectations. Hence, this book first summarizes social choice theory in order to explain the full force of its critique. Then it explains, in terms of social choice theory, how politics and public issues change and develop. Finally, it reconciles democratic ideals with this new understanding of politics.},
  isbn = {978-0-88133-367-1},
  langid = {english},
  pagetotal = {311}
}

@software{rimella2017,
  title = {{{RGeode}}: {{Geometric Density Estimation}}},
  shorttitle = {{{RGeode}}},
  author = {Rimella, Lorenzo},
  date = {2017-09-04},
  abstract = {Provides the hybrid Bayesian method Geometric Density Estimation. On the one hand, it scales the dimension of our data, on the other it performs inference. The method is fully described in the paper "Scalable Geometric Density Estimation" by Y. Wang, A. Canale, D. Dunson (2016) {$<$}http://proceedings.mlr.press/v51/wang16e.pdf{$>$}.},
  keywords = {done},
  version = {0.1.0}
}

@misc{rinaldi2020,
  title = {Notes on {{Convex Optimization}}},
  author = {Rinaldi, Francesco},
  date = {2020}
}

@book{ripley2008,
  title = {Pattern Recognition and Neural Networks},
  author = {Ripley, Brian D.},
  date = {2008-01-10},
  edition = {1 edizione},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  abstract = {This 1996 book is a reliable account of the statistical framework for pattern recognition and machine learning. With unparalleled coverage and a wealth of case-studies this book gives valuable insight into both the theory and the enormously diverse applications (which can be found in remote sensing, astrophysics, engineering and medicine, for example). So that readers can develop their skills and understanding, many of the real data sets used in the book are available from the author's website: www.stats.ox.ac.uk/\textasciitilde ripley/PRbook/. For the same reason, many examples are included to illustrate real problems in pattern recognition. Unifying principles are highlighted, and the author gives an overview of the state of the subject, making the book valuable to experienced researchers in statistics, machine learning/artificial intelligence and engineering. The clear writing style means that the book is also a superb introduction for non-specialists.},
  isbn = {978-0-521-71770-0},
  langid = {Inglese},
  pagetotal = {416}
}

@article{robbins1951,
  title = {A {{Stochastic Approximation Method}}},
  author = {Robbins, Herbert and Monro, Sutton},
  date = {1951-09},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {22},
  pages = {400--407},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729586},
  abstract = {Let \$M(x)\$ denote the expected value at level \$x\$ of the response to a certain experiment. \$M(x)\$ is assumed to be a monotone function of \$x\$ but is unknown to the experimenter, and it is desired to find the solution \$x = \textbackslash theta\$ of the equation \$M(x) = \textbackslash alpha\$, where \$\textbackslash alpha\$ is a given constant. We give a method for making successive experiments at levels \$x\_1,x\_2,\textbackslash cdots\$ in such a way that \$x\_n\$ will tend to \$\textbackslash theta\$ in probability.},
  keywords = {todo},
  number = {3}
}

@article{robbins1970,
  title = {Statistical {{Methods Related}} to the {{Law}} of the {{Iterated Logarithm}}},
  author = {Robbins, Herbert},
  date = {1970-10},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {41},
  pages = {1397--1409},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177696786},
  abstract = {The Annals of Mathematical Statistics},
  keywords = {todo},
  number = {5}
}

@book{robert2004,
  title = {Monte Carlo Statistical Methods},
  author = {Robert, Christian P. and Casella, George},
  date = {2004-09-01},
  publisher = {{Springer Nature}},
  location = {{New York}},
  abstract = {We have sold 4300 copies worldwide of the first edition (1999).This new edition contains five completely new chapters covering new developments.},
  isbn = {978-0-387-21239-5},
  langid = {Inglese},
  pagetotal = {645}
}

@book{robert2009,
  title = {Introducing Monte Carlo Methods with R},
  author = {Robert, Christian and Casella, George},
  date = {2009-12-10},
  edition = {1 edizione},
  publisher = {{Springer}},
  location = {{New York}},
  abstract = {This book covers the main tools used in statistical simulation from a programmer’s point of view, explaining the R implementation of each simulation technique and providing the output for better understanding and comparison.},
  isbn = {978-1-4419-1575-7},
  langid = {Inglese},
  pagetotal = {304}
}

@unpublished{robert2020,
  title = {Notes on {{Functional Analysis}}},
  author = {Robert, Frédéric},
  date = {2020},
  abstract = {Notes from lectures given in SMI Perugia 2020}
}

@online{robinson2020,
  title = {Tree-{{SNE}}: {{Hierarchical Clustering}} and {{Visualization Using}} t-{{SNE}}},
  shorttitle = {Tree-{{SNE}}},
  author = {Robinson, Isaac and Pierce-Hoffman, Emma},
  date = {2020-02-13},
  abstract = {t-SNE and hierarchical clustering are popular methods of exploratory data analysis, particularly in biology. Building on recent advances in speeding up t-SNE and obtaining finer-grained structure, we combine the two to create tree-SNE, a hierarchical clustering and visualization algorithm based on stacked one-dimensional t-SNE embeddings. We also introduce alpha-clustering, which recommends the optimal cluster assignment, without foreknowledge of the number of clusters, based off of the cluster stability across multiple scales. We demonstrate the effectiveness of tree-SNE and alpha-clustering on images of handwritten digits, mass cytometry (CyTOF) data from blood cells, and single-cell RNA-sequencing (scRNA-seq) data from retinal cells. Furthermore, to demonstrate the validity of the visualization, we use alpha-clustering to obtain unsupervised clustering results competitive with the state of the art on several image data sets. Software is available at https://github.com/isaacrob/treesne.},
  archiveprefix = {arXiv},
  eprint = {2002.05687},
  eprinttype = {arxiv},
  keywords = {Computer Science - Machine Learning,data visualization,done,hierarchical clustering,nearest neighbors,Statistics - Machine Learning,t-SNE},
  primaryclass = {cs, stat}
}

@book{rocha-arteaga2019,
  title = {Topics in {{Infinitely Divisible Distributions}} and {{Lévy Processes}}, {{Revised Edition}}},
  author = {Rocha-Arteaga, Alfonso and Sato, Ken-iti},
  date = {2019},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-22700-5},
  abstract = {This book deals with topics in the area of Lévy processes and infinitely divisible distributions such as Ornstein-Uhlenbeck type processes, selfsimilar additive processes and multivariate subordination. These topics are developed around a decreasing chain of classes of distributions Lm, m = 0,1,...,∞, from the class L0 of selfdecomposable distributions to the class L∞ generated by stable distributions through convolution and convergence.The book is divided into five chapters. Chapter 1 studies basic properties of Lm classes needed for the subsequent chapters. Chapter 2 introduces Ornstein-Uhlenbeck type processes generated by a Lévy process through stochastic integrals based on Lévy processes. Necessary and sufficient conditions are given for a generating Lévy process so that the OU type process has a limit distribution of Lm class.Chapter 3 establishes the correspondence between selfsimilar additive processes and selfdecomposable distributions and makes a close inspection of the Lamperti transformation, which transforms selfsimilar additive processes and stationary type OU processes to each other. Chapter 4 studies multivariate subordination of a cone-parameter Lévy process by a cone-valued Lévy process. Finally, Chapter 5 studies strictly stable and Lm properties inherited by the subordinated process in multivariate subordination.In this revised edition, new material is included on advances in these topics. It is rewritten as self-contained as possible. Theorems, lemmas, propositions, examples and remarks were reorganized; some were deleted and others were newly added. The historical notes at the end of each chapter were enlarged.This book is addressed to graduate students and researchers in probability and mathematical statistics who are interested in learning more on Lévy processes and infinitely divisible distributions.},
  isbn = {978-3-030-22699-2},
  langid = {english},
  series = {{{SpringerBriefs}} in {{Probability}} and {{Mathematical Statistics}}}
}

@article{rodriguez2011,
  title = {Nonparametric {{Bayesian}} Models through Probit Stick-Breaking Processes},
  author = {Rodríguez, Abel and Dunson, David B.},
  date = {2011-03},
  journaltitle = {Bayesian Analysis},
  shortjournal = {Bayesian Anal.},
  volume = {6},
  pages = {145--177},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/11-BA605},
  abstract = {We describe a novel class of Bayesian nonparametric priors based on stick-breaking constructions where the weights of the process are constructed as probit transformations of normal random variables. We show that these priors are extremely flexible, allowing us to generate a great variety of models while preserving computational simplicity. Particular emphasis is placed on the construction of rich temporal and spatial processes, which are applied to two problems in finance and ecology.},
  keywords = {Data Augmentation,done,Mixture Model,Nonparametric Bayes,Random Probability Measure,Spatial Data,Stick-breaking Prior,Time Series},
  langid = {english},
  mrnumber = {MR2781811},
  number = {1},
  zmnumber = {1330.62120}
}

@article{rosenblatt2018,
  title = {All-{{Resolutions Inference}} for Brain Imaging},
  author = {Rosenblatt, Jonathan D. and Finos, Livio and Weeda, Wouter D. and Solari, Aldo and Goeman, Jelle J.},
  date = {2018-11-01},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {181},
  pages = {786--796},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2018.07.060},
  abstract = {The most prevalent approach to activation localization in neuroimaging is to identify brain regions as contiguous supra-threshold clusters, check their significance using random field theory, and correct for the multiple clusters being tested. Besides recent criticism on the validity of the random field assumption, a spatial specificity paradox remains: the larger the detected cluster, the less we know about the location of activation within that cluster. This is because cluster inference implies “there exists at least one voxel with an evoked response in the cluster”, and not that “all the voxels in the cluster have an evoked response”. Inference on voxels within selected clusters is considered bad practice, due to the voxel-wise false positive rate inflation associated with this circular inference. Here, we propose a remedy to the spatial specificity paradox. By applying recent results from the multiple testing statistical literature, we are able to quantify the proportion of truly active voxels within selected clusters, an approach we call All-Resolutions Inference (ARI). If this proportion is high, the paradox vanishes. If it is low, we can further “drill down” from the cluster level to sub-regions, and even to individual voxels, in order to pinpoint the origin of the activation. In fact, ARI allows inference on the proportion of activation in all voxel sets, no matter how large or small, however these have been selected, all from the same data. We use two fMRI datasets to demonstrate the non-triviality of the spatial specificity paradox, and its resolution using ARI. We verify that the endless circularity permitted by ARI does not render its estimates overly conservative using both simulation, and a data split.},
  keywords = {todo},
  langid = {english}
}

@book{rudin1976,
  title = {Principles of {{Mathematical Analysis}}},
  author = {Rudin, Walter},
  date = {1976-01-01},
  edition = {3 edition},
  publisher = {{McGraw-Hill Education}}
}

@book{rudin1986,
  title = {Real and {{Complex Analysis}}},
  author = {Rudin, Walter},
  date = {1986-05-01},
  edition = {3rd edition},
  publisher = {{McGraw-Hill Education}},
  location = {{New York}},
  abstract = {This is an advanced text for the one- or two-semester course in analysis taught primarily to math, science, computer science, and electrical engineering majors at the junior, senior or graduate level. The basic techniques and theorems of analysis are presented in such a way that the intimate connections between its various branches are strongly emphasized. The traditionally separate subjects of 'real analysis' and 'complex analysis' are thus united in one volume. Some of the basic ideas from functional analysis are also included. This is the only book to take this unique approach. The third edition includes a new chapter on differentiation. Proofs of theorems presented in the book are concise and complete and many challenging exercises appear at the end of each chapter. The book is arranged so that each chapter builds upon the other, giving students a gradual understanding of the subject.  This text is part of the Walter Rudin Student Series in Advanced Mathematics.},
  isbn = {978-0-07-054234-1},
  langid = {english},
  pagetotal = {483}
}

@book{rynne2007,
  title = {Linear {{Functional Analysis}}},
  author = {Rynne, Bryan and Youngson, M. A. and Youngson, Martin A.},
  date = {2007-12-29},
  edition = {2 edition},
  publisher = {{Springer}},
  abstract = {This introduction to the ideas and methods of linear functional analysis shows how familiar and useful concepts from finite-dimensional linear algebra can be extended or generalized to infinite-dimensional spaces. Aimed at advanced undergraduates in mathematics and physics, the book assumes a standard background of linear algebra, real analysis (including the theory of metric spaces), and Lebesgue integration, although an introductory chapter summarizes the requisite material. A highlight of the second edition is a new chapter on the Hahn-Banach theorem and its applications to the theory of duality.},
  langid = {english},
  pagetotal = {336}
}

@book{sagan1994,
  title = {Space-{{Filling Curves}}},
  author = {Sagan, Hans},
  date = {1994},
  publisher = {{Springer-Verlag}},
  doi = {10.1007/978-1-4612-0871-6},
  abstract = {The subject of space-filling curves has fascinated mathematicians for over a century and has intrigued many generations of students of mathematics. Working in this area is like skating on the edge of reason. Unfortunately, no comprehensive treatment has ever been attempted other than the gallant effort by W. Sierpiriski in 1912. At that time, the subject was still in its infancy and the most interesting and perplexing results were still to come. Besides, Sierpiriski's paper was written in Polish and published in a journal that is not readily accessible (Sierpiriski [2]). Most of the early literature on the subject is in French, German, and Polish, providing an additional raison d'etre for a comprehensive treatment in English. While there was, understandably, some intensive research activity on this subject around the turn of the century, contributions have, nevertheless, continued up to the present and there is no end in sight, indicating that the subject is still very much alive. The recent interest in fractals has refocused interest on space­ filling curves, and the study of fractals has thrown some new light on this small but venerable part of mathematics. This monograph is neither a textbook nor an encyclopedic treatment of the subject nor a historical account, but it is a little of each. While it may lend structure to a seminar or pro-seminar, or be useful as a supplement in a course on topology or mathematical analysis, it is primarily intended for self-study by the aficionados of classical analysis.},
  isbn = {978-0-387-94265-0},
  langid = {english},
  series = {Universitext}
}

@book{salvan2020,
  title = {Modelli Lineari Generalizzati},
  author = {Salvan, Alessandra and Sartori, Nicola and Pace, Luigi},
  date = {2020},
  publisher = {{Springer-Verlag}},
  location = {{Mailand}},
  doi = {10.1007/978-88-470-4002-1},
  abstract = {Il volume fornisce un'introduzione a teoria e applicazioni dei modelli lineari generalizzati. Si presentano modelli di regressione per risposte continue, binarie, categoriali e di conteggio. Si offre anche un'introduzione ai modelli per risposte correlate. Utilizzando il software statistico R, vengono forniti gli strumenti per l'analisi dei dati tramite i diversi modelli parametrici e semiparametrici. Gli esempi con R alla fine di ciascun capitolo rappresentano una guida ad esercitazioni con il computer e richiedono una partecipazione attiva nello svolgere le analisi proposte. Numerosi esercizi concludono ogni capitolo. Il taglio adottato è funzionale ad approfondire in modo integrato aspetti teorici e applicativi. Unico nel suo genere, è rivolto agli studenti di Scienze Statistiche.},
  isbn = {978-88-470-4001-4},
  langid = {italian},
  series = {La Matematica per il 3+2}
}

@article{sanderson2016,
  title = {Armadillo: A Template-Based {{C}}++ Library for Linear Algebra},
  shorttitle = {Armadillo},
  author = {Sanderson, Conrad and Curtin, Ryan},
  date = {2016-06-10},
  journaltitle = {Journal of Open Source Software},
  volume = {1},
  pages = {26},
  issn = {2475-9066},
  doi = {10.21105/joss.00026},
  abstract = {Sanderson et al, (2016), Armadillo: a template-based C++ library for linear algebra, Journal of Open Source Software, 1(2), 26, doi:10.21105/joss.00026},
  keywords = {todo},
  langid = {english},
  number = {2}
}

@online{sanderson2019,
  title = {A {{User}}-{{Friendly Hybrid Sparse Matrix Class}} in {{C}}++},
  author = {Sanderson, Conrad and Curtin, Ryan},
  date = {2019-10-21},
  doi = {10.1007/978-3-319-96418-8_50},
  abstract = {When implementing functionality which requires sparse matrices, there are numerous storage formats to choose from, each with advantages and disadvantages. To achieve good performance, several formats may need to be used in one program, requiring explicit selection and conversion between the formats. This can be both tedious and error-prone, especially for non-expert users. Motivated by this issue, we present a user-friendly sparse matrix class for the C++ language, with a high-level application programming interface deliberately similar to the widely used MATLAB language. The class internally uses two main approaches to achieve efficient execution: (i) a hybrid storage framework, which automatically and seamlessly switches between three underlying storage formats (compressed sparse column, coordinate list, Red-Black tree) depending on which format is best suited for specific operations, and (ii) template-based meta-programming to automatically detect and optimise execution of common expression patterns. To facilitate relatively quick conversion of research code into production environments, the class and its associated functions provide a suite of essential sparse linear algebra functionality (eg., arithmetic operations, submatrix manipulation) as well as high-level functions for sparse eigendecompositions and linear equation solvers. The latter are achieved by providing easy-to-use abstractions of the low-level ARPACK and SuperLU libraries. The source code is open and provided under the permissive Apache 2.0 license, allowing unencumbered use in commercial products.},
  archiveprefix = {arXiv},
  eprint = {1805.03380},
  eprinttype = {arxiv},
  keywords = {65F50; 97H60; 68N99; 68P05; 97N80,Computer Science - Mathematical Software,E.1,G.1.3,G.4,H.3.4,todo},
  primaryclass = {cs}
}

@book{santambrogio2015,
  title = {Optimal {{Transport}} for {{Applied Mathematicians}}},
  author = {Santambrogio, Filippo},
  date = {2015},
  volume = {87},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-20828-2},
  isbn = {978-3-319-20827-5 978-3-319-20828-2},
  series = {Progress in {{Nonlinear Differential Equations}} and {{Their Applications}}}
}

@book{savage1972,
  title = {The Foundations of Statistics},
  author = {Savage, Leonard J.},
  date = {1972},
  publisher = {{Courier Corporation}},
  abstract = {Classic analysis of the foundations of statistics and development of personal probability, one of the greatest controversies in modern statistical thought. Revised edition. Calculus, probability, statistics, and Boolean algebra are recommended.},
  eprint = {zSv6dBWneMEC},
  eprinttype = {googlebooks},
  isbn = {978-0-486-62349-8},
  keywords = {Mathematics / General,Mathematics / Probability & Statistics / General,Social Science / Statistics},
  langid = {english},
  pagetotal = {356}
}

@article{scardapane2018,
  title = {Bayesian {{Random Vector Functional}}-{{Link Networks}} for {{Robust Data Modeling}}},
  author = {Scardapane, Simone and Wang, Dianhui and Uncini, Aurelio},
  date = {2018-07},
  journaltitle = {IEEE Transactions on Cybernetics},
  volume = {48},
  pages = {2049--2059},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2017.2726143},
  abstract = {Random vector functional-link (RVFL) networks are randomized multilayer perceptrons with a single hidden layer and a linear output layer, which can be trained by solving a linear modeling problem. In particular, they are generally trained using a closed-form solution of the (regularized) least-squares approach. This paper introduces several alternative strategies for performing full Bayesian inference (BI) of RVFL networks. Distinct from standard or classical approaches, our proposed Bayesian training algorithms allow to derive an entire probability distribution over the optimal output weights of the network, instead of a single pointwise estimate according to some given criterion (e.g., least-squares). This provides several known advantages, including the possibility of introducing additional prior knowledge in the training process, the availability of an uncertainty measure during the test phase, and the capability of automatically inferring hyper-parameters from given data. In this paper, two BI algorithms for regression are first proposed that, under some practical assumptions, can be implemented by a simple iterative process with closed-form computations. Simulation results show that one of the proposed algorithms, Bayesian RVFL, is able to outperform standard training algorithms for RVFL networks with a proper regularization factor selected carefully via a line search procedure. A general strategy based on variational inference is also presented, with an application to data modeling problems with noisy outputs or outliers. As we discuss in this paper, using recent advances in automatic differentiation this strategy can be applied to a wide range of additional situations in an immediate fashion.},
  eventtitle = {{{IEEE Transactions}} on {{Cybernetics}}},
  keywords = {automatic differentiation this strategy,Bayes methods,Bayesian inference,Bayesian inference (BI),Bayesian random vector functional-link networks,Bayesian RVFL,Bayesian training algorithms,BI algorithms,closed-form computations,Computational modeling,data handling,Data models,done,Inference algorithms,inference mechanisms,iterative methods,iterative process,learning (artificial intelligence),least squares approximations,least-squares approach,linear output layer,multilayer perceptrons,probability distribution,random vector functional-link (RVFL),relevance vector machine (RVM),robust data modeling,Robustness,single hidden layer,standard training algorithms,Standards,statistical distributions,Training,variational inference,vectors},
  number = {7}
}

@book{searle2017,
  title = {Matrix Algebra Useful for Statistics},
  author = {Searle, Shayle R. and Khuri, Andre I.},
  date = {2017-06-20},
  publisher = {{John Wiley \& Sons Inc}},
  location = {{Hoboken, New Jersey}},
  abstract = {A thoroughly updated guide to matrix algebra and it uses in statistical analysis and features SAS\&reg;, MATLAB\&reg;, and R throughout This Second Edition addresses matrix algebra that is useful in the statistical analysis of data as well as within statistics as a whole. The material is presented in an explanatory style rather than a formal theorem-proof format and is self-contained. Featuring numerous applied illustrations, numerical examples, and exercises, the book has been updated to include the use of SAS, MATLAB, and R for the execution of matrix computations. In addition, Andr\&eacute; I. Khuri, who has extensive research and teaching experience in the field, joins this new edition as co-author. The Second Edition also:  Contains new coverage on vector spaces and linear transformations and discusses computational aspects of matrices Covers the analysis of balanced linear models using direct products of matrices Analyzes multiresponse linear models where several responses can be of interest Includes extensive use of SAS, MATLAB, and R throughout Contains over 400 examples and exercises to reinforce understanding along with select solutions Includes plentiful new illustrations depicting the importance of geometry as well as historical interludes  Matrix Algebra Useful for Statistics, Second Edition is an ideal textbook for advanced undergraduate and first-year graduate level courses in statistics and other related disciplines. The book is also appropriate as a reference for independent readers who use statistics and wish to improve their knowledge of matrix algebra. THE LATE SHAYLE R. SEARLE, PHD, was professor emeritus of biometry at Cornell University. He was the author of Linear Models for Unbalanced Data and Linear Models and co-author of Generalized, Linear, and Mixed Models, Second Edition, Matrix Algebra for Applied Economics, and Variance Components, all published by Wiley. Dr. Searle received the Alexander von Humboldt Senior Scientist Award, and he was an honorary fellow of the Royal Society of New Zealand. ANDR\&Eacute; I. KHURI, PHD, is Professor Emeritus of Statistics at the University of Florida. He is the author of Advanced Calculus with Applications in Statistics, Second Edition and co-author of Statistical Tests for Mixed Linear Models, all published by Wiley. Dr. Khuri is a member of numerous academic associations, among them the American Statistical Association and the Institute of Mathematical Statistics.},
  isbn = {978-1-118-93514-9},
  langid = {Inglese},
  pagetotal = {480}
}

@book{senn2003,
  title = {Dicing with {{Death}}: {{Chance}}, {{Risk And Health}}},
  shorttitle = {Dicing with {{Death}}},
  author = {Senn, Stephen},
  date = {2003-11-24},
  edition = {1 edition},
  publisher = {{Cambridge University Press}},
  location = {{New York}},
  abstract = {If you think that statistics has nothing to say about what you do or how you could do it better, then you are either wrong or in need of a more interesting job. Stephen Senn explains here how statistics determines many decisions about medical care--from allocating resources for health, to determining which drugs to license, to cause-and-effect in relation to disease. He tackles big themes: clinical trials and the development of medicines, life tables, vaccines and their risks or lack of them, smoking and lung cancer and even the power of prayer. He entertains with puzzles and paradoxes and covers the lives of famous statistical pioneers. By the end of the book the reader will see how reasoning with probability is essential to making rational decisions in medicine, and how and when it can guide us when faced with choices that impact our health and/or life. Stephen Senn has been a Professor of Pharmaceutical and Health Statistics at the University College of London since 1995. In 2001 he won George C. Challis Award of the University of Florida for contributions to biostatistics. Senn's previous two books are Statistical Issues in Drug Development (Wiley, 1997) and Cross-over Trials in Clinical Research (Wiley, 1993). He is the member of seven editorial boards including Statistics in Medicine and Pharmaceutical Statistics.},
  isbn = {978-0-521-54023-0},
  langid = {english},
  pagetotal = {264}
}

@book{service2014,
  title = {Official GRE Verbal Reasoning Practice Questions: 1},
  shorttitle = {Official GRE Verbal Reasoning Practice Questions},
  author = {Service, N/A Educational Testing},
  date = {2014-09-01},
  publisher = {{McGraw-Hill Education}},
  location = {{New York}},
  abstract = {150 REAL GRE Verbal Reasoning questions--direct from the test maker!The best way to prepare for the Verbal Reasoning measure of the GRE revised General Test is with real GRE test questions--and that is what you will find in this unique guide! Specially created for you by ETS, it offers 150 actual GRE Verbal Reasoning questions with complete explanations. Plus, this guide includes an authoritative overview of the GRE Analytical Writing measure, completewith sample writing tasks and scored sample responses.Only ETS can show you exactly what to expect on the test. So for in-depth practice and accurate, reliable test preparation for the GRE Verbal Reasoning measure, this guide is your best choice!Look inside to find:Real GRE Verbal Reasoning test questions arranged by question type and difficulty level--to help you build your test-taking skills. Plus, mixed practice sets.Answers and explanations for every question!ETS's own test-taking strategies: Valuable hints and tips that can help you do your best on the test.Official information on the GRE Verbal Reasoning measure: The facts about test content, structure, and scoring--straight from ETS.Plus: An overview of the GRE Analytical Writing measure with writing strategies, sample writing tasks, and sample scored essays.},
  isbn = {978-0-07-183429-2},
  langid = {Inglese},
  pagetotal = {241}
}

@book{service2016,
  title = {The Official Guide to the GRE General Test, Third Edition},
  author = {Service, N/A Educational Testing},
  date = {2016-12-29},
  edition = {3 edizione},
  publisher = {{McGraw-Hill Education}},
  location = {{New York}},
  abstract = {Get the only official guide to the GRE® revised General Test that comes straight from the test makers!If you're looking for the best, most authoritative guide to the GRE revised General Test, you've found it! The Official Guide to the GRE revised General Test, 3rd Edition is the only GRE guide specially created the people who actually make the test. It's packed with everything you need to do your best on the test―and move toward your graduate or business school degree.You’ll discover exactly what to expect on the test and learn how the test is scored. Hundreds of authentic test questions are included for practice, making this guide your most reliable and accurate source for everything you need to know about the GRE revised General Test.No other guide to the GRE revised General Test gives you:•~4 complete, full-length real tests―2 in the book and 2 online•~Hundreds of authentic test question to help you hone your skills •~Access to the newly redesigned PowerPrep website•~Everything you need to know about the test, straight from the test makers•~Interactive content that is fully accessible to individuals with disabilities},
  isbn = {978-1-259-86241-0},
  langid = {Inglese},
  pagetotal = {587}
}

@book{service2017,
  title = {Official GRE Quantitative Reasoning Practice Questions, Second Edition, Volume 1},
  author = {Service, N/A Educational Testing},
  date = {2017-02-17},
  edition = {2 edizione},
  publisher = {{McGraw-Hill Education}},
  abstract = {150 REAL GRE Quantitative Reasoning questions―direct from the test maker!The best way to prepare for the Quantitative Reasoning measure of the GRE® General Test is with real GRE test questions―and that is what you will find in this unique guide. Created by the actual test maker, the Official GRE Quantitative Reasoning Practice Questions, Volume 1 provides 150 actual GRE Quantitative Reasoning questions with complete explanations. There’s also an in-depth review of each of the GRE Quantitative Reasoning question types.Only ETS can show you exactly what to expect on the test. For in-depth practice and accurate, reliable test preparation for the GRE Quantitative Reasoning measure, this guide is your best choice!This essential study-guide features: •~Authentic GRE Quantitative Reasoning practice questions, answers, and explanations•~Proven success strategies for the GRE Quantitative Reasoning measure•~Valuable hints and tips to help you do your best on the test•~Official information on the GRE General Test with facts about test content, structure and scoring•~Invaluable math review and math conventions reference material},
  isbn = {978-1-259-86350-9},
  langid = {Inglese},
  pagetotal = {352}
}

@article{sethuraman1994,
  title = {A {{Constructive Definition}} of {{Dirichlet Priors}}},
  author = {Sethuraman, Jayaram},
  date = {1994},
  journaltitle = {Statistica Sinica},
  volume = {4},
  pages = {639--650},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  abstract = {In this paper we give a simple and new constructive definition of Dirichlet measures removing the restriction that the basic space should be Rk. We also give complete, self contained proofs of the three basic results for Dirichlet measures: 1. The Dirichlet measure is a probability measure on the space of all probability measures. 2. It gives probability one to the subset of discrete probability measures. 3. The posterior distribution is also a Dirichlet measure.},
  eprint = {24305538},
  eprinttype = {jstor},
  keywords = {todo},
  number = {2}
}

@book{severini2000,
  title = {Likelihood Methods in Statistics},
  author = {Severini, Thomas A.},
  date = {2000-11-09},
  edition = {New edizione},
  publisher = {{OUP Oxford}},
  location = {{Oxford ; New York}},
  abstract = {This book provides an introduction to the modern theory of likelihood-based statistical inference. This theory is characterized by several important features. One is the recognition that it is desirable to condition on relevant ancillary statistics. Another is that probability approximations are based on saddlepoint and closely related approximations that generally have very high accuracy. A third aspect is that, for models with nuisance parameters, inference is often based on marginal or conditional likelihoods, or approximations to these likelihoods. These methods have been shown often to yield substantial improvements over classical methods. The book also provides an up-to-date account of recent results in the field, which has been undergoing rapid development.},
  isbn = {978-0-19-850650-8},
  langid = {Inglese},
  pagetotal = {392}
}

@book{severini2012,
  title = {Elements of Distribution Theory},
  author = {Severini, Thomas A.},
  date = {2012-05-31},
  edition = {New edizione},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge; New York}},
  abstract = {This detailed introduction to distribution theory uses no measure theory, making it suitable for students in statistics and econometrics as well as for researchers who use statistical methods. Good backgrounds in calculus and linear algebra are important and a course in elementary mathematical analysis is useful, but not required. An appendix gives a detailed summary of the mathematical definitions and results that are used in the book. Topics covered range from the basic distribution and density functions, expectation, conditioning, characteristic functions, cumulants, convergence in distribution and the central limit theorem to more advanced concepts such as exchangeability, models with a group structure, asymptotic approximations to integrals, orthogonal polynomials and saddlepoint approximations. The emphasis is on topics useful in understanding statistical methodology; thus, parametric statistical models and the distribution theory associated with the normal distribution are covered comprehensively.},
  isbn = {978-1-107-63073-4},
  langid = {Inglese},
  pagetotal = {528}
}

@article{shahriari2016,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
  date = {2016-01},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {104},
  pages = {148--175},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2015.2494218},
  keywords = {todo},
  number = {1},
  options = {useprefix=true}
}

@article{shen2016,
  title = {Self-{{Starting Monitoring Scheme}} for {{Poisson Count Data With Varying Population Sizes}}},
  author = {Shen, Xiaobei and Tsui, Kwok-Leung and Zou, Changliang and Woodall, William H.},
  date = {2016-10-01},
  journaltitle = {Technometrics},
  volume = {58},
  pages = {460--471},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2015.1075423},
  abstract = {In this article, we consider the problem of monitoring Poisson rates when the population sizes are time-varying and the nominal value of the process parameter is unavailable. Almost all previous control schemes for the detection of increases in the Poisson rate in Phase II are constructed based on assumed knowledge of the process parameters, for example, the expectation of the count of a rare event when the process of interest is in control. In practice, however, this parameter is usually unknown and not able to be estimated with a sufficiently large number of reference samples. A self-starting exponentially weighted moving average (EWMA) control scheme based on a parametric bootstrap method is proposed. The success of the proposed method lies in the use of probability control limits, which are determined based on the observations during rather than before monitoring. Simulation studies show that our proposed scheme has good in-control and out-of-control performance under various situations. In particular, our proposed scheme is useful in rare event studies during the start-up stage of a monitoring process. Supplementary materials for this article are available online.},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2015.1075423},
  keywords = {Average run length,done,Healthcare surveillance,Poisson process,Probability control limits.},
  number = {4}
}

@article{sherman1950,
  title = {Adjustment of an {{Inverse Matrix Corresponding}} to a {{Change}} in {{One Element}} of a {{Given Matrix}}},
  author = {Sherman, Jack and Morrison, Winifred J.},
  date = {1950-03},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {21},
  pages = {124--127},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729893},
  abstract = {The Annals of Mathematical Statistics},
  keywords = {todo},
  number = {1}
}

@book{shumway2017,
  title = {Time {{Series Analysis}} and {{Its Applications}}: {{With R Examples}}},
  shorttitle = {Time {{Series Analysis}} and {{Its Applications}}},
  author = {Shumway, Robert H. and Stoffer, David S.},
  date = {2017-04-11},
  edition = {4th ed.},
  publisher = {{Springer}},
  location = {{New York, NY}},
  abstract = {The fourth edition of this popular graduate textbook, like its predecessors, presents a balanced and comprehensive treatment of both time and frequency domain methods with accompanying theory. Numerous examples using nontrivial data illustrate solutions to problems such as discovering natural and anthropogenic climate change, evaluating pain perception experiments using functional magnetic resonance imaging, and monitoring a nuclear test ban treaty.The book is designed as a textbook for graduate level students in the physical, biological, and social sciences and as a graduate level text in statistics. Some parts may also serve as an undergraduate introductory course. Theory and methodology are separated to allow presentations on different levels. In addition to coverage of classical methods of time series regression, ARIMA models, spectral analysis and state-space models, the text includes modern developments including categorical time series analysis, multivariate spectral methods, long memory series, nonlinear models, resampling techniques, GARCH models, ARMAX models, stochastic volatility, wavelets, and Markov chain Monte Carlo integration methods.This edition includes R code for each numerical example in addition to Appendix R, which provides a reference for the data sets and R scripts used in the text in addition to a tutorial on basic R commands and R time series.~An additional file is available on the book’s website for download, making all the data sets and scripts easy to load into R.},
  isbn = {978-3-319-52451-1},
  langid = {english},
  pagetotal = {562}
}

@article{sidiropoulos2017,
  title = {Tensor {{Decomposition}} for {{Signal Processing}} and {{Machine Learning}}},
  author = {Sidiropoulos, Nicholas D. and De Lathauwer, Lieven and Fu, Xiao and Huang, Kejun and Papalexakis, Evangelos E. and Faloutsos, Christos},
  date = {2017-07},
  journaltitle = {IEEE Transactions on Signal Processing},
  volume = {65},
  pages = {3551--3582},
  issn = {1941-0476},
  doi = {10.1109/TSP.2017.2690524},
  abstract = {Tensors or multiway arrays are functions of three or more indices (i, j, k, . . . )-similar to matrices (two-way arrays), which are functions of two indices (r, c) for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.},
  eventtitle = {{{IEEE Transactions}} on {{Signal Processing}}},
  keywords = {alternating direction method of multipliers,alternating optimization,canonical polyadic decomposition (CPD),classification,collaborative filtering,communications,Cramér–Rao bound,Gauss–Newton,gradient descent,harmonic retrieval,higher-order singular value decomposition (HOSVD),Matrix decomposition,mixture modeling,multilinear singular value decomposition (MLSVD),NP-hard problems,Optimization,parallel factor analysis (PARAFAC),rank,Signal processing,Signal processing algorithms,source separation,speech separation,stochastic gradient,subspace learning,Tensile stress,Tensor decomposition,tensor factorization,todo,topic modeling,Tucker model,Tutorials,uniqueness},
  number = {13}
}

@online{singpurwalla2020,
  title = {What {{Does}} the "{{Mean}}" {{Really Mean}}?},
  author = {Singpurwalla, Nozer D. and Lai, Boya},
  date = {2020-03-04},
  abstract = {The arithmetic average of a collection of observed values of a homogeneous collection of quantities is often taken to be the most representative observation. There are several arguments supporting this choice the moment of inertia being the most familiar. But what does this mean? In this note, we bring forth the Kolmogorov-Nagumo point of view that the arithmetic average is a special case of a sequence of functions of a special kind, the quadratic and the geometric means being some of the other cases. The median fails to belong to this class of functions. The Kolmogorov-Nagumo interpretation is the most defensible and the most definitive one for the arithmetic average, but its essence boils down to the fact that this average is merely an abstraction which has meaning only within its mathematical set-up.},
  archiveprefix = {arXiv},
  eprint = {2003.01973},
  eprinttype = {arxiv},
  keywords = {Chisini’s Equation,done,Kolmogorov-Nagumo Functions,Statistics - Other Statistics,Weighted Means},
  primaryclass = {stat}
}

@book{sisson2018,
  title = {Handbook of {{Approximate Bayesian Computation}}},
  editor = {Sisson, Scott A. and Fan, Yanan and Beaumont, Mark},
  date = {2018-08-10},
  edition = {1st edition},
  publisher = {{Chapman and Hall/CRC}},
  location = {{Boca Raton}},
  abstract = {As the world becomes increasingly complex, so do the statistical models required to analyse the challenging problems ahead. For the very first time in a single volume, the Handbook of Approximate Bayesian Computation (ABC) presents an extensive overview of the theory, practice and application of ABC methods. These simple, but powerful statistical techniques, take Bayesian statistics beyond the need to specify overly simplified models, to the setting where the model is defined only as a process that generates data. This process can be arbitrarily complex, to the point where standard Bayesian techniques based on working with tractable likelihood functions would not be viable. ABC methods finesse the problem of model complexity within the Bayesian framework by exploiting modern computational power, thereby permitting approximate Bayesian analyses of models that would otherwise be impossible to implement. The Handbook of ABC provides illuminating insight into the world of Bayesian modelling for intractable models for both experts and newcomers alike. It is an essential reference book for anyone interested in learning about and implementing ABC techniques to analyse complex models in the modern world.},
  isbn = {978-1-4398-8150-7},
  langid = {english},
  pagetotal = {678}
}

@article{skilling2004,
  title = {Programming the {{Hilbert}} Curve},
  author = {Skilling, John and Erickson, Gary J and Zhai, Yuxiang},
  date = {2004-04-21},
  journaltitle = {AIP Conference Proceedings},
  shortjournal = {AIP Conference Proceedings},
  volume = {707},
  pages = {381--387},
  publisher = {{American Institute of Physics}},
  issn = {0094-243X},
  doi = {10.1063/1.1751381},
  keywords = {done},
  number = {1}
}

@article{spall1992,
  title = {Multivariate Stochastic Approximation Using a Simultaneous Perturbation Gradient Approximation},
  author = {Spall, J.C.},
  date = {1992-03},
  journaltitle = {IEEE Transactions on Automatic Control},
  shortjournal = {IEEE Trans. Automat. Contr.},
  volume = {37},
  pages = {332--341},
  issn = {00189286},
  doi = {10.1109/9.119632},
  keywords = {done},
  number = {3}
}

@article{spall2000,
  title = {Adaptive Stochastic Approximation by the Simultaneous Perturbation Method},
  author = {Spall, J. C.},
  date = {2000-10},
  journaltitle = {IEEE Transactions on Automatic Control},
  volume = {45},
  pages = {1839--1853},
  issn = {1558-2523},
  doi = {10.1109/TAC.2000.880982},
  abstract = {Stochastic approximation (SA) has long been applied for problems of minimizing loss functions or root finding with noisy input information. As with all stochastic search algorithms, there are adjustable algorithm coefficients that must be specified, and that can have a profound effect on algorithm performance. It is known that choosing these coefficients according to an SA analog of the deterministic Newton-Raphson algorithm provides an optimal or near-optimal form of the algorithm. However, directly determining the required Hessian matrix (or Jacobian matrix for root finding) to achieve this algorithm form has often been difficult or impossible in practice. The paper presents a general adaptive SA algorithm that is based on a simple method for estimating the Hessian matrix, while concurrently estimating the primary parameters of interest. The approach applies in both the gradient-free optimization (Kiefer-Wolfowitz) and root-finding/stochastic gradient-based (Robbins-Monro) settings, and is based on the "simultaneous perturbation (SP)" idea introduced previously. The algorithm requires only a small number of loss function or gradient measurements per iteration-independent of the problem dimension-to adaptively estimate the Hessian and parameters of primary interest. Aside from introducing the adaptive SP approach, the paper presents practical implementation guidance, asymptotic theory, and a nontrivial numerical evaluation. Also included is a discussion and numerical analysis comparing the adaptive SP approach with the iterate-averaging approach to accelerated SA.},
  eventtitle = {{{IEEE Transactions}} on {{Automatic Control}}},
  keywords = {Acceleration,Adaptive control,Backpropagation algorithms,Constraint optimization,Jacobian matrices,Least squares approximation,Noise measurement,Parameter estimation,Perturbation methods,Stochastic processes,todo},
  number = {10}
}

@book{spall2003,
  title = {Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control},
  shorttitle = {Introduction to Stochastic Search and Optimization},
  author = {Spall, James C.},
  date = {2003-03-26},
  edition = {1. edizione},
  publisher = {{Wiley-Interscience}},
  location = {{Hoboken, N.J}},
  abstract = {* Unique in its survey of the range of topics. * Contains a strong, interdisciplinary format that will appeal to both students and researchers. * Features exercises and web links to software and data sets.},
  isbn = {978-0-471-33052-3},
  langid = {Inglese},
  pagetotal = {595}
}

@incollection{sparapani2019,
  title = {Bayesian {{Additive Regression Trees}} ({{BART}})},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Sparapani, Rodney and McCulloch, Robert},
  date = {2019},
  pages = {1--9},
  publisher = {{American Cancer Society}},
  doi = {10.1002/9781118445112.stat08251},
  abstract = {This article introduces the nonparametric machine learning technique known as Bayesian additive regression trees (BART) for continuous, dichotomous, categorical, and time-to-event outcomes.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat08251},
  isbn = {978-1-118-44511-2},
  keywords = {black-box models,categorical outcomes,competing risks,continuous outcomes,dichotomous outcomes,ensemble predictive modeling,machine learning,nonparametric,recurrent events,survival analysis,todo},
  langid = {english}
}

@inproceedings{srivastava2015,
  title = {{{WASP}}: {{Scalable Bayes}} via Barycenters of Subset Posteriors},
  shorttitle = {{{WASP}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Srivastava, Sanvesh and Cevher, Volkan and Dinh, Quoc and Dunson, David},
  date = {2015-02-21},
  pages = {912--920},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {The promise of Bayesian methods for big data sets has not fully been realized due to the lack of scalable computational algorithms. For massive data, it is necessary to store and process subsets on...},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  keywords = {done},
  langid = {english}
}

@article{srivastava2018,
  title = {Scalable {{Bayes}} via {{Barycenter}} in {{Wasserstein Space}}},
  author = {Srivastava, Sanvesh and Li, Cheng and Dunson, David},
  date = {2018},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {Journal of Machine Learning Research},
  volume = {19},
  abstract = {We propose a novel approach WASP for Bayesian inference when massive size of the data prohibits posterior computations. WASP is estimated in three steps. First, data are divided into smaller computationally tractable subsets. Second, posterior draws of parameters are obtained for every subset after modifying subset posteriors using stochastic approximation. Finally, the empirical measures of samples from each subset posterior are combined through their barycenter in the Wasserstein space of probability measures. Stochastic approximation ensures that posterior uncertainty quantification of the barycenter matches with that of the full data posterior distribution. The combining step can be conducted efficiently through a sparse linear program, which takes negligible time relative to sampling from subset posteriors, facilitating scaling to massive data. WASP is very general and allows application of existing sampling algorithms to massive data with minimal modifications. We provide theoretical conditions under which rate of convergence of WASP to the delta measure centered at the true parameter coincides with the optimal parametric rate up to a logarithmic factor. WASP is applied for scalable Bayesian computations in a nonparametric mixture model and a movie recommender database containing tens of millions of ratings.},
  keywords = {done}
}

@book{steele2004,
  title = {The Cauchy-Schwarz Master Class},
  author = {Steele, J. Michael},
  date = {2004-07-15},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge ; New York}},
  abstract = {This lively, problem-oriented text, first published in 2004, is designed to coach readers toward mastery of the most fundamental mathematical inequalities. With the Cauchy-Schwarz inequality as the initial guide, the reader is led through a sequence of fascinating problems whose solutions are presented as they might have been discovered - either by one of history's famous mathematicians or by the reader. The problems emphasize beauty and surprise, but along the way readers will find systematic coverage of the geometry of squares, convexity, the ladder of power means, majorization, Schur convexity, exponential sums, and the inequalities of Hölder, Hilbert, and Hardy. The text is accessible to anyone who knows calculus and who cares about solving problems. It is well suited to self-study, directed study, or as a supplement to courses in analysis, probability, and combinatorics.},
  isbn = {978-0-521-54677-5},
  langid = {Inglese},
  pagetotal = {316}
}

@article{stefanucci2021,
  title = {Multiscale Stick-Breaking Mixture Models},
  author = {Stefanucci, Marco and Canale, Antonio},
  date = {2021-01-21},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {31},
  pages = {13},
  issn = {1573-1375},
  doi = {10.1007/s11222-020-09991-1},
  abstract = {Bayesian nonparametric density estimation is dominated by single-scale methods, typically exploiting mixture model specifications, exception made for Pólya trees prior and allied approaches. In this paper we focus on developing a novel family of multiscale stick-breaking mixture models that inherits some of the advantages of both single-scale nonparametric mixtures and Pólya trees. Our proposal is based on a mixture specification exploiting an infinitely deep binary tree of random weights that grows according to a multiscale generalization of a large class of stick-breaking processes; this multiscale stick-breaking is paired with specific stochastic processes generating sequences of parameters that induce stochastically ordered kernel functions. Properties of this family of multiscale stick-breaking mixtures are described. Focusing on a Gaussian specification, a Markov Chain Monte Carlo algorithm for posterior computation is introduced. The performance of the method is illustrated analyzing both synthetic and real datasets consistently showing competitive results both in scenarios favoring single-scale and multiscale methods. The results suggest that the method is well suited to estimate densities with varying degree of smoothness and local features.},
  keywords = {done},
  langid = {english},
  number = {2}
}

@article{steigleder2003,
  title = {Generalized {{Stratified Sampling Using}} the {{Hilbert Curve}}},
  author = {Steigleder, Mauro and McCool, Michael D.},
  date = {2003-01-01},
  journaltitle = {Journal of Graphics Tools},
  volume = {8},
  pages = {41--47},
  publisher = {{Taylor \& Francis}},
  issn = {1086-7651},
  doi = {10.1080/10867651.2003.10487589},
  abstract = {Stratified sampling is a widely used strategy to improve convergence in Monte Carlo techniques. The efficiency of a stratification technique mainly depends on the coherence of the strata. This paper presents an approach to generate an arbitrary number of coherent strata, independently of the dimensionality of the domain, using the Hilbert space-filling curve. Using this approach, it is possible to draw an arbitrary number of stratified samples from higher dimensional spaces using only one-dimensional stratification. This technique can also be used to generalize nonuniform stratified sampling. Source code is available online.},
  annotation = {\_eprint: https://doi.org/10.1080/10867651.2003.10487589},
  keywords = {done},
  number = {3}
}

@article{steiner2000,
  title = {Monitoring Surgical Performance Using Risk-Adjusted Cumulative Sum Charts},
  author = {Steiner, Stefan H. and Cook, Richard J. and Farewell, Vern T. and Treasure, Tom},
  date = {2000-12-01},
  journaltitle = {Biostatistics},
  shortjournal = {Biostatistics},
  volume = {1},
  pages = {441--452},
  publisher = {{Oxford Academic}},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/1.4.441},
  abstract = {Abstract. The cumulative sum (CUSUM) procedure is a graphical method that is widely used for quality monitoring in industrial settings. More recently it has bee},
  keywords = {done},
  langid = {english},
  number = {4}
}

@article{stetco2015,
  title = {Fuzzy {{C}}-Means++: {{Fuzzy C}}-Means with Effective Seeding Initialization},
  shorttitle = {Fuzzy {{C}}-Means++},
  author = {Stetco, Adrian and Zeng, Xiao-Jun and Keane, John},
  date = {2015},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {42},
  pages = {7541--7548},
  issn = {0957-4174},
  abstract = {Fuzzy C-means has been utilized successfully in a wide range of applications, extending the clustering capability of the K-means to datasets that are uncertain, vague and otherwise hard to cluster. This paper introduces the Fuzzy C-means++ algorithm which, by utilizing the seeding mechanism of the K-means++ algorithm, improves the effectiveness and speed of Fuzzy C-means. By careful seeding that disperses the initial cluster centers through the data space, the resulting Fuzzy C-means++ approach samples starting cluster representatives during the initialization phase. The cluster representatives are well spread in the input space, resulting in both faster convergence times and higher quality solutions. Implementations in R of standard Fuzzy C-means and Fuzzy C-means++ are evaluated on various data sets. We investigate the cluster quality and iteration count as we vary the spreading factor on a series of synthetic data sets. We run the algorithm on real world data sets and to account for the non-determinism inherent in these algorithms we record multiple runs while choosing different k parameter values. The results show that the proposed method gives significant improvement in convergence times (the number of iterations) of up to 40 (2.1 on average) times the standard on synthetic datasets and, in general, an associated lower cost function value and Xie–Beni value. A proof sketch of the logarithmically bounded expected cost function value is given.},
  keywords = {Cluster analysis,done,Fuzzy C-means clustering,Initialization},
  number = {21}
}

@book{stoker1989,
  title = {Differential {{Geometry}}},
  author = {Stoker, J. J.},
  date = {1989-01-18},
  edition = {1 edition},
  publisher = {{John Wiley \& Sons}},
  location = {{New York}},
  abstract = {This classic work is now available in an unabridged paperback edition. Stoker makes this fertile branch of mathematics accessible to the nonspecialist by the use of three different notations: vector algebra and calculus, tensor calculus, and the notation devised by Cartan, which employs invariant differential forms as elements in an algebra due to Grassman, combined with an operation called exterior differentiation. Assumed are a passing acquaintance with linear algebra and the basic elements of analysis.},
  isbn = {978-0-471-50403-0},
  langid = {english},
  pagetotal = {432}
}

@online{sukhbaatar2015,
  title = {End-{{To}}-{{End Memory Networks}}},
  author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
  date = {2015-11-24},
  abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
  archiveprefix = {arXiv},
  eprint = {1503.08895},
  eprinttype = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,todo},
  primaryclass = {cs}
}

@book{sutherland2010,
  title = {Introduction to Metric and Topological Spaces},
  author = {Sutherland, Wilson A.},
  date = {2010-09-01},
  edition = {2 edizione},
  publisher = {{OUP Oxford}},
  location = {{Oxford}},
  abstract = {One of the ways in which topology has influenced other branches of mathematics in the past few decades is by putting the study of continuity and convergence into a general setting. This new edition of Wilson Sutherland's classic text introduces metric and topological spaces by describing some of that influence. The aim is to move gradually from familiar real analysis to abstract topological spaces, using metric spaces as a bridge between the two. The language of metric and topological spaces is established with continuity as the motivating concept. Several concepts are introduced, first in metric spaces and then repeated for topological spaces, to help convey familiarity. The discussion develops to cover connectedness, compactness and completeness, a trio widely used in the rest of mathematics. Topology also has a more geometric aspect which is familiar in popular expositions of the subject as `rubber-sheet geometry', with pictures of Möbius bands, doughnuts, Klein bottles and the like; this geometric aspect is illustrated by describing some standard surfaces, and it is shown how all this fits into the same story as the more analytic developments. The book is primarily aimed at second- or third-year mathematics students. There are numerous exercises, many of the more challenging ones accompanied by hints, as well as a companion website, with further explanations and examples as well as material supplementary to that in the book.},
  isbn = {978-0-19-956308-1},
  langid = {Inglese},
  pagetotal = {224}
}

@article{szucs2017,
  title = {Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  date = {2017-03-02},
  journaltitle = {PLOS Biology},
  shortjournal = {PLOS Biology},
  volume = {15},
  pages = {e2000797},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2000797},
  abstract = {We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64–1.46) for nominally statistically significant results and D = 0.24 (0.11–0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50\% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.},
  keywords = {Behavioral neuroscience,Cognitive neuroscience,Cognitive psychology,Experimental psychology,Medical journals,Scientific publishing,Statistical data,Statistical distributions,todo},
  langid = {english},
  number = {3}
}

@book{talluri2004,
  title = {The Theory and Practice of Revenue Management},
  author = {Talluri, Kalyan T. and Ryzin, Garrett Van},
  date = {2004-06-01},
  edition = {2004° edizione},
  publisher = {{Kluwer Academic Pub}},
  location = {{Boston, Mass}},
  abstract = {Revenue management (RM) has emerged as one of the most important new business practices in recent times. This book is the first comprehensive reference book to be published in the field of RM. It unifies the field, drawing from industry sources as well as relevant research from disparate disciplines, as well as documenting industry practices and implementation details.Successful hardcover version published in April 2004.},
  isbn = {978-1-4020-7933-7},
  langid = {Inglese},
  pagetotal = {712}
}

@book{tan2005,
  title = {Introduction to Data Mining},
  author = {Tan, Pang-Ning and Steinbach, Michael and Kumar, Vipin},
  date = {2005},
  publisher = {{Pearson College Div}},
  location = {{Boston}},
  abstract = {Introduction to Data Mining presents fundamental concepts and algorithms for those learning data mining for the first time. Each major topic is organized into two chapters, beginning with basic concepts that provide necessary background for understanding each data mining technique, followed by more advanced concepts and algorithms.},
  isbn = {978-0-321-32136-7},
  langid = {Inglese},
  pagetotal = {769}
}

@article{tang2015,
  title = {Risk-{{Adjusted Cumulative Sum Charting Procedure Based}} on {{Multiresponses}}},
  author = {Tang, Xu and Gan, Fah F. and Zhang, Lingyun},
  date = {2015-01-02},
  journaltitle = {Journal of the American Statistical Association},
  volume = {110},
  pages = {16--26},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2014.960965},
  abstract = {The cumulative sum charting procedure is traditionally used in the manufacturing industry for monitoring the quality of products. Recently, it has been extended to monitoring surgical outcomes. Unlike a manufacturing process where the raw material is usually reasonably homogeneous, patients’ risks of surgical failure are usually different. It has been proposed in the literature that the binary outcomes from a surgical procedure be adjusted using the preoperative risk based on a likelihood-ratio scoring method. Such a crude classification of surgical outcome is naive. It is unreasonable to regard a patient who has a full recovery, the same quality outcome as another patient who survived but remained bed-ridden for life. For a patient who survives an operation, there can be many different grades of recovery. Thus, it makes sense to consider a risk-adjusted cumulative sum charting procedure based on more than two outcomes to better monitor surgical performance. In this article, we develop such a chart and study its performance.},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2014.960965},
  keywords = {Collocation method,done,Euroscores,Odds ratio,Parsonnet scores,Patient mix,Proportional odds logistic regression,Quality monitoring,Surgical outcomes},
  number = {509}
}

@article{tang2018,
  title = {Development of an {{Immune}}-{{Pathology Informed Radiomics Model}} for {{Non}}-{{Small Cell Lung Cancer}}},
  author = {Tang, Chad and Hobbs, Brian and Amer, Ahmed and Li, Xiao and Behrens, Carmen and Canales, Jaime Rodriguez and Cuentas, Edwin Parra and Villalobos, Pamela and Fried, David and Chang, Joe Y. and Hong, David S. and Welsh, James W. and Sepesi, Boris and Court, Laurence and Wistuba, Ignacio I. and Koay, Eugene J.},
  date = {2018-01-31},
  journaltitle = {Scientific Reports},
  volume = {8},
  pages = {1922},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-20471-5},
  abstract = {With increasing use of immunotherapy agents, pretreatment strategies for identifying responders and non-responders is useful for appropriate treatment assignment. We hypothesize that the local immune micro-environment of NSCLC is associated with patient outcomes and that these local immune features exhibit distinct radiologic characteristics discernible by quantitative imaging metrics. We assembled two cohorts of NSCLC patients treated with definitive surgical resection and extracted quantitative parameters from pretreatment CT imaging. The excised primary tumors were then quantified for percent tumor PDL1 expression and density of tumor-infiltrating lymphocyte (via CD3 count) utilizing immunohistochemistry and automated cell counting. Associating these pretreatment radiomics parameters with tumor immune parameters, we developed an immune pathology-informed model (IPIM) that separated patients into 4 clusters (designated A-D) utilizing 4 radiomics features. The IPIM designation was significantly associated with overall survival in both training (5 year OS: 61\%, 41\%, 50\%, and 91\%, for clusters A-D, respectively, P\,=\,0.04) and validation (5 year OS: 55\%, 72\%, 75\%, and 86\%, for clusters A-D, respectively, P\,=\,0.002) cohorts and immune pathology (all P\,{$<$}\,0.05). Specifically, we identified a favorable outcome group characterized by low CT intensity and high heterogeneity that exhibited low PDL1 and high CD3 infiltration, suggestive of a favorable immune activated state. We have developed a NSCLC radiomics signature based on the immune micro-environment and patient outcomes. This manuscript demonstrates model creation and validation in independent cohorts.},
  issue = {1},
  keywords = {done},
  langid = {english},
  number = {1}
}

@incollection{teh2010,
  title = {Dirichlet {{Process}}},
  booktitle = {Encyclopedia of {{Machine Learning}}},
  author = {Teh, Yee Whye},
  editor = {Sammut, Claude and Webb, Geoffrey I.},
  date = {2010},
  pages = {280--287},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-0-387-30164-8_219},
  isbn = {978-0-387-30164-8},
  langid = {english}
}

@article{tibshirani1996,
  title = {Regression {{Shrinkage}} and {{Selection}} via the {{Lasso}}},
  author = {Tibshirani, Robert},
  date = {1996},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {58},
  pages = {267--288},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
  eprint = {2346178},
  eprinttype = {jstor},
  keywords = {Algorithms,done,Error rates,Estimators,Least squares,Linear models,Regression coefficients,Simulations,Standard error},
  number = {1}
}

@article{tibshirani2011,
  title = {Regression Shrinkage and Selection via the Lasso: A Retrospective},
  shorttitle = {Regression Shrinkage and Selection via the Lasso},
  author = {Tibshirani, Robert},
  date = {2011},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {73},
  pages = {273--282},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {1369-7412},
  abstract = {In the paper I give a brief review of the basic idea and some history and then discuss some developments since the original paper on regression shrinkage and selection via the lasso.},
  eprint = {41262671},
  eprinttype = {jstor},
  keywords = {A posteriori knowledge,Algorithms,Approximation,Computational statistics,Coordinate systems,done,Estimators,Generalized linear model,Oracles},
  number = {3}
}

@online{tomasetti2019,
  title = {Updating {{Variational Bayes}}: {{Fast}} Sequential Posterior Inference},
  shorttitle = {Updating {{Variational Bayes}}},
  author = {Tomasetti, Nathaniel and Forbes, Catherine S. and Panagiotelis, Anastasios},
  date = {2019-08-01},
  abstract = {Variational Bayesian (VB) methods produce posterior inference in a time frame considerably smaller than traditional Markov Chain Monte Carlo approaches. Although the VB posterior is an approximation, it has been shown to produce good parameter estimates and predicted values when a rich classes of approximating distributions are considered. In this paper we propose Updating VB (UVB), a recursive algorithm used to update a sequence of VB posterior approximations in an online setting, with the computation of each posterior update requiring only the data observed since the previous update. An extension to the proposed algorithm, named UVB-IS, allows the user to trade accuracy for a substantial increase in computational speed through the use of importance sampling. The two methods and their properties are detailed in two separate simulation studies. Two empirical illustrations of the proposed UVB methods are provided, including one where a Dirichlet Process Mixture model with a novel posterior dependence structure is repeatedly updated in the context of predicting the future behaviour of vehicles on a stretch of the US Highway 101.},
  archiveprefix = {arXiv},
  eprint = {1908.00225},
  eprinttype = {arxiv},
  keywords = {62-04,Computer Science - Machine Learning,Statistics - Computation,todo},
  primaryclass = {cs, stat}
}

@book{trefethen1997,
  title = {Numerical Linear Algebra},
  author = {Trefethen, Lloyd N. and Bau, David},
  date = {1997-06-01},
  edition = {New edizione},
  publisher = {{Society for Industrial and Applied Mathematics}},
  location = {{Philadelphia}},
  abstract = {This is a concise, insightful introduction to the field of numerical linear algebra. The clarity and eloquence of the presentation make it popular with teachers and students alike. The text aims to expand the reader's view of the field and to present standard material in a novel way. All of the most important topics in the field are covered with a fresh perspective, including iterative methods for systems of equations and eigenvalue problems and the underlying principles of conditioning and stability. Presentation is in the form of 40 lectures, which each focus on one or two central ideas. The unity between topics is emphasized throughout, with no risk of getting lost in details and technicalities. The book breaks with tradition by beginning with the QR factorization - an important and fresh idea for students, and the thread that connects most of the algorithms of numerical linear algebra.},
  isbn = {978-0-89871-361-9},
  langid = {Inglese},
  pagetotal = {184}
}

@incollection{trivedi2014,
  title = {Markov {{Modeling}} in {{Reliability}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Trivedi, Kishor S. and Selvamuthu, Dharmaraja},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2014-09-29},
  pages = {stat03635},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat03635},
  isbn = {978-1-118-44511-2},
  keywords = {todo},
  langid = {english}
}

@book{tsay2013,
  title = {Multivariate Time Series Analysis: With R and Financial Applications},
  shorttitle = {Multivariate Time Series Analysis},
  author = {Tsay, Ruey S.},
  date = {2013-11-29},
  edition = {1. edizione},
  publisher = {{John Wiley \& Sons Inc}},
  location = {{Hoboken, New Jersey}},
  abstract = {An accessible guide to the multivariate time series tools used in numerous real-world applications Multivariate Time Series Analysis: With R and Financial Applications is the much anticipated sequel coming from one of the most influential and prominent experts on the topic of time series. Through a fundamental balance of theory and methodology, the book supplies readers with a comprehensible approach to financial econometric models and their applications to real-world empirical research. Differing from the traditional approach to multivariate time series, the book focuses on reader comprehension by emphasizing structural specification, which results in simplified parsimonious VAR MA modeling. Multivariate Time Series Analysis: With R and Financial Applications utilizes the freely available R software package to explore complex data and illustrate related computation and analyses. Featuring the techniques and methodology of multivariate linear time series, stationary VAR models, VAR MA time series and models, unitroot process, factor models, and factor-augmented VAR models, the book includes: • Over 300 examples and exercises to reinforce the presented content • User-friendly R subroutines and research presented throughout to demonstrate modern applications • Numerous datasets and subroutines to provide readers with a deeper understanding of the material Multivariate Time Series Analysis is an ideal textbook for graduate-level courses on time series and quantitative finance and upper-undergraduate level statistics courses in time series. The book is also an indispensable reference for researchers and practitioners in business, finance, and econometrics.},
  isbn = {978-1-118-61790-8},
  langid = {Inglese},
  pagetotal = {492}
}

@article{tsiamyrtzis2019,
  title = {Bayesian Statistical Process Control for {{Phase I}} Count Type Data},
  author = {Tsiamyrtzis, Panagiotis and Hawkins, Douglas M.},
  date = {2019-05},
  journaltitle = {Applied Stochastic Models in Business and Industry},
  shortjournal = {Appl Stochastic Models Bus Ind},
  volume = {35},
  pages = {766--787},
  issn = {1524-1904, 1526-4025},
  doi = {10.1002/asmb.2398},
  keywords = {todo},
  langid = {english},
  number = {3}
}

@book{vaart1998,
  title = {Asymptotic {{Statistics}}},
  author = {van der Vaart, A. W.},
  date = {1998},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9780511802256},
  abstract = {This book is an introduction to the field of asymptotic statistics. The treatment is both practical and mathematically rigorous. In addition to most of the standard topics of an asymptotics course, including likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures, the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, which gives the book one of its unifying themes. This entails mainly the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation. Thus, even the standard subjects of asymptotic statistics are presented in a novel way. Suitable as a graduate or Master's level statistics text, this book will also give researchers an overview of research in asymptotic statistics.},
  isbn = {978-0-521-78450-4},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}}
}

@book{vanbuuren2018,
  title = {Flexible {{Imputation}} of {{Missing Data}}, {{Second Edition}}},
  author = {van Buuren, Stef},
  date = {2018-07-16},
  edition = {2nd edition},
  publisher = {{Chapman and Hall/CRC}},
  location = {{Boca Raton}},
  abstract = {Missing data pose challenges to real-life data analysis. Simple ad-hoc fixes, like deletion or mean imputation, only work under highly restrictive conditions, which are often not met in practice. Multiple imputation replaces each missing value by multiple plausible values. The variability between these replacements reflects our ignorance of the true (but missing) value. Each of the completed data set is then analyzed by standard methods, and the results are pooled to obtain unbiased estimates with correct confidence intervals. Multiple imputation is a general approach that also inspires novel solutions to old problems by reformulating the task at hand as a missing-data problem.  This is the second edition of a popular book on multiple imputation, focused on explaining the application of methods through detailed worked examples using the MICE package as developed by the author. This new edition incorporates the recent developments in this fast-moving field. This class-tested book avoids mathematical and technical details as much as possible: formulas are accompanied by verbal statements that explain the formula in accessible terms. The book sharpens the reader’s intuition on how to think about missing data, and provides all the tools needed to execute a well-grounded quantitative analysis in the presence of missing data.},
  isbn = {978-1-138-58831-8},
  langid = {english},
  options = {useprefix=true},
  pagetotal = {444}
}

@online{vanwieringen2020,
  title = {Lecture Notes on Ridge Regression},
  author = {van Wieringen, Wessel N.},
  date = {2020-01-18},
  abstract = {The linear regression model cannot be fitted to high-dimensional data, as the high-dimensionality brings about empirical non-identifiability. Penalized regression overcomes this non-identifiability by augmentation of the loss function by a penalty (i.e. a function of regression coefficients). The ridge penalty is the sum of squared regression coefficients, giving rise to ridge regression. Here many aspect of ridge regression are reviewed e.g. moments, mean squared error, its equivalence to constrained estimation, and its relation to Bayesian regression. Finally, its behaviour and use are illustrated in simulation and on omics data. Subsequently, ridge regression is generalized to allow for a more general penalty. The ridge penalization framework is then translated to logistic regression and its properties are shown to carry over. To contrast ridge penalized estimation, the final chapter introduces its lasso counterpart.},
  archiveprefix = {arXiv},
  eprint = {1509.09169},
  eprinttype = {arxiv},
  keywords = {done,Statistics - Methodology},
  options = {useprefix=true},
  primaryclass = {stat}
}

@online{vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,todo},
  primaryclass = {cs}
}

@article{vehtari2012,
  title = {A Survey of {{Bayesian}} Predictive Methods for Model Assessment, Selection and Comparison},
  author = {Vehtari, Aki and Ojanen, Janne},
  date = {2012-01},
  journaltitle = {Statistics Surveys},
  volume = {6},
  pages = {142--228},
  publisher = {{Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada}},
  issn = {1935-7516},
  doi = {10.1214/12-SS102},
  abstract = {To date, several methods exist in the statistical literature for model assessment, which purport themselves specifically as Bayesian predictive methods. The decision theoretic assumptions on which these methods are based are not always clearly stated in the original articles, however. The aim of this survey is to provide a unified review of Bayesian predictive model assessment and selection methods, and of methods closely related to them. We review the various assumptions that are made in this context and discuss the connections between different approaches, with an emphasis on how each method approximates the expected utility of using a Bayesian model for the purpose of predicting future data.},
  issue = {none},
  keywords = {Bayesian,cross-validation,decision theory,done,Expected utility,information criteria,Model selection,predictive}
}

@online{vehtari2016,
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  date = {2016-09-12},
  doi = {10.1007/s11222-016-9696-4},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.},
  archiveprefix = {arXiv},
  eprint = {1507.04544},
  eprinttype = {arxiv},
  keywords = {done,Statistics - Computation,Statistics - Methodology},
  primaryclass = {stat}
}

@online{vehtari2019,
  title = {Expectation Propagation as a Way of Life: {{A}} Framework for {{Bayesian}} Inference on Partitioned Data},
  shorttitle = {Expectation Propagation as a Way of Life},
  author = {Vehtari, Aki and Gelman, Andrew and Sivula, Tuomas and Jylänki, Pasi and Tran, Dustin and Sahai, Swupnil and Blomstedt, Paul and Cunningham, John P. and Schiminovich, David and Robert, Christian},
  date = {2019-11-30},
  abstract = {A common divide-and-conquer approach for Bayesian computation with big data is to partition the data, perform local inference for each piece separately, and combine the results to obtain a global posterior approximation. While being conceptually and computationally appealing, this method involves the problematic need to also split the prior for the local inferences; these weakened priors may not provide enough regularization for each separate computation, thus eliminating one of the key advantages of Bayesian methods. To resolve this dilemma while still retaining the generalizability of the underlying local inference method, we apply the idea of expectation propagation (EP) as a framework for distributed Bayesian inference. The central idea is to iteratively update approximations to the local likelihoods given the state of the other approximations and the prior. The present paper has two roles: we review the steps that are needed to keep EP algorithms numerically stable, and we suggest a general approach, inspired by EP, for approaching data partitioning problems in a way that achieves the computational benefits of parallelism while allowing each local update to make use of relevant information from the other sites. In addition, we demonstrate how the method can be applied in a hierarchical context to make use of partitioning of both data and parameters. The paper describes a general algorithmic framework, rather than a specific algorithm, and presents an example implementation for it.},
  archiveprefix = {arXiv},
  eprint = {1412.4869},
  eprinttype = {arxiv},
  keywords = {doing,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  primaryclass = {stat}
}

@online{vehtari2020,
  title = {Rank-Normalization, Folding, and Localization: {{An}} Improved \$\textbackslash widehat\{\vphantom\}{{R}}\vphantom\{\}\$ for Assessing Convergence of {{MCMC}}},
  shorttitle = {Rank-Normalization, Folding, and Localization},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and Bürkner, Paul-Christian},
  date = {2020-01-16},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic \$\textbackslash widehat\{R\}\$ of Gelman and Rubin (1992) has serious flaws. Traditional \$\textbackslash widehat\{R\}\$ will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
  archiveprefix = {arXiv},
  eprint = {1903.08008},
  eprinttype = {arxiv},
  keywords = {done,Markov Chain mixing,MCMC,posterior convergence,Statistics - Computation,Statistics - Methodology},
  primaryclass = {stat}
}

@online{vehtari2021,
  title = {Pareto {{Smoothed Importance Sampling}}},
  author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
  date = {2021-02-23},
  abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be noisy when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates and convergence diagnostics.},
  archiveprefix = {arXiv},
  eprint = {1507.02646},
  eprinttype = {arxiv},
  keywords = {done,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  primaryclass = {stat}
}

@book{venables2004,
  title = {Modern {{Applied Statistics}} with {{S}}},
  author = {Venables, W. N. and Ripley, B. D.},
  date = {2004},
  edition = {4},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  doi = {10.1007/978-0-387-21706-2},
  abstract = {S is a powerful environment for the statistical and graphical analysis of data. It provides the tools to implement many statistical ideas that have been made possible by the widespread availability of workstations having good graphics and computational capabilities. This book is a guide to using S environments to perform statistical analyses and provides both an introduction to the use of S and a course in modern statistical methods. Implementations of S are available commercially in S-PLUS(R) workstations and as the Open Source R for a wide range of computer systems. The aim of this book is to show how to use S as a powerful and graphical data analysis system. Readers are assumed to have a basic grounding in statistics, and so the book is intended for would-be users of S-PLUS or R and both students and researchers using statistics. Throughout, the emphasis is on presenting practical problems and full analyses of real data sets. Many of the methods discussed are state of the art approaches to topics such as linear, nonlinear and smooth regression models, tree-based methods, multivariate analysis, pattern recognition, survival analysis, time series and spatial statistics. Throughout modern techniques such as robust methods, non-parametric smoothing and bootstrapping are used where appropriate. This fourth edition is intended for users of S-PLUS 6.0 or R 1.5.0 or later. A substantial change from the third edition is updating for the current versions of S-PLUS and adding coverage of R. The introductory material has been rewritten to emphasis the import, export and manipulation of data. Increased computational power allows even more computer-intensive methods to be used, and methods such as GLMMs,},
  isbn = {978-0-387-95457-8},
  langid = {english},
  series = {Statistics and {{Computing}}}
}

@incollection{vermunt2014,
  title = {Structural {{Equation Modeling}}: {{Mixture Models}}},
  shorttitle = {Structural {{Equation Modeling}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Vermunt, Jeroen K. and Magidson, Jay},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2014-09-29},
  pages = {stat06478},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat06478},
  isbn = {978-1-118-44511-2},
  keywords = {todo},
  langid = {english}
}

@book{villani2008,
  title = {Optimal {{Transport}}: {{Old}} and {{New}}},
  shorttitle = {Optimal {{Transport}}},
  author = {Villani, Cédric},
  date = {2008-09-30},
  edition = {2009th edition},
  publisher = {{Springer}},
  location = {{Aalborg}},
  abstract = {At the close of the 1980s, the independent contributions of Yann Brenier, Mike Cullen and John Mather launched a revolution in the venerable field of optimal transport founded by G. Monge in the 18th century, which has made breathtaking forays into various other domains of mathematics ever since. The author presents a broad overview of this area, supplying complete and self-contained proofs of all the fundamental results of the theory of optimal transport at the appropriate level of generality. Thus, the book encompasses the broad spectrum ranging from basic theory to the most recent research results.   PhD students or researchers can read the entire book without any prior knowledge of the field. A comprehensive bibliography with notes that extensively discuss the existing literature underlines the book’s value as a most welcome reference text on this subject.},
  isbn = {978-87-93102-13-2},
  langid = {english},
  pagetotal = {998}
}

@book{vinci2019,
  title = {Trattato della pittura},
  author = {da Vinci, Leonardo and Papa, Rodolfo},
  date = {2019-01-09},
  edition = {Unabridged edizione},
  publisher = {{Demetra}},
  location = {{Firenze}},
  abstract = {Leonardo afferma che la pittura ha il primato su tutte le arti ed è la guida di tutte le discipline; assimila la pittura alla filosofia e la pone al vertice dell'architettonica dei saperi, ovvero della struttura enciclopedica della conoscenza. Leonardo è contemporaneamente l'ultimo dei medioevali e il primo dei moderni, infatti mantiene la struttura dei saperi con le loro relazioni interne in continuità con la tradizione antica e medioevale, ma ne sovverte la gerarchia inventando una nuova dimensione scientifica: la scienza della pittura. Leonardo rivendica per la pittura un posto nobile tra le arti liberali, al pari della filosofia.},
  isbn = {978-88-440-5354-3},
  langid = {Italiano},
  pagetotal = {544}
}

@incollection{voncollani2014,
  title = {Acceptance {{Sampling}} in {{Modern Industrial Environments}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Von Collani, Elart and Göb, Rainer},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2014-09-29},
  pages = {stat04217},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat04217},
  isbn = {978-1-118-44511-2},
  keywords = {todo},
  langid = {english}
}

@article{vonhippel2009,
  title = {How to {{Impute Interactions}}, {{Squares}}, and {{Other Transformed Variables}}},
  author = {von Hippel, Paul T.},
  date = {2009},
  journaltitle = {Sociological Methodology},
  volume = {39},
  pages = {265--291},
  issn = {1467-9531},
  doi = {10.1111/j.1467-9531.2009.01215.x},
  abstract = {Researchers often carry out regression analysis using data that have missing values. Missing values can be filled in using multiple imputation, but imputation is tricky if the regression includes interactions, squares, or other transformations of the regressors. In this paper, we examine different approaches to imputing transformed variables; and we find one simple method that works well across a variety of circumstances. Our recommendation is to transform, then impute—i.e., calculate the interactions or squares in the incomplete data and then impute these transformations like any other variable. The transform-then-impute method yields good regression estimates, even though the imputed values are often inconsistent with one another. It is tempting to try and “fix” the inconsistencies in the imputed values, but methods that do so lead to biased regression estimates. Such biased methods include the passive imputation strategy implemented by the popular ice command for Stata.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9531.2009.01215.x},
  keywords = {todo},
  langid = {english},
  number = {1},
  options = {useprefix=true}
}

@incollection{wackernagel2014,
  title = {Multivariate {{Kriging}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Wackernagel, Hans},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  date = {2014-09-29},
  pages = {stat07732},
  publisher = {{John Wiley \& Sons, Ltd}},
  location = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat07732},
  isbn = {978-1-118-44511-2},
  keywords = {todo},
  langid = {english}
}

@article{walker2007,
  title = {Sampling the {{Dirichlet Mixture Model}} with {{Slices}}},
  author = {Walker, Stephen G.},
  date = {2007-01-30},
  journaltitle = {Communications in Statistics - Simulation and Computation},
  volume = {36},
  pages = {45--54},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610910601096262},
  abstract = {We provide a new approach to the sampling of the well known mixture of Dirichlet process model. Recent attention has focused on retention of the random distribution function in the model, but sampling algorithms have then suffered from the countably infinite representation these distributions have. The key to the algorithm detailed in this article, which also keeps the random distribution functions, is the introduction of a latent variable which allows a finite number, which is known, of objects to be sampled within each iteration of a Gibbs sampler.},
  annotation = {\_eprint: https://doi.org/10.1080/03610910601096262},
  keywords = {Bayesian nonparametrics,Density estimation,Dirichlet process,Gibbs sampler,Primary 62G99; 62F15,Secondary 65C60,Slice sampling,todo},
  number = {1}
}

@inproceedings{wang2016,
  title = {Scalable Geometric Density Estimation},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Wang, Ye and Canale, Antonio and Dunson, David},
  date = {2016-05-02},
  pages = {857--865},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {It is standard to assume a low-dimensional structure in estimating a high-dimensional density.  However, popular methods, such as probabilistic principal component analysis, scale poorly computatio...},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  keywords = {done},
  langid = {english}
}

@article{wang2016a,
  title = {Functional {{Data Analysis}}},
  author = {Wang, Jane-Ling and Chiou, Jeng-Min and Müller, Hans-Georg},
  date = {2016-06},
  journaltitle = {Annual Review of Statistics and Its Application},
  shortjournal = {Annu. Rev. Stat. Appl.},
  volume = {3},
  pages = {257--295},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-041715-033624},
  keywords = {todo},
  langid = {english},
  number = {1}
}

@book{welsh1996,
  title = {Aspects of {{Statistical Inference}}},
  author = {Welsh, A. H.},
  date = {1996-10-10},
  publisher = {{John Wiley \& Sons}},
  abstract = {Relevant, concrete, and thorough--the essential data-based text onstatistical inference  The ability to formulate abstract concepts and draw conclusionsfrom data is fundamental to mastering statistics. Aspects ofStatistical Inference equips advanced undergraduate and graduatestudents with a comprehensive grounding in statistical inference,including nonstandard topics such as robustness, randomization, andfinite population inference.  A. H. Welsh goes beyond the standard texts and expertly synthesizesbroad, critical theory with concrete data and relevant topics. Thetext follows a historical framework, uses real-data sets andstatistical graphics, and treats multiparameter problems, yet isultimately about the concepts themselves.  Written with clarity and depth, Aspects of Statistical Inference: * Provides a theoretical and historical grounding in statisticalinference that considers Bayesian, fiducial, likelihood, andfrequentist approaches * Illustrates methods with real-data sets on diabetic retinopathy,the pharmacological effects of caffeine, stellar velocity, andindustrial experiments * Considers multiparameter problems * Develops large sample approximations and shows how to use them * Presents the philosophy and application of robustness theory * Highlights the central role of randomization in statistics * Uses simple proofs to illuminate foundational concepts * Contains an appendix of useful facts concerning expansions,matrices, integrals, and distribution theory  Here is the ultimate data-based text for comparing and presentingthe latest approaches to statistical inference.},
  eprint = {4BFq21QpDnQC},
  eprinttype = {googlebooks},
  isbn = {978-0-471-11591-5},
  keywords = {Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes},
  langid = {english},
  pagetotal = {498}
}

@book{west1997,
  title = {Bayesian {{Forecasting}} and {{Dynamic Models}}},
  author = {West, Mike and Harrison, Jeff},
  date = {1997},
  edition = {2},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  doi = {10.1007/b98971},
  abstract = {This text is concerned with Bayesian learning, inference and forecasting in dynamic environments. We describe the structure and theory of classes of dynamic models and their uses in forecasting and time series analysis. The principles, models and methods of Bayesian forecasting and time - ries analysis have been developed extensively during the last thirty years. Thisdevelopmenthasinvolvedthoroughinvestigationofmathematicaland statistical aspects of forecasting models and related techniques. With this has come experience with applications in a variety of areas in commercial, industrial, scienti?c, and socio-economic ?elds. Much of the technical - velopment has been driven by the needs of forecasting practitioners and applied researchers. As a result, there now exists a relatively complete statistical and mathematical framework, presented and illustrated here. In writing and revising this book, our primary goals have been to present a reasonably comprehensive view of Bayesian ideas and methods in m- elling and forecasting, particularly to provide a solid reference source for advanced university students and research workers.},
  isbn = {978-0-387-94725-9},
  langid = {english},
  series = {Springer {{Series}} in {{Statistics}}}
}

@online{weston2015,
  title = {Memory {{Networks}}},
  author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  date = {2015-11-29},
  abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
  archiveprefix = {arXiv},
  eprint = {1410.3916},
  eprinttype = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Statistics - Machine Learning,todo},
  primaryclass = {cs, stat}
}

@book{wienke2010,
  title = {Frailty {{Models}} in {{Survival Analysis}}},
  author = {Wienke, Andreas},
  date = {2010},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781420073911},
  abstract = {The concept of frailty offers a convenient way to introduce unobserved heterogeneity and associations into models for survival data. In its simplest form, frailty is an unobserved random proportionality factor that modifies the hazard function of an individual or a group of related individuals. Frailty Models in Survival Analysis presents a comprehensive overview of the fundamental approaches in the area of frailty models. The book extensively explores how univariate frailty models can represent unobserved heterogeneity. It also emphasizes correlated frailty models as extensions of univariate and shared frailty models. The author analyzes similarities and differences between frailty and copula models; discusses problems related to frailty models, such as tests for homogeneity; and describes parametric and semiparametric models using both frequentist and Bayesian approaches. He also shows how to apply the models to real data using the statistical packages of R, SAS, and Stata. The appendix provides the technical mathematical results used throughout. Written in nontechnical terms accessible to nonspecialists, this book explains the basic ideas in frailty modeling and statistical techniques, with a focus on real-world data application and interpretation of the results. By applying several models to the same data, it allows for the comparison of their advantages and limitations under varying model assumptions. The book also employs simulations to analyze the finite sample size performance of the models.},
  isbn = {978-1-4200-7388-1},
  keywords = {empirical bayes,frailty,hierarchical,multilevel},
  series = {Chapman \& {{Hall}}/{{CRC Biostatistics Series}}}
}

@article{wilhelm2010,
  title = {Tmvtnorm: {{A Package}} for the {{Truncated Multivariate Normal Distribution}}},
  shorttitle = {Tmvtnorm},
  author = {Wilhelm, Stefan and Manjunath, B.},
  date = {2010-06-01},
  journaltitle = {The R Journal},
  shortjournal = {The R Journal},
  volume = {2},
  doi = {10.32614/RJ-2010-005},
  abstract = {In this article we present tmvtnorm , an R package implementation for the truncated multivariate normal distribution. We consider random number generation with rejection and Gibbs sampling, computation of marginal densi- ties as well as computation of the mean and co- variance of the truncated variables. This contri- bution brings together latest research in this field and provides useful methods for both scholars and practitioners when working with truncated normal variables.},
  keywords = {done}
}

@software{wilhelm2015,
  title = {Tmvtnorm: {{Truncated Multivariate Normal}} and {{Student}} t {{Distribution}}},
  shorttitle = {Tmvtnorm},
  author = {Wilhelm, Stefan and Manjunath, B. G.},
  date = {2015-08-28},
  abstract = {Random number generation for the truncated multivariate normal and Student t distribution. Computes probabilities, quantiles and densities, including one-dimensional and bivariate marginal densities. Computes first and second moments (i.e. mean and covariance matrix) for the double-truncated multinormal case.},
  keywords = {Distributions},
  version = {1.4-10}
}

@book{wilson2015,
  title = {Models for Dependent Time Series},
  author = {Wilson, Granville Tunnicliffe and Reale, Marco and Haywood, John},
  date = {2015-07-16},
  publisher = {{Chapman and Hall/CRC}},
  location = {{Boca Raton}},
  abstract = {Models for Dependent Time Series addresses the issues that arise and the methodology that can be applied when the dependence between time series is described and modeled. Whether you work in the economic, physical, or life sciences, the book shows you how to draw meaningful, applicable, and statistically valid conclusions from multivariate (or vector) time series data.  The first four chapters discuss the two main pillars of the subject that have been developed over the last 60 years: vector autoregressive modeling and multivariate spectral analysis. These chapters provide the foundational material for the remaining chapters, which cover the construction of structural models and the extension of vector autoregressive modeling to high frequency, continuously recorded, and irregularly sampled series. The final chapter combines these approaches with spectral methods for identifying causal dependence between time series. Web Resource A supplementary website provides the data sets used in the examples as well as documented MATLAB® functions and other code for analyzing the examples and producing the illustrations. The site also offers technical details on the estimation theory and methods and the implementation of the models.},
  isbn = {978-1-58488-650-1},
  langid = {Inglese},
  pagetotal = {340}
}

@article{woo2014,
  title = {Cluster-Extent Based Thresholding in {{fMRI}} Analyses: Pitfalls and Recommendations},
  shorttitle = {Cluster-Extent Based Thresholding in {{fMRI}} Analyses},
  author = {Woo, Choong-Wan and Krishnan, Anjali and Wager, Tor D.},
  date = {2014-05-01},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  volume = {91},
  pages = {412--419},
  issn = {1095-9572},
  doi = {10.1016/j.neuroimage.2013.12.058},
  abstract = {Cluster-extent based thresholding is currently the most popular method for multiple comparisons correction of statistical maps in neuroimaging studies, due to its high sensitivity to weak and diffuse signals. However, cluster-extent based thresholding provides low spatial specificity; researchers can only infer that there is signal somewhere within a significant cluster and cannot make inferences about the statistical significance of specific locations within the cluster. This poses a particular problem when one uses a liberal cluster-defining primary threshold (i.e., higher p-values), which often produces large clusters spanning multiple anatomical regions. In such cases, it is impossible to reliably infer which anatomical regions show true effects. From a survey of 814 functional magnetic resonance imaging (fMRI) studies published in 2010 and 2011, we show that the use of liberal primary thresholds (e.g., p{$<$}.01) is endemic, and that the largest determinant of the primary threshold level is the default option in the software used. We illustrate the problems with liberal primary thresholds using an fMRI dataset from our laboratory (N=33), and present simulations demonstrating the detrimental effects of liberal primary thresholds on false positives, localization, and interpretation of fMRI findings. To avoid these pitfalls, we recommend several analysis and reporting procedures, including 1) setting primary p{$<$}.001 as a default lower limit; 2) using more stringent primary thresholds or voxel-wise correction methods for highly powered studies; and 3) adopting reporting practices that make the level of spatial precision transparent to readers. We also suggest alternative and supplementary analysis methods.},
  eprint = {24412399},
  eprinttype = {pmid},
  keywords = {Cluster Analysis,Cluster-extent thresholding,Computer Simulation,Data Interpretation; Statistical,False discovery rate,False Positive Reactions,Family-wise error rate,fMRI,FSL,Gaussian random fields,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Multiple comparisons,Neuroimaging,Normal Distribution,Primary threshold,Signal-To-Noise Ratio,Software,SPM,todo},
  langid = {english},
  pmcid = {PMC4214144}
}

@article{yang2015,
  title = {An {{Arithmetic}}-{{Analytical Expression}} of the {{Hilbert}}-{{Type Space}}-{{Filling Curves}} and {{Its Applications}}},
  author = {Yang, Xiaoling and Tan, Ying},
  date = {2015-04},
  journaltitle = {Bulletin of the Malaysian Mathematical Sciences Society},
  shortjournal = {Bull. Malays. Math. Sci. Soc.},
  volume = {38},
  pages = {841--854},
  issn = {0126-6705, 2180-4206},
  doi = {10.1007/s40840-014-0052-6},
  keywords = {done},
  langid = {english},
  number = {2}
}

@article{yashin1995,
  title = {Correlated Individual Frailty:  An Advantageous Approach to Survival Analysis of Bivariate Data},
  shorttitle = {Correlated Individual Frailty},
  author = {Yashin, A. I. and Vaupel, J. W. and Iachine, I. A.},
  date = {1995},
  journaltitle = {Mathematical Population Studies},
  shortjournal = {Math Popul Stud},
  volume = {5},
  pages = {145--159, 183},
  issn = {0889-8480},
  doi = {10.1080/08898489509525394},
  abstract = {"We develop a new model of bivariate survival based on the notion of correlated individual frailty.  We analyze the properties of this model and suggest a new approach to the analysis of bivariate data that does not require a parametric specification--but permits estimation--of the form of the hazard function for individuals.  We empirically demonstrate the advantages of the model in the statistical analysis of bivariate data."  (SUMMARY IN FRE)},
  eprint = {12290053},
  eprinttype = {pmid},
  keywords = {correlated frailty,Data Analysis,Demographic Factors,Demography,done,Length Of Life,Longevity,Models; Theoretical,Mortality,Population,Population Dynamics,Research,Research Methodology,Statistical Studies,Statistics as Topic,Studies,survival,Survival Rate,Survivorship,World},
  langid = {english},
  number = {2}
}

@book{yee2015,
  title = {Vector Generalized Linear and Additive Models: With an Implementation in {{R}}},
  shorttitle = {Vector Generalized Linear and Additive Models},
  author = {Yee, Thomas W.},
  date = {2015},
  publisher = {{Springer}},
  location = {{New York, NY}},
  annotation = {OCLC: ocn907271683},
  isbn = {978-1-4939-2817-0},
  keywords = {Datenverarbeitung,Hochleistungsrechnen,Linear models (Statistics),Lineares Modell,Regression analysis,Statistisches Modell,Vector spaces,Wahrscheinlichkeitstheorie},
  langid = {english},
  pagetotal = {589},
  series = {Springer Series in Statistics}
}

@book{young2005,
  title = {Essentials of {{Statistical Inference}}},
  author = {Young, G. A. and Smith, R. L.},
  date = {2005-07-25},
  edition = {1 edition},
  publisher = {{Cambridge University Press}},
  abstract = {Aimed at advanced undergraduate and graduate students in mathematics and related disciplines, this book presents the concepts and results underlying the Bayesian, frequentist and Fisherian approaches, with particular emphasis on the contrasts between them. Computational ideas are explained, as well as basic mathematical theory. Written in a lucid and informal style, this concise text provides both basic material on the main approaches to inference, as well as more advanced material on  developments in statistical theory, including: material on Bayesian computation, such as MCMC, higher-order likelihood theory, predictive inference, bootstrap methods and conditional inference. It contains numerous extended examples of the application of formal inference techniques to real data, as well as historical commentary on the development of the subject. Throughout, the text concentrates on concepts, rather than mathematical detail, while maintaining appropriate levels of formality. Each chapter ends with a set of accessible problems.},
  langid = {english},
  pagetotal = {238}
}

@online{yu2020,
  title = {Hyper-{{Parameter Optimization}}: {{A Review}} of {{Algorithms}} and {{Applications}}},
  shorttitle = {Hyper-{{Parameter Optimization}}},
  author = {Yu, Tong and Zhu, Hong},
  date = {2020-03-12},
  abstract = {Since deep neural networks were developed, they have made huge contributions to everyday lives. Machine learning provides more rational advice than humans are capable of in almost every aspect of daily life. However, despite this achievement, the design and training of neural networks are still challenging and unpredictable procedures. To lower the technical thresholds for common users, automated hyper-parameter optimization (HPO) has become a popular topic in both academic and industrial areas. This paper provides a review of the most essential topics on HPO. The first section introduces the key hyper-parameters related to model training and structure, and discusses their importance and methods to define the value range. Then, the research focuses on major optimization algorithms and their applicability, covering their efficiency and accuracy especially for deep learning networks. This study next reviews major services and toolkits for HPO, comparing their support for state-of-the-art searching algorithms, feasibility with major deep learning frameworks, and extensibility for new modules designed by users. The paper concludes with problems that exist when HPO is applied to deep learning, a comparison between optimization algorithms, and prominent approaches for model evaluation with limited computational resources.},
  archiveprefix = {arXiv},
  eprint = {2003.05689},
  eprinttype = {arxiv},
  keywords = {auto-tuning,Computer Science - Machine Learning,deep neural network,Hyper-parameter,Statistics - Machine Learning,todo},
  primaryclass = {cs, stat}
}

@article{zhang2015,
  title = {Dynamic Probability Control Limits for Risk-Adjusted {{CUSUM}} Charts Based on Multiresponses},
  author = {Zhang, Xiang and Woodall, William H.},
  date = {2015},
  journaltitle = {Statistics in Medicine},
  volume = {34},
  pages = {3336--3348},
  issn = {1097-0258},
  doi = {10.1002/sim.6547},
  abstract = {The risk-adjusted Bernoulli cumulative sum (CUSUM) chart developed by Steiner et al. (2000) is an increasingly popular tool for monitoring clinical and surgical performance. In practice, however, the use of a fixed control limit for the chart leads to a quite variable in-control average run length performance for patient populations with different risk score distributions. To overcome this problem, we determine simulation-based dynamic probability control limits (DPCLs) patient-by-patient for the risk-adjusted Bernoulli CUSUM charts. By maintaining the probability of a false alarm at a constant level conditional on no false alarm for previous observations, our risk-adjusted CUSUM charts with DPCLs have consistent in-control performance at the desired level with approximately geometrically distributed run lengths. Our simulation results demonstrate that our method does not rely on any information or assumptions about the patients' risk distributions. The use of DPCLs for risk-adjusted Bernoulli CUSUM charts allows each chart to be designed for the corresponding particular sequence of patients for a surgeon or hospital. Copyright © 2015 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6547},
  keywords = {average run length (ARL),done,false alarm rate,run length distribution,statistical process control,surgical performance},
  langid = {english},
  number = {25}
}

@inproceedings{zhao2009,
  title = {Parallel {{K}}-{{Means Clustering Based}} on {{MapReduce}}},
  booktitle = {Cloud {{Computing}}},
  author = {Zhao, Weizhong and Ma, Huifang and He, Qing},
  date = {2009},
  pages = {674--679},
  abstract = {Data clustering has been received considerable attention in many applications, such as data mining, document retrieval, image segmentation and pattern classification. The enlarging volumes of information emerging by the progress of technology, makes clustering of very large scale of data a challenging task. In order to deal with the problem, many researchers try to design efficient parallel clustering algorithms. In this paper, we propose a parallel k-means clustering algorithm based on MapReduce, which is a simple yet powerful parallel programming technique. The experimental results demonstrate that the proposed algorithm can scale well and efficiently process large datasets on commodity hardware.},
  isbn = {978-3-642-10665-1},
  keywords = {Data mining,done,Hadoop,K-means,MapReduce,Parallel clustering},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@online{zorzetto2020,
  title = {Bayesian Non-Asymptotic Extreme Value Models for Environmental Data},
  author = {Zorzetto, Enrico and Canale, Antonio and Marani, Marco},
  date = {2020-05-25},
  abstract = {Motivated by the analysis of extreme rainfall data, we introduce a general Bayesian hierarchical model for estimating the probability distribution of extreme values of intermittent random sequences, a common problem in geophysical and environmental science settings. The approach presented here relaxes the asymptotic assumption typical of the traditional extreme value (EV) theory, and accounts for the possible underlying variability in the distribution of event magnitudes and occurrences, which are described through a latent temporal process. Focusing on daily rainfall extremes, the structure of the proposed model lends itself to incorporating prior geo-physical understanding of the rainfall process. By means of an extensive simulation study, we show that this methodology can significantly reduce estimation uncertainty with respect to Bayesian formulations of traditional asymptotic EV methods, particularly in the case of relatively small samples. The benefits of the approach are further illustrated with an application to a large data set of 479 long daily rainfall historical records from across the continental United States. By comparing measures of in-sample and out-of-sample predictive accuracy, we find that the model structure developed here, combined with the use of all available observations for inference, significantly improves robustness with respect to overfitting to the specific sample.},
  archiveprefix = {arXiv},
  eprint = {2005.12101},
  eprinttype = {arxiv},
  keywords = {Statistics - Methodology,todo},
  primaryclass = {stat}
}

@article{zou2005,
  title = {Regularization and Variable Selection via the Elastic Net},
  author = {Zou, Hui and Hastie, Trevor},
  date = {2005-04},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  shortjournal = {J Royal Statistical Soc B},
  volume = {67},
  pages = {301--320},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2005.00503.x},
  keywords = {done},
  langid = {english},
  number = {2}
}

@article{zou2008,
  title = {Monitoring {{Profiles Based}} on {{Nonparametric Regression Methods}}},
  author = {Zou, Changliang and Tsung, Fugee and Wang, Zhaojun},
  date = {2008-11-01},
  journaltitle = {Technometrics},
  volume = {50},
  pages = {512--526},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/004017008000000433},
  abstract = {The use statistical process control (SPC) in monitoring and diagnosis of process and product quality profiles remains an important problem in various manufacturing industries. The SPC problem with a nonlinear profile is particularly challenging. This article proposes a novel scheme to monitor changes in both the regression relationship and the variation of the profile online. It integrates the multivariate exponentially weighted moving average procedure with the generalized likelihood ratio test based on nonparametric regression. The proposed scheme not only provides an effective SPC solution to handle nonlinear profiles, which are common in industrial practice, but it also resolves the latent problem in popular parametric monitoring methods of being unable to detect certain types of changes due to a misspecified, out-of-control model. Our simulation results demonstrate the effectiveness and efficiency of the proposed monitoring scheme. In addition, a systematic diagnostic approach is provided to locate the change point of the process and identify the type of change in the profile. Finally, a deep reactive ion-etching example from semiconductor manufacturing is used to illustrate the implementation of the proposed monitoring and diagnostic approach.},
  annotation = {\_eprint: https://doi.org/10.1198/004017008000000433},
  keywords = {done,Exponentially weighted moving average,Generalized likelihood ratio test,Lack-of-fit test,Local linear smoother,Nonlinear profile,Statistical process control},
  number = {4}
}

@article{zou2009,
  title = {On the Adaptive Elastic-Net with a Diverging Number of Parameters},
  author = {Zou, Hui and Zhang, Hao Helen},
  date = {2009-08},
  journaltitle = {The Annals of Statistics},
  volume = {37},
  pages = {1733--1751},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/08-AOS625},
  abstract = {We consider the problem of model selection and estimation in situations where the number of parameters diverges with the sample size. When the dimension is high, an ideal method should have the oracle property [J. Amer. Statist. Assoc.96 (2001) 1348–1360] and [Ann. Statist.32 (2004) 928–961] which ensures the optimal large sample performance. Furthermore, the high-dimensionality often induces the collinearity problem, which should be properly handled by the ideal method. Many existing variable selection methods fail to achieve both goals simultaneously. In this paper, we propose the adaptive elastic-net that combines the strengths of the quadratic regularization and the adaptively weighted lasso shrinkage. Under weak regularity conditions, we establish the oracle property of the adaptive elastic-net. We show by simulations that the adaptive elastic-net deals with the collinearity problem better than the other oracle-like methods, thus enjoying much improved finite sample performance.},
  keywords = {62J05,62J07,Adaptive regularization,done,elastic-net,high dimensionality,Model selection,oracle property,shrinkage methods},
  number = {4}
}

@book{zucchini2016,
  title = {Hidden {{Markov Models}} for {{Time Series}}: {{An Introduction Using R}}},
  shorttitle = {Hidden {{Markov Models}} for {{Time Series}}},
  author = {Zucchini, Walter and Macdonald, Iain and Langlock, Ronald},
  date = {2016},
  publisher = {{CRC Press}},
  doi = {10.1201/9781420010893},
  abstract = {Reveals How HMMs Can Be Used as General-Purpose Time Series Models Implements all methods in RHidden Markov Models for Time Series: An Introduction Using R applies hidden Markov models (HMMs) to a wide range of time series types, from continuous-valued, circular, and multivariate series to binary data, bounded and unbounded counts, and categorical observations. It also discusses how to employ the freely available computing environment R to carry out computations for parameter estimation, model selection and checking, decoding, and forecasting. Illustrates the methodology in actionAfter presenting the simple Poisson HMM, the book covers estimation, forecasting, decoding, prediction, model selection, and Bayesian inference. Through examples and applications, the authors describe how to extend and generalize the basic model so it can be applied in a rich variety of situations. They also provide R code for some of the examples, enabling the use of the codes in similar applications. Effectively interpret data using HMMs This book illustrates the wonderful flexibility of HMMs as general-purpose models for time series data. It provides a broad understanding of the models and their uses.},
  isbn = {978-1-4822-5384-9},
  keywords = {hidden markov,markov,time series},
  series = {Monographs on Statistics and Applied Probability 150}
}


