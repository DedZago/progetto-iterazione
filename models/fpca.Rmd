---
title: "Multilevel Functional PCA"
author: "Daniele Zago"
date: "2021-05-05"
output: 
    pdf_document:
        dev: png
        extra_dependencies: ["bm"]
        df_print: kable
        latex_engine: xelatex
---

```{r fpca_setup, cache=FALSE, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, engine.opts='-l', tidy=FALSE, fig.width=12, fig.height=8)
knitr::knit_hooks$set(inline=function(x) prettyNum(round(x,2)))
setwd("~/Documents/git/progetto-iterazione/models")
```

```{r fpca_data}
library(tidyverse)
load("../data/mandarino-proc.RData")
mandarino = mandarino[ , -6 ]
mandarino_wide = spread(mandarino, time, "f0")
Y = as.matrix(mandarino_wide[ , 5:(NCOL(mandarino_wide)) ])     # Funzioni osservate
```

# Analisi con componenti principali funzionali (fPCA)#

Stima delle funzioni principali che spieghino il 95\% della varianza funzionale:

```{r fpca_base}
library(fdapace)

Y_list = lapply(seq_len(NROW(Y)), function(i) Y[i,])
t_list = lapply(seq_len(NROW(Y)), function(i) 1:20)

fpcaBase = FPCA(Ly = Y_list, Lt = t_list, optns = list(FVEthreshold = 0.95))
plot(fpcaBase)
K = fpcaBase$selectK                # Selected number of fPC

matplot(fpcaBase$phi, type = "l")
legend("topleft", legend=paste0("fPC ", 1:K), col=1:K,cex=0.8, fill=1:K)

```


Confronto tra dati originali e dati ricostruiti con le prime `r print(K)` componenti principali stimate:

```{r fpca_plot}
ypred = predict(fpcaBase, Y_list, t_list)
{
    par(mfrow = c(2,1))
    matplot(t(Y), type = "l")
    matplot(t(ypred$predCurves), type = "l")
    par(mfrow = c(1,1))
}
```

```{r fpca_test_equal}
# Verifico che le componenti principali sono stimate come mu + eigenfunctions %*% t(score)
scoresFPCA = fpcaBase$xiEst                              # Scores for each observation
eigenFPCA = fpcaBase$phi                                 # Estimated eigenfunctions
predY = fpcaBase$mu + eigenFPCA %*% t(scoresFPCA)        # Y stimata a mano

{
    par(mfrow = c(2,1))
    matplot(t(ypred$predCurves), type = "l")
    matplot(predY, type = "l")
    par(mfrow = c(1,1))
}
```

```{r fpca_pred}
predFPCA = function(fpca, predictedScores){
    # Funzione che calcola le Y previste sulla base degli scores ottenuti da un modello
	#
    # @param fpca, risultato di una chiamata alla funzione FPCA
    # @param predictedScores scores previsti, matrice n x K
	#
    # @return matrice 20 x n contenente su ogni colonna la funzione Y prevista

    eigenFun = fpca$phi
    mu       = fpca$mu
    predY = mu + eigenFun %*% t(predictedScores)
    return(predY)
}

```

## Modello lineare multivariato sulle fPC ##
Tutti effetti fissi
```{r fpca_lm}
fitScoresBase = vector(mode = "list", length = K)
for(i in 1:K){
    fitScoresBase[[i]] = lm(scoresFPCA[ , i ] ~ cog_load + syllable1 + syllable2 + subject + syllable1:syllable2, data = mandarino_wide)
}
lapply(fitScoresBase, summary)
```

## Modello lineare multivariato con penalizzazione elastic-net ##

```{r fpca_glmnet}
library(glmnet)
X = model.matrix(scoresFPCA[ , 1 ] ~ subject + cog_load + cog_load:syllable1 + cog_load:syllable2 + syllable1 + syllable2 + syllable1:syllable2, data = mandarino_wide)[ , -1 ]
mgausGlmnet = cv.glmnet(X, scoresFPCA, family = "mgaussian")
plot(mgausGlmnet)
coef(mgausGlmnet)

# Confronto osservate con previste
predScores  = predict(mgausGlmnet, newx = X)[ , , 1]
predYglmnet = predFPCA(fpcaBase, predScores)
{
    par(mfrow = c(2,1))
    matplot(predY, type = "l")
    matplot(predYglmnet, type = "l")
    par(mfrow = c(1,1))
}
```

## Modello multivariato ad effetti casuali con Stan ##

```{r fpca_lmmStan}
library(rstanarm)
library(bayesplot)
cores  = min(4, parallel::detectCores())
chains = min(4, parallel::detectCores())
set.seed(123)

# Preparo i dati per Stan
data_stan = cbind(mandarino_wide[ , 1:4], "scores" = scoresFPCA)
# Stima del modello
fitStan = stan_mvmer(
                     formula = list(
                                    scores.1 ~ cog_load + syllable1 + syllable2 + syllable1:syllable2 + (1 + syllable1 + syllable2 + syllable1:syllable2|subject),
                                    scores.2 ~ cog_load + syllable1 + syllable2 + syllable1:syllable2 + (1 + syllable1 + syllable2|subject),
                                    scores.3 ~ cog_load + syllable1 + syllable2 + syllable1:syllable2 + (1 + syllable1 + syllable2|subject)
                                    ),
                     data = data_stan,
                     cores = cores,
                     chains = chains
)
save(fitStan, file="fitStan.Rdata")
summary(fitStan)
posterior_interval(fitStan)

scoresPred1 = posterior_predict(fitStan, draws = 50, m = 1)
scoresPred2 = posterior_predict(fitStan, draws = 50, m = 2)
scoresPred3 = posterior_predict(fitStan, draws = 50, m = 3)


ppc_dens_overlay_grouped(data_stan$scores.1, scoresPred1, group = data_stan$subject)
ppc_dens_overlay_grouped(data_stan$scores.2, scoresPred2, group = data_stan$subject)
ppc_dens_overlay_grouped(data_stan$scores.3, scoresPred3, group = data_stan$subject)

```

```{r fpca_predcurves}
plotPredictiveInterval = function(fitStan, fpcaBase, i, Y = NULL, p = c(0.025, 0.975), draws = 500){
    # Plot di E(Y*)|Y e intervallo previsivo di probabilita' p
    # per la curva di indice i nel dataframe passato a Stan.
	#
    # @param fitStan, modello Stan stimato
    # @param fpcaBase, risultato della chiamata a FPCA
    # @param i, indice della curva, 1 \le i \le NROW(data_stan)
    # @param p, quantili per l'intervallo previsivo a posteriori
    # @param draws, numero di simulazioni di Y* | Y
	#
    # @return un oggetto `ggplot2` contenente il grafico

    require(magrittr)
    require(ggplot2)

    # Curve osservate
    if(!is.null(Y)){
        ytrue = Y[i, ]
        brks  = c("y_obs", "E(fpca|y_obs)")
    } else {
        predY = fpcaBase$mu + fpcaBase$phi %*% t(fpcaBase$xiEst)
        ytrue = predY[ , i ]
        brks  = c("fpca_obs", "E(fpca|y_obs)")
    }
    scores1 = posterior_predict(fitStan, draws = draws, m = 1)
    scores2 = posterior_predict(fitStan, draws = draws, m = 2)
    scores3 = posterior_predict(fitStan, draws = draws, m = 3)

    ypred = predFPCA(fpcaBase, cbind(scores1[ , i ], scores2[ , i ], scores3[ , i ]))
    qqypred = apply(ypred, 1, function(col) quantile(col, probs = p))
    meanpred = apply(ypred, 1, mean)

    df = data.frame("ytrue" = ytrue, "meanpred" = meanpred,
               "lower" = qqypred[1, ], "upper" = qqypred[2, ])

    ggplot(df) +
        geom_ribbon(aes(x=as.integer(rownames(df)), ymin = lower, ymax = upper), alpha = 0.65, fill = "gray") +
        geom_line(aes(x=as.integer(rownames(df)), y = ytrue, colour = brks[1])) +
        geom_line(aes(x=as.integer(rownames(df)), y = meanpred, colour = brks[2])) +
        scale_colour_manual("", 
                      breaks = brks,
                      values = c("black", "blue")) +
        xlab("t") +
        ylab("f0") 
}

library(gridExtra)
library(lemon)
library(ggpubr)

plots_fpca = lapply(1:12, function(i) plotPredictiveInterval(fitStan, fpcaBase, i))
ggarrange(plotlist = plots_fpca, common.legend = TRUE, legend = "right")

plots_yobs = lapply(13:24, function(i) plotPredictiveInterval(fitStan, fpcaBase, i, Y))
ggarrange(plotlist = plots_yobs, common.legend = TRUE, legend = "right")
```

TODO:

* Selezione degli effetti fissi usando la loocv, si possono togliere effetti fissi da una, due o tutte le var risposta (scores). Il cog_load sembra la più promettente da rimuovere senza perdere troppa flessibilità del modello.
  Per confrontare i due modelli, usare `loo` e `loo_compare` e vedere se la differenza tra i criteri di informazione è "significativamente" più grande dello standard error.
  Occhio a guardare i warnings dei valori di $K$ di Pareto, che non ce ne siano di troppo brutti. In caso è meglio mettere un threshold come dice il warning. 

* Tirare fuori la posteriori dei beta sillabe globali e di interazione, simulare scores al variare delle combinazioni e guardare le funzioni $Y$ risultanti dalla somma delle componenti principali previste.
  Questo ci (dovrebbe) dare un'indicazione degli effetti di anticipazione / trascinamento / mediazione delle sillabe.

* Eventualmente provare cose più complesse perché come al solito arriva Gelman e ci smonta tutto [link](https://discourse.mc-stan.org/t/stan-glmer-marginal-effects/14234/4)
