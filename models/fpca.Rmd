---
title: "Multilevel Functional PCA"
author: "Daniele Zago"
date: "2021-05-05"
output: 
    pdf_document:
        dev: png
        extra_dependencies: ["bm"]
        df_print: kable
        latex_engine: xelatex
---

```{r fpca_setup, cache=FALSE, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, engine.opts='-l', tidy=FALSE, fig.width=12, fig.height=8)
knitr::knit_hooks$set(inline=function(x) prettyNum(round(x,2)))
setwd("~/Documents/git/progetto-iterazione/models")
```

```{r fpca_data}
library(tidyverse)
load("../data/mandarino-proc.RData")
mandarino = mandarino[ , -6 ]
mandarino_wide = spread(mandarino, time, "f0")
Y = as.matrix(mandarino_wide[ , 5:(NCOL(mandarino_wide)) ])     # Funzioni osservate
```

# Analisi con componenti principali funzionali (fPCA)#

Stima delle funzioni principali che spieghino il 95\% della varianza funzionale:

```{r fpca_base}
library(fdapace)

Y_list = lapply(seq_len(NROW(Y)), function(i) Y[i,])
t_list = lapply(seq_len(NROW(Y)), function(i) 1:20)

fpcaBase = FPCA(Ly = Y_list, Lt = t_list, optns = list(FVEthreshold = 0.95))
plot(fpcaBase)
K = fpcaBase$selectK                # Selected number of fPC

matplot(fpcaBase$phi, type = "l")
legend("topleft", legend=paste0("fPC ", 1:K), col=1:K,cex=0.8, fill=1:K)

```


Confronto tra dati originali e dati ricostruiti con le prime `r print(K)` componenti principali stimate:

```{r fpca_plot}
ypred = predict(fpcaBase, Y_list, t_list)
{
    par(mfrow = c(2,1))
    matplot(t(Y), type = "l")
    matplot(t(ypred$predCurves), type = "l")
    par(mfrow = c(1,1))
}
```

```{r fpca_test_equal}
# Verifico che le componenti principali sono stimate come mu + eigenfunctions %*% t(score)
scoresFPCA = fpcaBase$xiEst                              # Scores for each observation
eigenFPCA = fpcaBase$phi                                 # Estimated eigenfunctions
predY = fpcaBase$mu + eigenFPCA %*% t(scoresFPCA)        # Y stimata a mano

{
    par(mfrow = c(2,1))
    matplot(t(ypred$predCurves), type = "l")
    matplot(predY, type = "l")
    par(mfrow = c(1,1))
}
```

```{r fpca_pred}
predFPCA = function(fpca, predictedScores){
    # Funzione che calcola le Y previste sulla base degli scores ottenuti da un modello
	#
    # @param fpca, risultato di una chiamata alla funzione FPCA
    # @param predictedScores scores previsti, matrice n x K
	#
    # @return matrice 20 x n contenente su ogni colonna la funzione Y prevista

    eigenFun = fpca$phi
    mu       = fpca$mu
    predY = mu + eigenFun %*% t(predictedScores)
    return(predY)
}

```

## Modello lineare multivariato sulle fPC ##
Tutti effetti fissi
```{r fpca_lm}
fitScoresBase = vector(mode = "list", length = K)
for(i in 1:K){
    fitScoresBase[[i]] = lm(scoresFPCA[ , i ] ~ cog_load + syllable1 + syllable2 + subject + syllable1:syllable2, data = mandarino_wide)
}
lapply(fitScoresBase, summary)
```

## Modello lineare multivariato con penalizzazione elastic-net ##

```{r fpca_glmnet}
library(glmnet)
X = model.matrix(scoresFPCA[ , 1 ] ~ subject + cog_load + cog_load:syllable1 + cog_load:syllable2 + syllable1 + syllable2 + syllable1:syllable2, data = mandarino_wide)[ , -1 ]
mgausGlmnet = cv.glmnet(X, scoresFPCA, family = "mgaussian")
plot(mgausGlmnet)
coef(mgausGlmnet)

# Confronto osservate con previste
predScores  = predict(mgausGlmnet, newx = X)[ , , 1]
predYglmnet = predFPCA(fpcaBase, predScores)
{
    par(mfrow = c(2,1))
    matplot(predY, type = "l")
    matplot(predYglmnet, type = "l")
    par(mfrow = c(1,1))
}
```

## Modello multivariato ad effetti casuali con Stan ##

```{r fpca_lmmStan}
library(rstanarm)
library(bayesplot)
cores  = min(4, parallel::detectCores())
chains = min(4, parallel::detectCores())
set.seed(123)

# Preparo i dati per Stan
data_stan = cbind(mandarino_wide[ , 1:4], "scores" = scoresFPCA)
# Stima del modello
fitStan = stan_mvmer(
                     formula = list(
                                    scores.1 ~ cog_load + cog_load:syllable1 + cog_load:syllable2 + syllable1 + syllable2 + syllable1:syllable2 + (1 + syllable1 + syllable2 |subject),
                                    scores.2 ~ cog_load + cog_load:syllable1 + cog_load:syllable2 + syllable1 + syllable2 + syllable1:syllable2 + (1 + syllable1 + syllable2 |subject),
                                    scores.3 ~ cog_load + cog_load:syllable1 + cog_load:syllable2 + syllable1 + syllable2 + syllable1:syllable2 + (1 + syllable1 + syllable2 |subject)
                                    ),
                     data = data_stan,
                     cores = cores,
                     chains = chains
)
save(fitStan, file="fitStan.Rdata")
summary(fitStan)

scoresPred1 = posterior_predict(fitStan, draws = 50, m = 1)
ppc_dens_overlay_grouped(data_stan$scores.1, scoresPred1, group = data_stan$subject)

scoresPred2 = posterior_predict(fitStan, draws = 50, m = 2)
ppc_dens_overlay_grouped(data_stan$scores.2, scoresPred2, group = data_stan$subject)

scoresPred3 = posterior_predict(fitStan, draws = 50, m = 3)
ppc_dens_overlay_grouped(data_stan$scores.3, scoresPred3, group = data_stan$subject)

```


TODO:

* Selezione degli effetti fissi usando la loocv, si possono togliere effetti fissi da una, due o tutte le var risposta (scores). Il cog_load sembra la più promettente da rimuovere senza perdere troppa flessibilità del modello.
  Per confrontare i due modelli, usare `loo` e `loo_compare` e vedere se la differenza tra i criteri di informazione è "significativamente" più grande dello standard error.
  Occhio a guardare i warnings dei valori di $K$ di Pareto, che non ce ne siano di troppo brutti. In caso è meglio mettere un threshold come dice il warning. 

* Tirare fuori la posteriori dei beta sillabe globali e di interazione, simulare scores al variare delle combinazioni e guardare le funzioni $Y$ risultanti dalla somma delle componenti principali previste.
  Questo ci (dovrebbe) dare un'indicazione degli effetti di anticipazione / trascinamento / mediazione delle sillabe.

* Eventualmente provare cose più complesse perché come al solito arriva Gelman e ci smonta tutto [link](https://discourse.mc-stan.org/t/stan-glmer-marginal-effects/14234/4)
